{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Topics</h4>\n",
    "\n",
    "* Boolean\tRetrieval\tand\tVector\tSpace\tModels\n",
    "* Inverted\tIndex\n",
    "* Evaluation Measures\n",
    "* IR\n",
    "* [Relevence Feedback](#Relevence Feedback)\n",
    "* Recommender Systems\n",
    "* Web Search\n",
    "* Mapreduce & Spark\n",
    "* Association Rule Mining\n",
    "* Sentiment analysis\n",
    "* Online behavioral mining\n",
    "\n",
    "-------------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> What is anchor text</b>\n",
    "\n",
    "* Anchor text is any text that comes after the main domain name that helps locate content\n",
    "    - http://en.wikipedia.org/<b>wiki/Main_Page\">Wikipedia</b>\n",
    "    - https://www.infoq.com/<b>news/2017/03/tensorflow-spark</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Boolean\tRetrieval\tand\tVector\tSpace Models</h4>\n",
    "\n",
    "* Boolean Retrieval: Traditional exact match search (AND, NOT, OR)\n",
    "* Ranked Boolean Retrieval: Same but rank the results at end by freq. of query terms\n",
    "* Both these are too precise or too loose, don't do weighted rankings, and are bad at discriminating the important results\n",
    "* Similarity of doc d to query terms A AND B = $1-\\sqrt{\\frac{(1-d_A)^2+(1-d_B)^2}2}$  \n",
    "* where ($d_A, d_B$ =1 if match, 0 if not)\n",
    "* For an OR query: $\\sqrt{\\frac{(d_A)^2+(d_B)^2}2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is Zipf's Law?</h4>\n",
    "\n",
    "* An example of the power law distribution applied to IR\n",
    "* It states that the ith most frequent term has frequency proportional to 1/i\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain TF-IDF</h4>\n",
    "\n",
    "* Two methods joiend together for information retreieval\n",
    "    - Term-Frequency: frequency of occurence of a term in a document)\n",
    "    - Inverted Document Frequency: (more weighting to terms that don't occur across too many documents)\n",
    "\n",
    "..tbc\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is an inverted index?</h4>\n",
    "\n",
    "* A key-value pair of search terms (the key) and a list of documents IDs they appears in (value) e.g. 'fish {4}'\n",
    "* Can also unclude count e.g. 'fish {4:2}'\n",
    "* or positions e.g. 'fish {4:100, 4:110}'\n",
    "* Can perform text transformation before indexing (e.g. tokenization, stemming etc.)\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain text transformation</h4>\n",
    "* Tokenization: \n",
    "    - Extract the true word from any sequence of characters.\n",
    "    - Must consider issues like capitalization, hyphens, apostrophes, non-alpha\n",
    "characters, separators\n",
    "* Stop words: Remove common words, e.g. 'and': Limited use\n",
    "* Stemming: Group words derived from a common stem, e.g., “computer”, “computers”, “computing”, “compute”: Effective but not for all queries or languages\n",
    "    - Dictionary-based: uses list of related words\n",
    "    - Algorithmic-based: Uses rules usch as remove s from endings, but can cause mistakes\n",
    "    - Porter Stemmer is one example of an algorithmic stemmer - created in 70's ,creates stems not words, creates a number of errors though\n",
    "    - Token Normalization: process of canonicalizing tokens so that matches occur despite superficial differences in the character sequences of the tokens. For instance, if the tokens anti-discriminatory and antidiscriminatory are both mapped onto the term antidiscriminatory, in both the document text and queries, then searches for one term will retrieve documents that contain either. \n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Evaluaton Measures</h3>\n",
    "Several measures exist for measuring the effectiveness of ranking systems. \n",
    "\n",
    "<b>Precision:</b>\n",
    "Documents retrieved divided by the total number of relevent documents retrieved, where documents retrieved are limited to the same count as the number of relevent documents\n",
    "\n",
    "<b>R-Precision</b>\n",
    "* Number of relevent documents retrieved divided by total number of relevent documents\n",
    "<b>Recall: the\tnumber\tof\trelevant documents retrieved\tdivided\tby\tthe\ttotal\tnumber\tof\texisting\t\n",
    "relevant documents</b>\n",
    "\n",
    "<b>DCG</b>\n",
    "\n",
    "$${\\mathrm  {DCG_{{p}}}}=\\sum _{{i=1}}^{{p}}{\\frac  {2^{{rel_{{i}}}}-1}{\\log _{{2}}(i+1)}}$$\n",
    "\n",
    "The version of DCG shown here is the one that is more commonly used [3]\n",
    "Here $rel_i$ represents the relevence of document i to the query and the numerator here is formulated to place more emphasis on highly relevent documents than using $rel_i$ on its on own. The denominator ${\\log _{{2}}(i+1)}$ is a logarithmic discounting factor based on the position of the ranking i, where i=1 would represent a the highest ranked document. The discounted relevence is summed up from position 1 to position p, to provide the a relevence score for the top p positions.\n",
    "\n",
    "<b>NDCG</b>\n",
    "\n",
    "The DCG score is not comparable across multiple queries. To make it comparable it requires normalizing by dividing the above by the ideal DCG (IDCG) - which is the DCG of the idea ranking.\n",
    "\n",
    "$$NDGC = \\frac{DCG}{optDCG}$$\n",
    "\n",
    "<b>MAP</b>\n",
    "\n",
    "Mean Average preision is simply the mean of the average precision across queries - that is to say given a query and a set of rankings, calculate:  $$\\frac{sum \\ of \\ correct \\ rank \\ positions}{total \\ count \\ of \\ ranked \\ items}$$. Then average this across all queries.\n",
    "\n",
    "<b>Kendalls Tau</b>\n",
    "\n",
    "This is a measure derived from the field of statistics and mesures the similarity between two sets of rankings by calculating the number of concordant and discordant pairs - that is it say are the items that both $x_i > x_j$ and $y_i > y_j$ or $x_i < x_j$ and $y_i < y_j$  where i and j represent rank positions [5]\n",
    "\n",
    "<b>RMSE</b>\n",
    "\n",
    "The Root-Mean Squared error is a common measure used in regression and hence suitable for pointwise models. It's calculated as the square-root of the mean squared difference between an actual and predicted parameter. \n",
    "\n",
    "$$\\operatorname{RMSE}= \\sqrt{\\frac{\\sum_{t=1}^n (x_{1,t} - x_{2,t})^2}{n}}.$$\n",
    "\n",
    "<b>TREC_eval</b>\n",
    "\n",
    "Trec_eval is the standard tool used by the TREC community for evaluating an ad hoc retrieval run, given the results file and a standard set of judged results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is the Wilcoxon signed-rank test?</b>\n",
    "\n",
    "* The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used when comparing two related samples, matched samples, or repeated measurements on a single sample to assess whether their population mean ranks differ (i.e. it is a paired difference test). \n",
    "* It can be used as an alternative to the paired Student's t-test, t-test for matched pairs, or the t-test for dependent samples when the population cannot be assumed to be normally distributed.\n",
    "* Used to measure: When observing a difference in effectiveness scores across two retrieval systems: Does this difference occur by random chance?\n",
    "\n",
    "$$W=\\sum _{{i=1}}^{{N_{r}}}[\\operatorname{sgn}(x_{{2,i}}-x_{{1,i}})\\cdot R_{i}]$$\n",
    "\n",
    "<img src=\"../_img/irdm_1.jpg\" width=\"700\"> \n",
    "\n",
    "where A and B refer to different search engines, and the number is their average score on a query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Relevence Feedback'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Relevence Feedback</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Vector Space</b>\n",
    "* We can put documents and queries into the same vector space through the terms in the documents such a in TF-ID* \n",
    "* Each axis in the vector space is a query term -- so a vector space\n",
    "* Relevence Feeback means improving the return of results in this vetor space based on what users find relevent\n",
    "* When measuring the similarity between any two points in the vetor space we use the cosine distance (angle from 0) between them rather than the Eucledian distance. The latter for un-normalized data would be quite large (see image)\n",
    "\n",
    "<img src=\"../_img/irdm_2.jpg\" width=\"200\"> \n",
    "\n",
    "* Measuring the angle between them is like saying I'm more more interested in the direction in the vector space that makes them different from the average document\n",
    "* Document length normalization is used to normalize the length of the arrows --- long story short tf-idf algorithms perform a lot of this under the hood\n",
    "\n",
    "<b>Rocchio Alogirthm</b>\n",
    "* One example of this is the Roccho algorithm seeks to find a theoretical optimal query that would maximimze the getting back relevent documents only (so its ability to separate between relevent and non-relevant is optimal given two classes)\n",
    "* It does this by working out the centroid (point in space) of all relevent documents, A, and non-relevent documents, B. The difference between these is added on to A to find the point in space where the optimal query resides\n",
    "\n",
    "<img src=\"../_img/irdm_3.jpg\" width=\"300\"> \n",
    "\n",
    "* The adjustment made is added whenever someone types in the query again, using Rocchio's formula where $\\alpha$ and $\\gamma$ are hyper-parameters:\n",
    "$$q_m = \\left(\\alpha q_0+\\beta \\frac{1}{{\\lvert}D_{rel}{\\rvert}}\\sum_{d_j\\in D_{rel}}d_j \\right) - \\left( \\ \\gamma\\frac{1}{{\\lvert}D_{notRel}{\\rvert}}\\sum_{d_j\\in D_{notRel}}d_j\\right)$$\n",
    "* Obviously the problem with this is that all 'relevent' documents are not known in advance so in practice you store what you know as being relevent to a query over time (based on user feedback) and utilize that instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>IRDM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Topics</h4>\n",
    "* Indexing\n",
    "    - tokenisation \n",
    "    - normalisation\t\n",
    "    - stopword\tremoval\n",
    "    - stemming\n",
    "    - inverted\tindexing\n",
    "\n",
    "* Evaluation Measures\n",
    "* [Information Retrieval](#Information Retrieval)\n",
    "    * BM25\n",
    "    * Language Models\n",
    "* [Relevence Feedback](#Relevence Feedback)\n",
    "    - Rocchio\talgorithm\n",
    "    - Pseudo-relevance\tfeedback\n",
    "* [Recommender Systems](#Recommender Systems)\n",
    "     - User-based\n",
    "     - Item-based\n",
    "     - Probabilistic justification (not done)\n",
    "     - Singular\tValue\tDecomposition\t(SVD) and factorisation machine (not done)\n",
    "     - The\tcold-start\tproblem\tand\tactive learning solution\n",
    "\n",
    "* Web Search\n",
    "-Web graph\n",
    "    - PageRank\t\n",
    "    - Hubs and\tauthorities\n",
    "    - learning\tto\trank  (not done)\n",
    "* Mapreduce & Spark\n",
    "    - Inverted\tIndexing and\tPageRank\texamples (not done)\n",
    "* Association Rule Mining\n",
    "    - Apriori\tAlgorithm \n",
    "* Sentiment analysis\n",
    "* Online behavioral mining\n",
    "* Deep learning\n",
    "    - Single\tlayer\tperceptron/two-layer neural\t networks\n",
    "    - Back\tbackpropagation\talgorithm\n",
    "    - Word\tembedding\n",
    "    - CNN/RNN\n",
    "\n",
    "-------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> What is anchor text</b>\n",
    "\n",
    "* Anchor text is any text that comes after the main domain name that helps locate content\n",
    "    - http://en.wikipedia.org/<b>wiki/Main_Page\">Wikipedia</b>\n",
    "    - https://www.infoq.com/<b>news/2017/03/tensorflow-spark</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Indexing'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Indexing</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain text transformation</h4>\n",
    "\n",
    "* Parsing: Process sequence of text tokens to recognize structural elements such as titles,links\n",
    "\n",
    "* Tokenization: \n",
    "    - Extract the true word from any sequence of characters.\n",
    "    - Must consider issues like capitalization, hyphens, apostrophes, non-alpha\n",
    "    - Markup languages separate structure from content\n",
    "\n",
    "* Stop words: Remove common words, e.g. 'and': Limited use\n",
    "\n",
    "* Stemming: \n",
    "    - Group words derived from a common stem, e.g., “computer”, “computers”, “computing”, “compute”: \n",
    "    - Effective but not for all queries or languages\n",
    "    - Dictionary-based: uses list of related words\n",
    "    - Algorithmic-based: Uses rules usch as remove s from endings, but can cause mistakes\n",
    "\n",
    "* Porter Stemmer\n",
    "    - one example of an algorithmic stemmer - created in 70's\n",
    "    - creates stems not words, creates a number of errors though\n",
    "\n",
    "* Token Normalization: \n",
    "    - process of canonicalizing tokens so that matches occur despite superficial differences in the character sequences of the tokens. \n",
    "    - For instance, if the tokens anti-discriminatory and antidiscriminatory are both mapped onto the term antidiscriminatory, in both the document text and queries, then searches for one term will retrieve documents that contain either. \n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is a term-document incidence matrix?</h4>\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm001.png\" height=\"100\" width=\"150\">\n",
    "\n",
    "\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is an inverted index?</h4>\n",
    "\n",
    "Simple Inverted index:\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm002.png\" height=\"100\" width=\"240\">\n",
    "\n",
    "With Counts (better for ranking):\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm003.png\" height=\"100\" width=\"250\">\n",
    "\n",
    "With Positions (better for proximity matches):\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm004.png\" height=\"100\" width=\"310\">\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is Zipf's Law?</h4>\n",
    "\n",
    "* It states that the $i_{th}$ most frequent term has frequency proportional to $\\frac{1}{i}$\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain TF-IDF</h4>\n",
    "\n",
    "* Two methods joiend together for information retreieval\n",
    "    - Term-Frequency: frequency of occurence of a term in a document)\n",
    "    - Inverted Document Frequency: (more weighting to terms that don't occur across too many documents)\n",
    "\n",
    "..tbc\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3 style=\"background-color:#616161;color:white\">Evaluation Measures</h3>\n",
    "\n",
    "Several measures exist for measuring the effectiveness of ranking systems. \n",
    "\n",
    "<b>Recall:</b> Number\tof\trelevant documents retrieved divided\tby\tthe\ttotal number\tof existing relevant documents\n",
    "$$\\frac{Relevent}{Total Docs}$$\n",
    "\n",
    "<b>Precision:</b> Of the documents that were retrieved, how many of them were relevent?\n",
    "\n",
    "$$\\frac{Relevent}{Retrieved}$$\n",
    "\n",
    "\n",
    "<b>F1 Recall</b>\n",
    "Gives equal weight to recall and precision\n",
    "$$F1 = \\frac{2*recall*precision}{recall+precision}$$\n",
    "\n",
    "Notes:\n",
    "* Precision/recall/F are measures for unranked sets\n",
    "* Typically, Precision and Recall are inversely related, ie. as Precision increases, recall falls and vice-versa. A balance between these two needs to be achieved by the IR system, and to achieve this and to compare performance, the precision-recall curves come in handy.\n",
    "* We can easily turn set measures into measures of lists that are already ranked.\n",
    "* Just compute the set measure for each “cut-off”: the top 1, top 2, top 3, top 4 etc results \n",
    "* Doing this for precision and recall gives you a precision-recall curve.\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm015.png\" height=\"100\" width=\"250\">\n",
    "\n",
    "* Note: This curve doesn't capture which point was for which cut-off.\n",
    "\n",
    "<b>Break-even precision</b>: When precision = recall based on starting from the top 1 document and expanding to top k\n",
    "\n",
    "<b>Average-Precision:</b> Average the precision numbers used in your precision-recall curve. Ignore cases where precision was 0.\n",
    "\n",
    "<b>R-Precision</b> : The precision at a specific rank level\n",
    "\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>DCG (with nonlinear gain function)</b>\n",
    "\n",
    "$${\\mathrm  {DCG_{{p}}}}=\\sum _{{i=1}}^{{p}}{\\frac  {(2^{{Relv_{{i}}}})-1}{\\log _{{2}}(rankPos+1)}}$$\n",
    "\n",
    "* The version of DCG shown here is the one that is more commonly used\n",
    "* Here $Relv_i$ represents the relevence of document i to the query and the numerator here is formulated to place more emphasis on highly relevent documents than using $rel_i$ on its on own. \n",
    "* The denominator ${\\log _{{2}}(i+1)}$ is a logarithmic discounting factor based on the position of the ranking i, where i=1 would represent a the highest ranked document. \n",
    "* The discounted relevence is summed up from position 1 to position p, to provide the a relevence score for the top p positions.\n",
    "\n",
    "<b>NDCG</b>\n",
    "\n",
    "The DCG score is not comparable across multiple queries. To make it comparable it requires normalizing by dividing the above by the ideal DCG (IDCG) - which is the DCG of the idea ranking.\n",
    "\n",
    "$$NDGC = \\frac{DCG}{optDCG}$$\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm016.png\" height=\"100\" width=\"350\">\n",
    "\n",
    "<b>MAP</b>\n",
    "\n",
    "Mean Average preision is simply the mean of the average precision across queries - that is to say given a query and a set of rankings, calculate:  $$\\frac{sum \\ of \\ correct \\ rank \\ positions}{total \\ count \\ of \\ ranked \\ items}$$. Then average this across all queries.\n",
    "\n",
    "<b>Kendalls Tau</b>\n",
    "\n",
    "This is a measure derived from the field of statistics and mesures the similarity between two sets of rankings by calculating the number of concordant and discordant pairs - that is it say are the items that both $x_i > x_j$ and $y_i > y_j$ or $x_i < x_j$ and $y_i < y_j$  where i and j represent rank positions [5]\n",
    "\n",
    "<b>RMSE</b>\n",
    "\n",
    "The Root-Mean Squared error is a common measure used in regression and hence suitable for pointwise models. It's calculated as the square-root of the mean squared difference between an actual and predicted parameter. \n",
    "\n",
    "$$\\operatorname{RMSE}= \\sqrt{\\frac{\\sum_{t=1}^n (x_{1,t} - x_{2,t})^2}{n}}.$$\n",
    "\n",
    "<b>TREC_eval</b>\n",
    "\n",
    "Trec_eval is the standard tool used by the TREC community for evaluating an ad hoc retrieval run, given the results file and a standard set of judged results.  \n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What's the difference between Online and Offline Evaluation?</h4>\n",
    "\n",
    "Offline:\n",
    "* Controlled laboratory experiments\n",
    "* The users’ interaction with the engine is only\n",
    "simulated\n",
    "\n",
    "Online:\n",
    "* Clickthrough rate\n",
    "* Queries per user\n",
    "* Probability user skips over results they have\n",
    "considered (pSkip)\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What is result interleaving?</h4>\n",
    "\n",
    "* A method to combine the rankings given by two different methods\n",
    "* Method - Randomly select one ranking to go first. Pick their high ranked item remaining\n",
    "* Then pick the highest ranked item from the other\n",
    "* Randomly select again and repeat\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What is the Wilcoxon signed-rank test?</b>\n",
    "\n",
    "* The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used when comparing two related samples, matched samples, or repeated measurements on a single sample to assess whether their population mean ranks differ (i.e. it is a paired difference test). \n",
    "* It can be used as an alternative to the paired Student's t-test, t-test for matched pairs, or the t-test for dependent samples when the population cannot be assumed to be normally distributed.\n",
    "* Used to measure: When observing a difference in effectiveness scores across two retrieval systems: Does this difference occur by random chance?\n",
    "\n",
    "$$W=\\sum _{{i=1}}^{{N_{r}}}[\\operatorname{sgn}(x_{{2,i}}-x_{{1,i}})\\cdot R_{i}]$$\n",
    "\n",
    "<img src=\"../_img/irdm_1.jpg\" width=\"600\"> \n",
    "\n",
    "where A and B refer to different search engines, and the number is their average score on a query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Information Retrieval'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Information Retrieval</h3>\n",
    "\n",
    "<b>Older models</b>\n",
    "* Boolean retrieval\n",
    "* Vector Space model\n",
    "\n",
    "<b>Probabilistic Models</b>\n",
    "* Language models (developed for NLP community but now used a lot in IR)\n",
    "* BM25 (used a lot, tends to perform better than even Language models. Also similar to TF-IDF)\n",
    "\n",
    "Note: Without taking user knoweldge into account the algorithm would assume two different users means the same thing when they type in the same word\n",
    "\n",
    "<b>Combining evidence</b>\n",
    "* Inference networks\n",
    "* Learning to Rank (state-of-the-art, utilizes other models such as BM25 as features)\n",
    "\n",
    "-------------\n",
    "\n",
    "<b>Retrieval Models</b>\n",
    "\n",
    "* Boolean\n",
    "    – Basic Boolean\n",
    "    – Extended Boolean model\n",
    "* Vector space model SMART\n",
    "* Probabilistic models\n",
    "    * Basic probabilistic model\n",
    "    * Two Poisson model – BM25 Okapi\n",
    "    * Bayesian inference networks (e.g. Indri)\n",
    "    * Statistical language models (e.g. Lemur)\n",
    "    * Portfolio retrieval\n",
    "* Citation analysis models\n",
    "    * Hubs & authorities (e.g. CLEVER by IBM)\n",
    "    * PageRank (e.g. Google Search)\n",
    "• Learning to rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Boolean Retrieval'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Boolean Retrieval</h4>\n",
    "\n",
    "* <b>Boolean Retrieval</b>: Traditional exact match search (AND, NOT, OR)\n",
    "* <b>Ranked Boolean Retrieval</b>: Same but rank the results at end by the frequency of query terms\n",
    "* Both these are too precise or too loose, don't do weighted rankings, and are bad at discriminating the important results\n",
    "\n",
    "Extended:\n",
    "* Similarity of doc d to query terms A AND B = $1-\\sqrt{\\frac{(1-d_A)^2+(1-d_B)^2}2}$  \n",
    "* where ($d_A, d_B$ =1 if match, 0 if not)\n",
    "* For an OR query: $\\sqrt{\\frac{(d_A)^2+(d_B)^2}2}$\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm017.png\" height=\"100\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Vector Space Model'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Vector Space Model</h4>\n",
    "\n",
    "<b>Vector Space</b>\n",
    "* We can put documents and queries into the same vector space through the terms in the documents such a in TF-ID* \n",
    "\n",
    "* Each axis in the vector space is a query term -- so a vector space\n",
    "\n",
    "* Relevence Feedback means improving the return of results in this vector space based on what users find relevent\n",
    "\n",
    "* When measuring the similarity between any two points in the vetor space we use the cosine distance (angle from 0) between them rather than the Eucledian distance. The latter for un-normalized data would be quite large (see image)\n",
    "\n",
    "<img src=\"../_img/irdm_2.jpg\" width=\"200\"> \n",
    "\n",
    "* Measuring the angle between them is like saying I'm more more interested in the direction in the vector space that makes them different from the average document. It's a measure of the similarity between two vectors than the distance between them, because Euclidean distance is large for vectors of different lengths\n",
    "\n",
    "* <b>We find similarity by ranking documents in <b>increasing</b> order of cosine(query,document) as Cosine is a monotonically decreasing function for the interval [0-180 degrees]</b>\n",
    "\n",
    "\n",
    "* Document length normalization is used to normalize the length of the arrows\n",
    "\n",
    "* A vector can be length-normalized by dividing each of its components by its length, where length is the L2 norm: $\\sqrt{\\sum d^2_i}$\n",
    "\n",
    "*  Dividing a vector by its L2 norm makes it a unit (length) vector (on surface of unit hypersphere)\n",
    "\n",
    "* Effect on the two documents d and dʹ (d appended to itself) from earlier slide: they have identical vectors after length-normalization.\n",
    "\n",
    "* Long and short documents now have comparable\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color:#616161;color:white\">Relevence Feedback</h4>\n",
    "\n",
    "<b>Centroid</b>\n",
    "* \n",
    "\n",
    "<b>Rocchio Algorithm</b>\n",
    "* Suppose given a query, we observed that, out of N documents, there are R number of relevant documents\n",
    "* The Roccho algorithm seeks to find a <b>theoretical optimal query (i.e. keywords represented as a vector)</b> that would maximimze the getting back relevent documents only (so its ability to separate between relevent and non-relevant is optimal given two classes)\n",
    "* It does this by working out the centroid (point in space) of all relevent documents, A, and non-relevent documents, B. The difference between these is added on to A to find the point in space where the optimal query resides\n",
    "\n",
    "<img src=\"../_img/irdm_3.jpg\" width=\"300\"> \n",
    "\n",
    "* The adjustment made is added whenever someone types in the query again, using Rocchio's formula where $q_0$ is the original query and $\\alpha$ and $\\gamma$ are hyper-parameters:\n",
    "\n",
    "$$q_m = \\left(\\alpha q_0+\\beta \\frac{1}{{\\lvert}D_{Relv}{\\rvert}}\\sum_{d_j\\in D_{Relv}}d_j \\right) - \\left( \\ \\gamma\\frac{1}{{\\lvert}D_{notRelv}{\\rvert}}\\sum_{d_j\\in D_{notRelv}}d_j\\right)$$\n",
    "\n",
    "Note:\n",
    "* $q_1$ = $q_0$ + something\n",
    "* the one 1 over and sum bits are the centroid function for relevent and non-rel respectively\n",
    "\n",
    "Obviously the problem with this is that all 'relevent' documents are not known in advance so in practice you store what you know as being relevent to a query over time (based on user feedback) and utilize that instead\n",
    "\n",
    "---------------\n",
    "\n",
    "<h4>Pseudo-Relevence Feedback</h4>\n",
    "\n",
    "* Pseudo-relevance feedback\tautomates\tthe\t “manual” part\tof\ttrue\trelevance\tfeedback.\n",
    "* It does this by assuming the top k results from a query are definitely relevent and using that to score\n",
    "* Works well on average but can go horribly wrong for some queries and repeated iteraitons can cause query drift \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Probabilistic Model'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Probabilistic Models</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is the probability ranking principal</h4>\n",
    "\n",
    "The overall effectiveness of a system to its user will be the best that is obtainable on the basis of those data, if the response to each request is a ranking of the\n",
    "documents in the collection in order of decreasing probability of relevance to the user who submitted the request. \n",
    "\n",
    "And where the probabilities are estimated as accurately as possible on the basis of\n",
    "whatever data have been made available to the system for this purpose.\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What is Bayes Decision Rule?</b>\n",
    "\n",
    "** May not come up so haven't fully completed notes **\n",
    "\n",
    "* A document D is relevent, R if  $p(R|D) >= p(NotR|D)$\n",
    "(i.e. it is more relevent than not relevent)\n",
    "\n",
    "\n",
    "$p(R|D)= \\frac{p(D|R)}{p(R)}{p(D)} $\n",
    "\n",
    "$p(NotR|D) = \\frac{p(D|NotR)}{p(NotR)}{p(D)}$\n",
    "\n",
    "-----------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='BM25'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">BM25 Model</h4>\n",
    "\n",
    "\n",
    "* The BM25 model for example is a document scoring method based on a weighted sum of individual query terms present in the document\n",
    "* Shown here is the BM25+ model which doesn't penalize large documents (through the inclusion of $+\\delta$)\n",
    "$$score(D,Q)=\\sum _{i=1}^{n}{\\text{IDF}}(q_{i})\\cdot \\left[{\\frac {f(q_{i},D)\\cdot (k_{1}+1)}{f(q_{i},D)+k_{1}\\cdot \\left(1-b+b\\cdot {\\frac {|D|}{\\text{avgdl}}}\\right)}}+\\delta \\right]$$\n",
    "\n",
    "where:\n",
    "* IDF is the inverse document frequency weight of $q_i$\n",
    "* $f(q_i, D)$ = frequency of query term i in Document D\n",
    "* $\\lvert D \\rvert$ is the length of the document in words\n",
    "* avgdl is the average document length in the text collection from which documents are drawn\n",
    "* $k_1, \\delta $ and b are hyper-parameters, usually chosen, in absence of an advanced optimization, as k = 1.2 or 2, $\\delta$ = 1, and b = 0.75\n",
    "\n",
    "IDF is calculated as:\n",
    "$${IDF}(q_i) = \\log \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}$$\n",
    "\n",
    "where N is the total number of documents in the collection, and $n(q_i)$ is the number of documents containing $q_i$ \n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='LTR'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Learning to Rank</h4>\n",
    "\n",
    "LTR algorithms can be split into three distinct categories:\n",
    "\n",
    "<b>Pointwise:</b> Pointwise algorithms consider the relationship be-\n",
    "tween a single search query and a single document (a query-document\n",
    "pair). The aim is to score how relevant the document is for the given\n",
    "query on some defined scale.\n",
    "\n",
    "<b>Pairwise:</b> These methods are a step towards machine learning\n",
    "algorithms that directly rank a list of documents. In this para-\n",
    "digm, two documents are considered per query. The aim is to label\n",
    "whether the first document is of greater relevance than the sec-\n",
    "ond.\n",
    "\n",
    "<b>Listwise:</b> This approach seeks to directly order the entire list of\n",
    "documents (of any length) returned for a search query. As with\n",
    "pointwise and pairwise methods, there are numerous implemen-\n",
    "tations of the listwise philosophy. The loss function of a listwise\n",
    "method considers the ranking of a document list generated by the\n",
    "algorithm, and compares it some ground-truth ordering.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Language Models'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Language Models</h4>\n",
    "\n",
    "\n",
    "Three main approaches to LM in IR: Given a query Q and document D,\n",
    "* Query likelihood model: P($Q|M_D$)\n",
    "    - Create a unigram or bigram language model for every doc\n",
    "    - Work out the prob. of a doc being from each language model, for those queries that you think ought to predict relevent documents, by multiplying the prob of the document terms$$\n",
    "    - no notion of relevance in the model: everything is random sampling\n",
    "    - user feedback / query expansion not part of the model\n",
    "        - examples of relevant documents cannot help us improve the language model\n",
    "        \n",
    "        \n",
    "* Document likelihood model: P($D|M_Q$ )\n",
    "    - Create a unigram or bigram language model for every query\n",
    "    - Estimate a language model M Q for the query Q\n",
    "    - rank docs D by the likelihood of being a random sample from M\n",
    "        - different doc lengths, probabilities not comparable\n",
    "        - favours documents that contain frequent (low content) words\n",
    "        \n",
    "        \n",
    "* Divergence model: D($P(M_D$) || P($M_Q$))\n",
    "    - Estimate a model of both the query $M_Q$ and the document $M_D$\n",
    "    - directly compare similarity of the two models\n",
    "    - natural measure of similarity is cross-entropy (interpolation) (Kullback-Leiblar divergence)\n",
    "    $$H( M_Q || M_D ) -\\sum P(w|M_Q) log P ( w | M_D )$$\n",
    "    - cross-entropy is not symmetric. favours different document, Reverse works consistently worse.\n",
    "--------\n",
    "\n",
    "<b>Smoothing methods</b>\n",
    "* Smoothing is required in language models to deal with unseen words\n",
    "* Don’t smooth too much though.\n",
    "* Dirichlet works well for short queries (need to tune the parameter)\n",
    "* Jelinek-Mercer works well for longer queries (also needs tuning)\n",
    "* In general, Dirichlet smoothing seems to provide the best “happy-medium”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Association Rule Mining'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Association Rule Mining</h3>\n",
    "\n",
    "Recommended: http://www.kdnuggets.com/2016/04/association-rules-apriori-algorithm-tutorial.html\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm012.png\" height=\"100\" width=\"200\">\n",
    "\n",
    "\n",
    "* Support (i.e. Popularity): \n",
    "$$\\frac{Transactions \\  with   \\ product(s)}{Total \\ Transactions}$$ \n",
    "E.g. Apple = 4/8, and Apple & Beer = 3/8\n",
    "\n",
    "* Confidence (how likely item Y is purchased WITH X): Support of both / Support of one. Beer is 75% likely to be bought with Apple because 3/4= 75%.\n",
    "$$\\frac{Transactions \\  with   \\ both \\ products}{Transactions \\  with \\ X}$$ \n",
    "\n",
    "* Lift (Confidence taking into account popularity of Beer):\n",
    "$$\\frac{SupportOfBothProducts}{(SupportOfX)*(SupportOfY)}$$ \n",
    "\n",
    "e.g. Apple and Beer = 0.38 / 0.38  = 1\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apriori Algorithm</h4>\n",
    "\n",
    "* Based on the idea that an itemset cannot be frequent unless all it's subsets are frequent (Items, then pairs, then  triples)\n",
    "\n",
    "* JoinStep: Go through and build up a list $C_1$ of all single items and counts\n",
    "* PruneStep: Exclude anything that does not meet minSupport and store in $f_1$ -- remember support is count / total transations, so a minSupport of 0.5 here means 2/4 transactions.\n",
    "* Go through and build up a list $_C2$ of all pairs of items and counts but only for the items that appeared in $f_1$\n",
    "* And repeat process to build up $F_2$ for pairs, and $F_3$ for triplets etc.\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm013.png\" height=\"100\" width=\"500\">\n",
    "\n",
    "Step 2:\n",
    "* For each proper subset, calculation the associations using the confidence formula\n",
    "* Confidence that XYZ is purchased WITH ab = Support of XYZ / Support of ab\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm014.png\" height=\"100\" width=\"500\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Recommender Systems'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Recommender Systems</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='userBased'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">User Based</h4>\n",
    "\n",
    "* Recommendations based on finding similar users\n",
    "* Pearsons correlation and Cosine angle are two ways to meaasure this\n",
    "* Once you have that you can use it to predict ratings:\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm006.png\" height=\"100\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='ItemBased'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Item Based</h4>\n",
    "\n",
    "* Recommendations based on similarity of items\n",
    "\n",
    "* Works by predicting the rating for unrated items by a user, based on the ratings of similar items by the query user\n",
    "\n",
    "* Pearson and Cosine can be used to measure similarity between items based on their ratings, but Cosine is adjusted to take into account the difference in rating scales between different users\n",
    "\n",
    "* Return the top-N most similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='ProbJust'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Probabilistic Justification</h4>\n",
    "\n",
    "* You can say that a users preference follows a Gaussian distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='SVD'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Singular Value Decomposition (SVD) and factorisation machine</h4>\n",
    "\n",
    "* SVDs can be used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='ColdStartProblem'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Cold Start Problem</h4>\n",
    "\n",
    "Dealing with cold-starts:\n",
    "\n",
    "1) Extensive initial interaction (rating feedback). Not only num. ratings, but also what types of items rated\n",
    "\n",
    "2) Initialize with pre-cooked profiles\n",
    "\n",
    "3) Active-Learning: Questionnaire to minimize efforts to undderstand user-tastes\n",
    "\n",
    "* Method 1: Select “most uncertain” item given our current recommendation model and its parameters\n",
    "    - However the min-max strategy (select the most uncertainty item) does not necessarily reduce the “uncertainty” of our model maximally\n",
    "\n",
    "* Method 2: Select “most informative” item to maximally reduce the uncertainty of our recommendation prediction model (and its parameters)    \n",
    "* Reduce the entropy\n",
    "    - Entropy of an event is zero when the outcome is known\n",
    "    - Entropy is maximal when all outcomes are equally likely\n",
    "\n",
    "How to do this (in order of worst to best):\n",
    "* Find the parameter that would maximally reduce the uncertainty of our user model and make that the question\n",
    "\n",
    "* Find the parameter that would maximally reduce the uncertainy of the prediction\n",
    "\n",
    "* Random\n",
    "\n",
    "* Bayesian (Best way): If a 'true' or 'desired' user model is given use KL divergence to determine the queston to ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='MAP Reduce'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">MAP Reduce</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Sentiment Analysis'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Sentiment Analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Online Behavioural Mining'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Online Behavioural Mining</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Web-Search'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Web-Search / Web Graph</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='PageRank'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Page Rank</h4>\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm009.png\" height=\"100\" width=\"400\">\n",
    "\n",
    "What is PageRank:\n",
    "* PageRank score is the the long-term visitation rate experienced by a web-page\n",
    "* Can also be thought of as the steady-state probability if we modelled the system as a Markov Chain\n",
    "\n",
    "\n",
    "Power Iteration Algorithm:\n",
    "1. Initial page rank value, R is 1/n (n=number of pages in total)\n",
    "2. Now imagine each page also has a vote of 1 to allocate out\n",
    "3. Work out the transition matrix shown in top right. Reading <b>across</b> each row, it shows the <b>how much of someone elses vote each page is getting</b>\n",
    "3. The new value on each iteration is the sum of (share of someones vote * that persons PR score)\n",
    "\n",
    "<b>Dead Ends and Spider Traps</b>\n",
    "* Dead End: A page with no outlink\n",
    "* Spider trap: A group of page which collectively have no outlinks\n",
    "\n",
    "<b>Dealing with Dead Ends & Spider-traps</b>\n",
    "\n",
    "* In a dead end when a page has no outvotes beyond itself it can lead to votes pooling at a node\n",
    "* Google's solution is to assume there is a chance of jumping to a random page so you assume say:<br>\n",
    "0.8 * calculation above \n",
    "plus <br>\n",
    "0.2 * calculation above using 1/3 as the share of the vote you are getting from every page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='HITS'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">HITS</h4>\n",
    "\n",
    "* Hyperlink-Induced\tTopic\tSearch\t(HITS) Model\n",
    "\n",
    "* Authorities are\tpages\tcontaining\tuseful\t information\n",
    "* Hubs are\tpages\tthat\tlink\tto\tauthorities\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "* Generate adj matrix A (1 if\tpage i links to page j,else =0)\n",
    "* Set a and h vectors to be all 1\n",
    "* Calculate a as Axh and h as Axa\n",
    "* Divide each result by the sum of the vector to normalize it\n",
    "* Repeat using new a and b until h and a converge\n",
    "* Top h pages are your hubs, Top a pages are your authorities\n",
    "\n",
    "Iteration 1:\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm010.png\" height=\"100\" width=\"200\">\n",
    "\n",
    "Iteration 2:\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/irdm/irdm011.png\" height=\"100\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='NN'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Neural Nets</h4>\n",
    "\n",
    "Sigmoid Function\n",
    "$1-\\frac{1}{e^{-\\sum wx}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Bayesian Statistics </h3>\n",
    "\n",
    "Bayesian theory on probability it a core part of machine learning as well a fundamentally alternate viewpoint on statistical theory itself. The frequentist view of the world is that one can only make make statements based on observed data (i.e. data sampling). Bayesians allow for any prior beliefs about the data, prior to doing any sampling, allowing it to alter the posterior belief based on data.\n",
    "\n",
    "This is helpful in\\ situations where there is not much data. For example in earthquake\n",
    "modelling there maybe only be 4 or 5 earthquakes to have ever\n",
    "occurred on some particular fault.\n",
    "\n",
    "Bayesian probability statements are also easier to interpret which is\n",
    "important when communicating with non-statisticians. A frequentist 95 % confidence interval for a parameter θ does not mean that we are 95 % sure that θ lies in the interval. However, Bayesian credible intervals do have this interpretation. \n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<H4>Q: What  is a discreet random variable?</H4>\n",
    "\n",
    "Ans: variable representing an uncertain variable based off a probability distribution. Given {X=x), X is the discreet random variable.\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H4>Q: What is a probability mass function?</H4>\n",
    "\n",
    "The name given to the overall probabilty distribution. Can be represented as a chart or as a formula.\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: Explain prior and posterior probabilities in relation to estimating parameters</h4>\n",
    "\n",
    "Ans: In a typical inference problem we have an unknown parameter θ which\n",
    "we wish to estimate. For example, θ may be the mean of a Normal\n",
    "distribution, or the probability of a particular coin landing heads when\n",
    "tossed. We also have data Y , such as the outcome of tossing the coin\n",
    "multiple times. We wish to use the data Y to learn about θ .\n",
    "\n",
    "The prior distribution p (θ) represents our beliefs about θ before\n",
    "incorporating the information from the data.\n",
    "The posterior distribution p (θ| Y ) represents our beliefs about θ\n",
    "after incorporating the information from the data.\n",
    "Bayes theorem tells us how to move from p (θ) to p (θ| Y ) . I.e. given we\n",
    "have some beliefs about θ before seeing the data, it tells us the beliefs\n",
    "p (θ| Y ) we should have about θ after seeing the data.\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Q: List the fundamental rules of probability</H4>\n",
    "\n",
    "<b>1) UNION (OR) rule:</b> $$p(a \\ or \\ b) = p(a) + p(b) - p(both)$$  i.e. a union b = p(a) + p(b) - p(a intersect b)\n",
    "\n",
    "<b>2) Product/Joint probability rule:</b>\n",
    "$$p(a,b)=p(a|b)*p(b) = p(b|a)*p(a) $$\n",
    "\n",
    "i.e. Specific combination of a & b = prob. of one given the other * prob of the other\n",
    "\n",
    "<b>3) SUM rule / Marginal distribution:</b>\n",
    "$$p(a)=\\sum_{b} p(a,b) =\\sum_{b} p(a|b)*p(b) $$\n",
    "i.e. p(a) across all the possible values of b\n",
    "\n",
    "<b>4) Bayes Rule</b>\n",
    "$$ P(a \\mid b) = \\frac{P(b \\mid a) \\, P(a)}{P(b)}=\\frac{P(a,b) \\,}{P(b)} $$\n",
    "\n",
    "The denominator here can be extended using the Marginal distribution rule to become:\n",
    "\n",
    "$$ P(a \\mid b) =\\frac{P(b \\mid a) \\, P(a)}{\\sum_{b} p(b|a)*p(a)} $$\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "A new medical screening test is developed to assess whether a patient\n",
    "has a particular disease. The test is advertised to have the following\n",
    "degrees of accuracy: ”if the patient truly has the disease, then the test\n",
    "will correctly detect this and return a positive result with probability 0.95.\n",
    "If the patient truly does not have the disease, the test will correctly detect\n",
    "this and return a negative result with probability 0.98”\n",
    "Given that 1 in 1000 people in the population have the disease, what is\n",
    "the chance that a person testing positive on the test really has the\n",
    "disease?</h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "* p(d) = 1/1000 = 0.0001\n",
    "* p(pos|d) = 0.95\n",
    "* p(not pos | not d) = 0.98\n",
    "\n",
    "* Therefore: p(d|pos) =  P(pos | d)P(d) / P(pos) = \n",
    "\n",
    "$(0.95 * 0.001) / (0.95 * 0.001 + 0.02 * 0.999) = 0.045$\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q) Question</h4>\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q) Explain the chain rule</h4>\n",
    "\n",
    "P(A,B,C) = P(A| B,C) P(B,C) = P(A|B,C) P(B|C) P(C)\n",
    "\n",
    "P(A, B, ..., Z) = P(A| B, ..., Z) P(B| C, ..., Z) P(Y|Z) P(Z)\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Q: What is the difference between generative and discriminative models?\n",
    "\n",
    "* Discriminative models learn the (hard or soft) boundary between classes.\n",
    "* Generative models model the distribution of individual classes. I.e. they build a model of the external world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: Explain conditional independence</h4>\n",
    "\n",
    "Ans: If a and b are independent then p(a and b) = P(a) + p(b)\n",
    "Unfortuntely total independence is rare. Instead two variables may be independent under certain scenarios. \n",
    "Hence we can say, a and b are independent, given c.\n",
    "\n",
    "If a <i>independent</i> b | c, then p(a,b|c) = p(a|c)*p(c)\n",
    "\n",
    "i.e. If a and b are independent given c, then the probability of one of those, let's say a, is gonna be the probability of a given c, times the probability of c happening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: What is Expectation? </h4>\n",
    "\n",
    "* Another way of saying average\n",
    "* The average value of some function f(x) under a probability distribution p(x)\n",
    "\n",
    "$ \\mathbf{E}(f) = \\displaystyle \\sum p(x)f(x)  $ -- discreet case\n",
    "\n",
    "$ \\mathbf{E}(f) = \\displaystyle \\int p(x)f(x)dx  $ -- continuous case\n",
    "\n",
    "* Expectation can be estimated from a N samples drawn from a probabilty distribution function:\n",
    "\n",
    "$ \\mathbf{E}(f) \\simeq \\frac1 N \\sum f(x_n)$\n",
    "\n",
    "Note: Multiplying by \\frac1 N is the same as dividing by N. You'll see this a lot in machine learning formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q) Question</h4>\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What is variance and covariance?\n",
    "\n",
    "* <b>Variance</b> is the amount of variablility around the Expectation:\n",
    "\n",
    "$$ var[f] = \\mathbf{E}[(f(x) - \\mathbf{E}[f(x)])^2]= \\mathbf{E}[f(x)^2] - \\mathbf{E}[f(x)]^2  $$\n",
    "\n",
    "\n",
    "This translates to: Variation = Mean of what you predicted$^2$ minus what you expected$^2$\n",
    "\n",
    "<br>\n",
    "* <b>Covariance</b>: Measures the joint-variability between two variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What does this show?\n",
    "\n",
    "<img src=\"img/covariance.png\" height=\"200\" width=\"400\">\n",
    "\n",
    "Ans: \n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: What is maximum liklihood estimation</h4>\n",
    "\n",
    "A method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters.\n",
    "\n",
    "In formulas:<br>\n",
    "$\\theta$ represents the parameter <br>\n",
    "$\\theta$ can be one more variables (e.g. mean and std. dev)<br>\n",
    "\n",
    "Max $p(data |  \\theta)$ across all possible values of $\\theta$<br>\n",
    "=Max $p(X_i =x_i|\\theta)$ across all possible values of $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro's:\n",
    "- Easy to compute & Interpret<br>\n",
    "- Asymptotically Consistent (converges towards to true solution as side of data, N, increases)\n",
    "- Lowest asymptotic variance (lowest possible error)\n",
    "- Invariant: Any transformation on the real $\\theta$ can also be applied to the MLE $\\theta$\n",
    "\n",
    "Cons:<br>\n",
    "- Point estimate - no indication of how much uncertainty there is\n",
    "- $\\theta$ may not be unique - could have more than 1 solution\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Theory is concerned with making a decision based on probabilities and particularly Bayes Theorem.\n",
    "How this is applied to machine learning and classification is examined here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: What is a decision region, decision boundary?</h4>\n",
    "\n",
    "Ans: \n",
    "* Decision region - a subset of your solution space that has been labelled as one classification.\n",
    "* Decision boundary - the boundary between decision regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: Describe minimizing the risk of misclassification</h4>\n",
    "    \n",
    "For classification we can either minimize the probability of misclassification or maximize the probability of correct classification. We can model the decision in terms of Bayes theorem and then pick the classicification based on the whichever option has the lower (minimization of risk) or higher (maximization of being correct) probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Q: What is a loss function?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Also known as a cost function. \n",
    "    \n",
    "$$ \\mathbf{E}[L] = \\sum_k \\sum_j \\int_{R} L_{kj}p(x,C_k)dx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translates to: Expected Loss = Loss matrix (L_kj) * probability of it being x and each possible Class C.   This is expressed as a continuous solution space rather than discreet (so we are interested in the probability of a region, not a point) hence the integral. And we sum up this loss for all k and j (so yeah the sum of the loss in the loss matrix)\n",
    "\n",
    "In the loss matrix, k and j represent the class labels (e.g. isCancer, isNormal). \n",
    "\n",
    "<img src=\"img/lossMatrix.png\" height=\"100\" width=\"200\">\n",
    "\n",
    "Down the side it's what you say the label is, along the top it's what it actually is, and the value in the cross-section is some loss function you devise (in this case loss may be heavier if you say isNormal when its actually isCancer)\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What is Utility?</h4>\n",
    "Ans: The opposite of loss, U = -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Q: Explain Liklihood, Maximum Liklihood, and Log-likihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Liklihood:</b>\n",
    "\n",
    "Liklihood is the opposite of knowing a probability distribution and asking questions about the probability of seeing a value based on that distributions. Likilhood asks what the is the probability of seeing that distribution given the observed values.\n",
    "\n",
    "$$ L(\\theta) = p(X|\\theta) = \\prod^N_{n=1} p(X_n|\\theta) $$\n",
    "\n",
    "The liklihood of a model with parameter(s) $\\theta$ = the probability of seeing the data sample X given $\\theta$ = the probability of all x given $\\theta$ multiplied together (assuming each x is independent)\n",
    "\n",
    "\n",
    "<b>Log-liklihood</b>\n",
    "\n",
    "For computational reasons its better to work with the log-liklihood.\n",
    "\n",
    "$$ ln \\, L(\\theta) = \\sum^N_{n=1} ln\\,p(X_n|\\theta) $$\n",
    "\n",
    "Note: By using log you can move from a 'product of' equation to a 'sum of' equation as <a href=\"https://people.richland.edu/james/lecture/m116/logs/properties.html\">\"the log of a product is the sum of the logs\"</a>\n",
    "\n",
    "<b> Maximum Liklihood Estimation</b>\n",
    "\n",
    "Think back to generative models where you are trying to build an internal view of an unknown external world  based on your observations. This is what Maximum liklihood estimation (MLE) seeks to do. It is a way of determining the parameter(s) $\\theta$ of whatever model you assume the external world to be, so as to maximize the chances of the values X being observed.\n",
    "\n",
    "$$ \\hat{\\theta} _{MLE} = \\underset{\\theta}{\\arg\\max} \\sum\\limits_{i=1}^n \\log f(x_i|\\theta)  $$\n",
    "\n",
    "Maximizing the log-liklihood is done by actually minimizing the negative log-liklihood  $- \\sum^N_{n=1} ln\\,p(X_n|\\theta) $ \n",
    "\n",
    "\n",
    "To find the minimum of a function we need to find the point at which the gradient is 0.\n",
    "\n",
    "<b> Maximum Liklihood Estimation for a Gaussian distribution</b>\n",
    "\n",
    "If we suspect the external model to be a Gaussian distribution then the process of determining the parameters gets simplified to:\n",
    "\n",
    "mean = $ \\frac1 N * \\sum^N_{n=1} x_n$ - i.e. the mean of your sample\n",
    "\n",
    "variance = $ \\frac1 N * \\sum^N_{n=1} (x_n - \\hat\\mu)^2$ - i.e. the variance of your sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Good video: https://www.youtube.com/watch?v=TaotW-u6eys\n",
    "\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Imagine you observe two samples of data that come from two Gaussian distributions. You then recieve a new data point. How would you use Maximum Liklihood Estimation to determine which of the two underyling classess the data point belongs to?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Assign based on whether $ P(x | \\mu_1^* \\sum_1^*) > P(x | \\mu_0^* \\sum_0^*)$\n",
    "\n",
    "i.e. If the probabiloty of x happening given class 1 is greater than the probability of x happening given class 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Q: What are the two main philosophies to predicting the class of something given x, (p|x)?</h4>\n",
    "\n",
    "Ans: Empirical Risk Empirical distribution and Bayesian Decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: Explain zero-one loss utility</h4>\n",
    "Ans: Measuring the prediction performance based on the count of correct predictions. In the case of 1 class, Sum of (If correct = 1 else 0). For 2 classes, Sum of (If correct then 1 or 2 depending on which class it was, else 0).\n",
    "\n",
    "For j classes:\n",
    "\n",
    "$$ U(c* = j) = \\sum_i{U}ij p(c^{true} = i|x^*) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><h1>Data science techniques</h1></p>\n",
    "<p><strong>Source: http://www.datasciencecentral.com/profiles/blogs/40-techniques-used-by-data-scientists</strong></p>\n",
    "\n",
    "<ol>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Linear+Regression\">Linear Regression</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Logistic+regression\">Logistic Regression</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Jackknife+Regression\">Jackknife Regression</a>&nbsp;*</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Density+Estimation\">Density Estimation</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Confidence+Interval\">Confidence Interval</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Test+of+Hypotheses\">Test of Hypotheses</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Pattern+Recognition\">Pattern Recognition</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Clustering\">Clustering&nbsp;</a>- (aka Unsupervised Learning)</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Supervised+Learning\">Supervised Learning</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Time+series\">Time Series</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Decision+Trees\">Decision Trees</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Random+Numbers\">Random Numbers</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Monte+Carlo+Simulation\">Monte-Carlo Simulation</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Bayesian+Statistics\">Bayesian Statistics</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Naive+Bayes\">Naive Bayes</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=pcs\">Principal Component Analysis&nbsp;</a>- (PCA)</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Ensembles\">Ensembles</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Neural+Networks\">Neural Networks</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=svm\">Support Vector Machine&nbsp;</a>- (SVM)</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Nearest+Neighbors\">Nearest Neighbors&nbsp;</a>- (k-NN)</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Feature+Selection\">Feature Selection&nbsp;</a>- (aka Variable Reduction)</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Indexation\">Indexation / Cataloguing</a>&nbsp;*</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Spatial+Modeling\">(Geo-) Spatial Modeling</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Recommendation+Engine\">Recommendation Engine</a>&nbsp;*</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Search+Engine\">Search Engine</a>&nbsp;*</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Attribution+Modeling\">Attribution Modeling</a>&nbsp;*</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Collaborative+Filtering\">Collaborative Filtering</a>&nbsp;*</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Rule+System\">Rule System</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Linkage+Analysis\">Linkage Analysis</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Association+Rules\">Association Rules</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=scoring\">Scoring Engine</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Segmentation\">Segmentation</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Predictive+Modeling\">Predictive Modeling</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=graph\">Graphs</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Deep+Learning\">Deep Learning</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Game+Theory\">Game Theory</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=imputation\">Imputation</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Survival+Analysis\">Survival Analysis</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=arbitrage\">Arbitrage</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=Lift+Modeling\">Lift Modeling</a>&nbsp;</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=yield+optimization\" target=\"_blank\">Yield Optimization</a></li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=cross+validation\" target=\"_blank\">Cross-Validation</a></li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=model+fitting\" target=\"_blank\">Model Fitting</a></li>\n",
    "<li><a href=\"http://www.analyticbridge.com/profiles/blogs/online-advertising-a-solution-to-optimize-ad-relevancy\" target=\"_blank\">Relevancy Algorithm</a>&nbsp;*</li>\n",
    "<li><a href=\"http://www.datasciencecentral.com/page/search?q=experimental+design\" target=\"_blank\">Experimental Design</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Science Applications</h1>\n",
    "<p><strong>Source: <a href=\"http://www.datasciencecentral.com/profiles/blogs/20-data-science-systems-used-by-amazon-to-operate-its-business\">http://www.datasciencecentral.com/profiles/blogs/20-data-science-systems-used-by-amazon-to-operate-its-business</a></strong></p>\n",
    "<p>&nbsp;</p>\n",
    "<ol>\n",
    "<li><strong>Supply chain optimization (I).&nbsp;</strong>Sites selection for warehouses to minimize distribution costs (proximity to vendors, balanced against proximity to consumers). How many warehouses are needed, and what capacity each of them should have.</li>\n",
    "<li><strong>Supply chain optimization (II)</strong>. Selection of optimal routes, schedules, and products groupings, to minimize delivery costs (using graph theory)</li>\n",
    "<li><strong>Supply chain optimization (III).&nbsp;</strong>Optimization of gas purchases for delivery trucks.</li>\n",
    "<li><strong>Supply chain optimization (IV)</strong><strong>.&nbsp;</strong>Minimize time spent by drivers in traffic jams (requires traffic prediction) while optimizing delivery speed, gas usage and other factors (better be stuck 20 minutes in a traffic jam than a costly detour, or departing later?)</li>\n",
    "<li><strong>Pricing and profit optimization</strong> (per-product <em>price elasticity</em> studies needed; may require products to be aggregated in categories, to create buckets that yield statistical significance)</li>\n",
    "<li><strong>Fraud detection</strong> for credit card transactions (use <a href=\"http://www.datasciencecentral.com/profiles/blogs/hidden-decision-trees-revisited\" target=\"_blank\">decision tree</a>&nbsp;methods). Also detect criminal behavior <a href=\"http://www.datasciencecentral.com/forum/topics/getting-blacklisted-for-hosting-a-public-analytic-service-on-aws\" target=\"_blank\">taking place on AWS</a>&nbsp;for instance. Detect system intrusions and hacking attempts (to prevent stealing data such as credit card data, or prevent employee ID theft, or other malicious activity).&nbsp;</li>\n",
    "<li><strong>Fake reviews detection</strong>. They still have tons of progress to make in this area: at least categorizing users would be a first step, so that buyers know what kind of user produced a specific review; then <a href=\"http://www.analyticbridge.com/profiles/blogs/online-advertising-a-solution-to-optimize-ad-relevancy\" target=\"_blank\">relevancy algorithms</a>&nbsp;must be used to assess how relevant a review is for a specific product, knowing that most <em>likes</em> and <em>stars</em> assigned by users are biased - partly because most <em>normal</em> people don't have time or interest to write a review. Indeed, fake reviews <a href=\"http://www.datasciencecentral.com/profiles/blogs/could-fake-reviews-kill-amazon\" target=\"_blank\">is a lucrative business</a>&nbsp;taking advantages of inefficiencies in platforms such as Amazon. The best solution is to remove user-generated reviews and replace them, for each product, by number of sales over the last 30 days.</li>\n",
    "<li><strong>Taxonomy creation</strong> to categorize products, produce and maintain great catalogs, and help with user searches: this is a gigantic clustering problem, that can be done efficiently using&nbsp;<a href=\"http://www.datasciencecentral.com/page/search?q=indexation\" target=\"_blank\">tagging and indexing algorithms</a></li>\n",
    "<li>Smart <strong>search engine technology</strong> (based also on taxonomy discussed above) to help users find what they want to buy quickly</li>\n",
    "<li><strong>Multivariate testing</strong>, for instance to find out which version of a search engine increases sales, everything else being constant</li>\n",
    "<li><strong>Recommendation engine</strong> (and detection of artificial purchases aimed at fooling these algorithms)</li>\n",
    "<li><strong>Customer segmentation</strong>, churn analysis, using survival analysis models, to increase marketing and advertising efficiency</li>\n",
    "<li><strong>Advertising optimization</strong>, including automated bidding on Google Adwords for millions of keywords in real time, most having no historical data (use <a href=\"http://www.datasciencecentral.com/page/search?q=bucketisation\" target=\"_blank\">bucketasition</a>&nbsp; techniques to group keywords in buckets that have real predictive power); algorithms to identify millions of keywords worth purchasing, based on expected yield. Advertising mix optimization and <a href=\"http://www.datasciencecentral.com/forum/topics/attribution-modeling-key-concept\" target=\"_blank\">attribution modeling</a>. SEO and SEM.</li>\n",
    "<li><strong>Inventory forecasting</strong>: how many copies of each product should they keep at anytime in any warehouse, to optimize &nbsp;a few metrics (profit, product decay if perishable, delivery time etc.)</li>\n",
    "<li><strong>Sales forecasting</strong> broken down by category / location based on tons of factors that need to be identified first, using <a href=\"http://www.datasciencecentral.com/profiles/blogs/feature-selection-based-on-predictive-power\" target=\"_blank\">feature selection algorithms</a>&nbsp;(including economic forecasts; requires time series techniques)&nbsp;</li>\n",
    "<li><strong>HR analytics</strong>: who to hire, how to score candidates to better predict who will succeed; detect employees at risk of leaving or committing fraud; optimize purchase of office supplies; optimize employee compensation given several market constraints; optimize travel expenses&nbsp;</li>\n",
    "<li><strong>Real estate analytics</strong></li>\n",
    "<li><strong>Software/hardware system analytics</strong>: minimizing/predicting server crashes, optimizing redundancy with budget constraints, optimizing load balance and bandwidth usage; how many servers must be purchased, how frequently should they be replaced. Also, create email alert systems, automatically prioritize messages and select recipients. Also manage external email campaigns (delivery rate, open and click rate optimization).</li>\n",
    "<li><strong>Payments analytics</strong>. Optimization of payments: to authors, vendors, publishers, while maximizing profits and minimizing publisher / author / vendor churn; vendor and publisher selection algorithms.</li>\n",
    "<li><strong>Competitive analysis</strong>: automatically process billions of comments posted by users on social networks about Amazon, its competitors, and new trends; summarize this data, take actions based on the insights derived from this daily / hourly / real-time, automated analyses</li>\n",
    "<li><strong>Tax engineering&nbsp;</strong></li>\n",
    "<li><strong>Ad Relevancy Algorithm</strong> to select and rank Ads to be displayed on a particular webpage to a particular visitor, to maximize some yield metrics (click through rate): <a href=\"http://www.analyticbridge.com/profiles/blogs/online-advertising-a-solution-to-optimize-ad-relevancy\" target=\"_blank\">click here</a>&nbsp;to see how it works.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data science overlaps with</h1>\n",
    "<ul>\n",
    "<li>Computer science: computational complexity, <a href=\"http://www.datasciencecentral.com/group/research/forum/topics/internet-topology-mapping\" target=\"_blank\">Internet topology</a> and graph theory, distributed architectures such as <a href=\"http://www.hadoop360.com/\" target=\"_blank\">Hadoop</a>, data plumbing (optimization of data flows and in-memory analytics), data compression, computer programming (Python, Perl, R) and processing sensor and streaming data (to design cars that drive automatically)</li>\n",
    "<li>Statistics: design of experiments including multivariate testing, cross-validation, stochastic processes, sampling, <a href=\"http://www.analyticbridge.com/profiles/blogs/how-to-build-simple-accurate-data-driven-model-free-confidence-in\" target=\"_blank\">model-free confidence intervals</a>,&nbsp;but not <a href=\"http://www.datasciencecentral.com/profiles/blogs/p-values-the-gold-standard-of-statistical-validity-are-not-as\" target=\"_blank\">p-value</a>&nbsp;nor obscure tests of thypotheses that are subjects to <a href=\"http://www.datasciencecentral.com/group/research/forum/topics/how-to-detect-spurious-correlations-and-how-to-find-the-real-ones\" target=\"_blank\">the curse of big data</a>&nbsp;</li>\n",
    "<li>Machine learning and data mining: data science indeed fully encompasses these two domains.</li>\n",
    "<li>Operations research: data science encompasses most of operations research as well as any techniques aimed at optimizing decisions based on analysing data.&nbsp;</li>\n",
    "<li>Business intelligence: every BI aspect of designing/creating/identifying great metrics and KPI's, creating database schemas (be it NoSQL or not), dashboard design and visuals, and data-driven strategies to optimize decisions and ROI, is data science.</li>\n",
    "</ul>\n",
    "<p><strong>Comparison with other analytic discplines</strong></p>\n",
    "<ul>\n",
    "<li><strong>Machine learning</strong>: Very popular computer science discipline,&nbsp;data-intensive,&nbsp;part of data science and closely related to data mining. Machine learning is about designing algorithms (like data mining), but emphasis is on prototyping algorithms for production mode, and designing automated systems (bidding algorithms, ad targeting algorithms) that automatically update themselves,&nbsp;constantly train/retrain/update training sets/cross-validate, and refine or discover new rules (fraud detection) on a daily basis. Python is now a popular language for ML development. Core algorithms include clustering and supervised classification, rule systems, and scoring techniques. A sub-domain, close to Artificial Intelligence (see entry below) is deep learning.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Data mining</strong>: This discipline is about designing algorithms to extract insights from rather large and potentially unstructured data (text mining), sometimes called nugget discovery, for instance unearthing a massive Botnets after looking at 50 million rows of data.Techniques include pattern recognition,&nbsp;feature selection, clustering, supervised classification and encompasses a few statistical techniques (though without the p-values or confidence intervals attached to most statistical methods being used). Instead, emphasis is on robust, data-driven, scalable techniques, without much interest in discovering causes or interpretability. Data mining thus have some intersection with statistics, and it is a subset of data science. Data mining is applied computer engineering, rather than a mathematical science. Data miners use open source and software such as Rapid Miner.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Predictive modeling</strong>: Not a discipline per se. Predictive modeling projects occur in all industries across all disciplines. Predictive modeling applications aim at predicting future based on past data, usually but not always based on statistical modeling. Predictions often come with confidence intervals. Roots of predictive modeling are in statistical science.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Statistics</strong>. Currently, statistics is mostly about surveys (typically performed with SPSS software), theoretical academic research, bank and insurance analytics (marketing mix optimization, cross-selling, fraud detection, usually with SAS and R), statistical programming, social sciences, global warming research (and space weather modeling), economic research,&nbsp;clinical trials (pharmaceutical industry), medical statistics, epidemiology, biostatistics.and government statistics. Agencies hiring statisticians include the Census Bureau, IRS, CDC, EPA, BLS, SEC, and EPA (environmental/spatial statistics). Jobs requiring a security clearance are well paid and relatively secure, but the well paid jobs in the pharmaceutical industry (the golden goose for statisticians) are threatened by a number of factors - outsourcing, company mergings, and pressures to make healthcare affordable. Because of the big influence of the conservative, risk-adverse pharmaceutical industry, statistics has become a narrow field not adapting to new data, and not innovating, loosing ground to data science, industrial statistics, operations research, data mining, machine learning -- where the same clustering, cross-validation and statistical training techniques are used, albeit in a more automated way and on bigger data. Many professionals who were called statisticians 10 years ago, have seen their job title changed to data scientist or analyst in the last few years. Modern sub-domains include statistical computing, statistical learning (closer to machine learning), computational statistics (closer to data science),&nbsp;data-driven (model-free) inference, sport statistics, and Bayesian statistics (MCMC, Bayesian networks and hierarchical Bayesian models&nbsp;being popular, modern techniques). Other new techniques include SVM, structural equation modeling, predicting election results, and ensemble models.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Industrial statistics</strong>. Statistics frequently performed by non-statisticians (engineers with good statistical training), working on engineering projects such as yield optimization or load balancing (system analysts). They use very applied statistics, and their framework is closer to six sigma, quality control and operations research, than to traditional statistics.&nbsp;Also found in oil and manufacturing industries. Techniques used include time series, ANOVA, experimental design, survival analysis, signal processing (filtering, noise removal, deconvolution), spatial models, simulation, Markov chains, risk and reliability models.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Mathematical optimization</strong>. Solves business optimization problems with techniques such as the simplex algorithm, Fourier transforms (signal processing), differential equations, and software such as Matlab. These applied mathematicians are found in big companies such as IBM, research labs, NSA (cryptography) and in the finance industry (sometimes recruiting physics or engineer graduates). These professionals sometimes solve the exact same problems as statisticians do, using the exact same techniques, though they use different names. Mathematicians use least square optimization for interpolation or extrapolation;&nbsp;statisticians use linear regression for predictions and model fitting, but both concepts are identical, and rely on the exact same mathematical machinery: it's just two names describing the same thing. Mathematical optimization is however closer to operations research than statistics, the choice of hiring a mathematician rather than another practitioner (data scientist) is often dictated by historical reasons, especially for organizations such as NSA or IBM.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Actuarial sciences</strong>. Just a subset of statistics focusing on insurance (car, health, etc.) using survival models: predicting when you will die, what your health expenditures&nbsp;will be based on your health status (smoker, gender, previous diseases)&nbsp;to determine your insurance premiums. Also predicts extreme floods and weather events to determine premiums. These latter models are notoriously erroneous (recently) and have resulted in far bigger payouts than expected. For some reasons, this is a very vibrant, secretive community of statisticians, that do not call themselves statisticians anymore (job title is actuary). They have seen their average salary increase nicely over time: access to profession is restricted and regulated just like for lawyers, for no other reasons than protectionism to boost salaries and reduce the number of qualified applicants to job openings. Actuarial sciences is indeed data science (a sub-domain).</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>HPC</strong>. High performance computing, not a discipline per se, but should be of concern to data scientists, big data practitioners, computer scientists and mathematicians, as it can redefine the computing paradigms in these fields. If quantum computing ever becomes successful, it will totally change the way algorithms are designed and implemented. HPC should not be confused with Hadoop and Map-Reduce: HPC is hardware-related, Hadoop is software-related (though heavily relying on Internet bandwidth and servers configuration and proximity).</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Operations research</strong>. Abbreviated as OR. They separated from statistics a while back (like 20 years ago), but they are like twin brothers, and their respective organizations (INFORMS and ASA) partner together. OR is about decision science and optimizing traditional business projects: inventory management, supply chain, pricing. They heavily use Markov Chain models, Monter-Carlo simulations, queuing and graph theory, and software such as AIMS, Matlab or Informatica. Big, traditional old companies use OR, new and small ones (start-ups) use data science to handle pricing, inventory management or supply chain problems. Many operations research analysts are becoming data scientists, as there is far more innovation and thus growth prospect in data science, compared to OR. Also, OR problems can be solved by data science. OR has a siginficant overlap with six-sigma (see below), also solves econometric problems, and has many practitioners/applications in the army and defense sectors.&nbsp;car traffic optimization is a modern example of OR problem, solved with simulations, commuter surveys, sensor data and statistical modeling.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Six sigma</strong>. It's more a way of thinking (a business philosophy, if not a cult) rather than a discipline, and was heavily promoted by Motorola and GE a few decades ago. Used for quality control and to optimize engineering processes (see entry on industrial statistics in this article), by large, traditional companies. They have a LinkedIn group with 270,000 members, twice as large as any other analytic LinkedIn groups including our data science group. Their motto is simple: focus your efforts on the 20% of your time that yields 80% of the value. Applied, simple statistics are used (simple stuff works must of the time, I agree), and the idea is to eliminate sources of variances in business processes, to make them more predictable and improve quality. Many people consider six sigma to be old stuff that will disappear. Perhaps, but the fondamental concepts are solid and will remain: these are also fundamental concepts for all data scientists. You could say that six sigma is a much more simple if not simplistic version of operations research (see above entry), where statistical modeling is kept to a minimum. Risks: non qualified people use non-robust black-box statistical tools to solve problems, it can result in disasters. In some ways, six sigma is a discipline more suited for business analysts (see business intelligence entry below) than for serious statisticians.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Quant</strong>. Quant people are just data scientists working for Wall Street on problems such as high frequency trading or <a href=\"http://www.analyticbridge.com/profiles/blogs/new-pattern-to-predict-stock-prices-multiplies-return-by-factor-5\" target=\"_blank\">stock market arbitraging</a>. They use C++, Matlab, and come from prestigious universities, earn big bucks but lose their job right away when ROI goes too South too quickly. They can also be employed in energy trading. Many who were fired during the great recession now work on problems such as click arbitraging, ad optimization and keyword bidding. Quants have backgrounds in statistics (few of them), mathematical optimization, and industrial statistics.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Artificial intelligence</strong>. It's coming back. The intersection with data science is pattern recognition (image analysis) and the design of automated (some would say intelligent) systems to perform various tasks, in machine-to-machine communication mode, such as identifying the right keywords (and right bid) on Google AdWords (pay-per-click campaigns involving millions of keywords per day). I also consider smart search (creating a search engine returning the results that you expect and being much broader than Google) one of the greatest problems in data science, arguably also an AI and machine learning problem.&nbsp;An old AI technique is neural networks, but it is now loosing popularity. To the contrary, neuroscience is gaining popularity.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Computer science</strong>. Data science has some overlap with computer science: Hadoop and Map-Reduce implementations, algorithmic and computational complexity to design fast, scalable algorithms, data plumbing, and problems such as <a href=\"http://www.datasciencecentral.com/group/research/forum/topics/internet-topology-mapping\" target=\"_blank\">Internet topology mapping</a>, <a href=\"http://www.analyticbridge.com/forum/topics/challenge-of-the-week-random-numbers\" target=\"_blank\">random number generation</a>, <a href=\"http://www.analyticbridge.com/group/codesnippets/forum/topics/credit-card-number-and-password-encoder-decoder\" target=\"_blank\">encryption</a>, <a href=\"http://www.analyticbridge.com/forum/topics/challenge-of-the-week-may-20\" target=\"_blank\">data compression</a>, and <a href=\"http://www.datasciencecentral.com/profiles/blogs/interesting-data-science-application-steganography\" target=\"_blank\">steganography</a>&nbsp;(though these problems overlap with statistical science and mathematical optimization as well).</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Econometrics</strong>. Why it became separated from statistics is unclear. So many branches disconnected themselves from statistics, as they became less generic and start developing their own ad-hoc tools. But in short, econometrics is heavily statistical in nature, using time series models such as auto-regressive processes. Also overlapping with operations research (itself overlaping with statistics!) and mathematical optimization (simplex algorithm). Econometricians like ROC and efficiency curves (so do six sigma practitioners, see corresponding entry in this article). Many do not have a strong statistical background, and Excel is their main or only tool.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Data engineering</strong>. Performed by software engineers (developers) or architects (designers) in large organizations (sometimes by data scientists in tiny companies), this is the applied part of computer science (see entry in this article), to power systems that allow all sorts of data to be easily processed in-memory or near-memory, and to flow nicely to (and between) end-users, including heavy data consumers such as data scientists. A sub-domain currently under attack is data warehousing, as this term is associated with static, siloed &nbsp;<a href=\"http://www.datasciencecentral.com/profiles/blogs/11-features-any-database-sql-or-nosql-should-have\" target=\"_blank\">conventational data bases</a>, data architectures, and data flows, threatened by the rise of NoSQL, NewSQL and graph databases. Transforming these old architectures into new ones (only when needed) or make them compatible with new ones, is a lucrative business.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Business intelligence</strong>. Abbreviated as BI. Focuses on dashboard creation, metric selection, producing and scheduling data reports (statistical summaries) sent by email or delivered/presented to executives, competitive intelligence (analyzing third party data), as well as involvement in database schema design (working with data architects) to collect useful, actionable business data efficiently. Typical job title is business analyst, but some are more involved with marketing, product or finance (forecasting sales and revenue). They typically have an MBA degree. Some have learned advanced statistics <a href=\"http://www.analyticbridge.com/forum/topics/challenge-of-the-week-time-series\" target=\"_blank\">such as time series</a>, but most only use (and need) basic stats, and light analytics, relying on IT to maintain databases and harvest data. They use tools such as Excel (including cubes and pivot tables, but not advanced analytics), Brio (Oracle browser client), Birt, Micro-Sreategy or Business Objects (as end-users to run queries), though some of these tools are increasingly equipped with better analytic capabilities. Unless they learn how to code, they are competing with some polyvalent data scientists that excel in decision science, insights extraction and presentation (visualization), KPI design, business consulting, and ROI/yield/business/process optimization. BI and market research (but not competitive intelligence) are currently experiencing a decline, while AI is experiencing a come-back. This could be cyclical. Part of the decline is due to not adapting to new types of data (e.g. unstructured text) that require engineering or data science techniques to process and extract value.</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Data analysis</strong>. This is the new term for business statistics since at least 1995, and it covers a large spectrum of applications including fraud detection, advertising mix modeling, attribution modeling, sales forecasts, cross-selling optimization (retails), user segmentation, churn analysis, computing long-time value of a customer and cost of acquisition, and so on. Except in big companies, data analyst is a junior role; these practitioners have a much more narrow knwoledge and experience than data scientists, and they lack (and don't need) business vision. They are detail-orirented and report to managers such as data scientists or director of analytics, In big companies, someone with a job title such as data analyst III might be very senior, yet they usually are specialized and lack the broad knowledge gained by data scientists working in a variety of companies large and small.&nbsp;</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<ul>\n",
    "<li><strong>Business analytics</strong>. Same as data analysis, but restricted to business problems only. Tends to have a bit more of a finacial, marketing or ROI flavor. Popular job titles include data analyst and data scientist, but not business analyst (see business intelligence entry for business intelligence, a different domain).</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "<p>Finally, there are more specialized analytic disciplines that recently emerged: health analytics, computational chemistry and bioinformatics &nbsp;(genome research), for instance.</p>\n",
    "<li></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span class=\"font-size-2\">Data Science Interview Questions</span></h1>\n",
    "<ol>\n",
    "<li><span class=\"font-size-2\">What is the biggest data set that you processed, and how did you process it, what were the results?</span></li>\n",
    "<li><span class=\"font-size-2\">Tell me two success stories about your analytic or computer science projects? How was lift (or success) measured?</span></li>\n",
    "<li><span class=\"font-size-2\">What is: lift, KPI, robustness, model fitting, design of experiments, 80/20 rule?</span></li>\n",
    "<li><span class=\"font-size-2\">What is: collaborative filtering, n-grams, map reduce, cosine distance?</span></li>\n",
    "<li><span class=\"font-size-2\">How to optimize a web crawler to run much faster, extract better information, and better summarize data to produce cleaner databases?</span></li>\n",
    "<li><span class=\"font-size-2\">How would you come up with a solution to identify plagiarism?</span></li>\n",
    "<li><span class=\"font-size-2\">How to detect individual paid accounts shared by multiple users?</span></li>\n",
    "<li><span class=\"font-size-2\">Should click data be handled in real time? Why? In which contexts?</span></li>\n",
    "<li><span class=\"font-size-2\">What is better: good data or good models? And how do you define \"good\"? Is there a universal good model? Are there any models that are definitely not so good?</span></li>\n",
    "<li><span class=\"font-size-2\">What is probabilistic merging (AKA fuzzy merging)? Is it easier to handle with SQL or other languages? Which languages would you choose for semi-structured text data reconciliation?&nbsp;</span></li>\n",
    "<li><span class=\"font-size-2\">How do you handle missing data? What imputation techniques do you recommend?</span></li>\n",
    "<li><span class=\"font-size-2\">What is your favorite programming language / vendor? why?</span></li>\n",
    "<li><span class=\"font-size-2\">Tell me 3 things positive and 3 things negative about your favorite statistical software.</span></li>\n",
    "<li><span class=\"font-size-2\">Compare SAS, R, Python, Perl</span></li>\n",
    "<li><span class=\"font-size-2\">What is the curse of big data?</span></li>\n",
    "<li><span class=\"font-size-2\">Have you been involved in database design and data modeling?</span></li>\n",
    "<li><span class=\"font-size-2\">Have you been involved in dashboard creation and metric selection? What do you think about Birt?</span></li>\n",
    "<li><span class=\"font-size-2\">What features of Teradata do you like?</span></li>\n",
    "<li><span class=\"font-size-2\">You are about to send one million email (marketing campaign). How do you optimze delivery? How do you optimize response? Can you optimize both separately? (answer: not really)</span></li>\n",
    "<li><span class=\"font-size-2\">Toad or Brio or any other similar clients are quite inefficient to query Oracle databases. Why? How would you do to increase speed by a factor 10, and be able to handle far bigger outputs?&nbsp;</span></li>\n",
    "<li><span class=\"font-size-2\">How would you turn unstructured data into structured data? Is it really necessary? Is it OK to store data as flat text files rather than in an SQL-powered RDBMS?</span></li>\n",
    "<li><span class=\"font-size-2\">What are hash table collisions? How is it avoided? How frequently does it happen?</span></li>\n",
    "<li><span class=\"font-size-2\">How to make sure a mapreduce application has good load balance? What is load balance?</span></li>\n",
    "<li><span class=\"font-size-2\">Examples where mapreduce does not work? Examples where it works very well? What are the security issues involved with the cloud? What do you think of EMC's solution offering an hybrid approach - both internal and external cloud - to mitigate the risks and offer other advantages (which ones)?</span></li>\n",
    "<li><span class=\"font-size-2\">Is it better to have 100 small hash tables or one big hash table, in memory, in terms of access speed (assuming both fit within RAM)? What do you think about in-database analytics?</span></li>\n",
    "<li><span class=\"font-size-2\">Why is naive Bayes so bad? How would you improve a spam detection algorithm that uses naive Bayes?</span></li>\n",
    "<li><span class=\"font-size-2\">Have you been working with white lists? Positive rules? (In the context of fraud or spam detection)</span></li>\n",
    "<li><span class=\"font-size-2\">What is star schema? Lookup tables?&nbsp;</span></li>\n",
    "<li><span class=\"font-size-2\">Can you perform logistic regression with Excel? (yes) How? (use linest on log-transformed data)? Would the result be good? (Excel has numerical issues, but it's very interactive)</span></li>\n",
    "<li><span class=\"font-size-2\">Have you optimized code or algorithms for speed: in SQL, Perl, C++, Python etc. How, and by how much?</span></li>\n",
    "<li><span class=\"font-size-2\">Is it better to spend 5 days developing a 90% accurate solution, or 10 days for 100% accuracy? Depends on the context?</span></li>\n",
    "<li><span class=\"font-size-2\">Define: quality assurance, six sigma, design of experiments. Give examples of good and bad designs of experiments.</span></li>\n",
    "<li><span class=\"font-size-2\">What are the drawbacks of general linear model? Are you familiar with alternatives (Lasso, ridge regression, boosted trees)?</span></li>\n",
    "<li><span class=\"font-size-2\">Do you think 50 small decision trees are better than a large one? Why?</span></li>\n",
    "<li><span class=\"font-size-2\">Is actuarial science not a branch of statistics (survival analysis)? If not, how so?</span></li>\n",
    "<li><span class=\"font-size-2\">Give examples of data that does not have a Gaussian distribution, nor log-normal. Give examples of data that has a very chaotic distribution?</span></li>\n",
    "<li><span class=\"font-size-2\">Why is mean square error a bad measure of model performance? What would you suggest instead?</span></li>\n",
    "<li><span class=\"font-size-2\">How can you prove that one improvement you've brought to an algorithm is really an improvement over not doing anything? Are you familiar with A/B testing?</span></li>\n",
    "<li><span class=\"font-size-2\">What is sensitivity analysis? Is it better to have low sensitivity (that is, great robustness) and low predictive power, or the other way around? How to perform good cross-validation? What do you think about the idea of injecting noise in your data set to test the sensitivity of your models?</span></li>\n",
    "<li><span class=\"font-size-2\">Compare logistic regression w. decision trees, neural networks. How have these technologies been vastly improved over the last 15 years?</span></li>\n",
    "<li><span class=\"font-size-2\">Do you know / used data reduction techniques other than PCA? What do you think of step-wise regression? What kind of step-wise techniques are you familiar with? When is full data better than reduced data or sample?</span></li>\n",
    "<li><span class=\"font-size-2\">How would you build non parametric confidence intervals, e.g. for scores? (see the&nbsp;<a href=\"http://www.analyticbridge.com/profiles/blogs/how-to-build-simple-accurate-data-driven-model-free-confidence-in\" target=\"_blank\">AnalyticBridge theorem</a>)</span></li>\n",
    "<li><span class=\"font-size-2\">Are you familiar either with extreme value theory, monte carlo simulations or mathematical statistics (or anything else) to correctly estimate the chance of a very rare event?</span></li>\n",
    "<li><span class=\"font-size-2\">What is root cause analysis? How to identify a cause vs. a correlation? Give examples.</span></li>\n",
    "<li><span class=\"font-size-2\">How would you define and measure the predictive power of a metric?</span></li>\n",
    "<li><span class=\"font-size-2\">How to detect the best rule set for a fraud detection scoring technology? How do you deal with rule redundancy, rule discovery, and the combinatorial nature of the problem (for finding optimum rule set - the one with best predictive power)? Can an approximate solution to the rule set problem be OK? How would you find an OK approximate solution? How would you decide it is good enough and stop looking for a better one?</span></li>\n",
    "<li><span class=\"font-size-2\">How to create a keyword taxonomy?</span></li>\n",
    "<li><span class=\"font-size-2\">What is a Botnet? How can it be detected?</span></li>\n",
    "<li><span class=\"font-size-2\">Any experience with using API's? Programming API's? Google or Amazon API's? AaaS (Analytics as a service)?</span></li>\n",
    "<li><span class=\"font-size-2\">When is it better to write your own code than using a data science software package?</span></li>\n",
    "<li><span class=\"font-size-2\">Which tools do you use for visualization? What do you think of Tableau? R? SAS? (for graphs). How to efficiently represent 5 dimension in a chart (or in a video)?</span></li>\n",
    "<li><span class=\"font-size-2\">What is POC (proof of concept)?</span></li>\n",
    "<li><span class=\"font-size-2\">What types of clients have you been working with: internal, external, sales / finance / marketing / IT people? Consulting experience? Dealing with vendors, including vendor selection and testing?</span></li>\n",
    "<li><span class=\"font-size-2\">Are you familiar with software life cycle? With IT project life cycle - from gathering requests to maintenance?&nbsp;</span></li>\n",
    "<li><span class=\"font-size-2\">What is a cron job?&nbsp;</span></li>\n",
    "<li><span class=\"font-size-2\">Are you a lone coder? A production guy (developer)? Or a designer (architect)?</span></li>\n",
    "<li><span class=\"font-size-2\">Is it better to have too many false positives, or too many false negatives?</span></li>\n",
    "<li><span class=\"font-size-2\">Are you familiar with pricing optimization, price elasticity, inventory management, competitive intelligence? Give examples.&nbsp;</span></li>\n",
    "<li><span class=\"font-size-2\">How does Zillow's algorithm work? (to estimate the value of any home in US)</span></li>\n",
    "<li><span class=\"font-size-2\">How to detect bogus reviews, or bogus Facebook accounts used for bad purposes?</span></li>\n",
    "<li><span class=\"font-size-2\">How would you create a new anonymous digital currency?</span></li>\n",
    "<li><span class=\"font-size-2\">Have you ever thought about creating a startup? Around which idea / concept?</span></li>\n",
    "<li><span class=\"font-size-2\">Do you think that typed login / password will disappear? How could they be replaced?</span></li>\n",
    "<li><span class=\"font-size-2\">Have you used time series models? Cross-correlations with time lags? Correlograms? Spectral analysis? Signal processing and filtering techniques? In which context?</span></li>\n",
    "<li><span class=\"font-size-2\">Which data scientists do you admire most? which startups?</span></li>\n",
    "<li><span class=\"font-size-2\">How did you become interested in data science?</span></li>\n",
    "<li><span class=\"font-size-2\">What is an efficiency curve? What are its drawbacks, and how can they be overcome?</span></li>\n",
    "<li><span class=\"font-size-2\">What is a recommendation engine? How does it work?</span></li>\n",
    "<li><span class=\"font-size-2\">What is an exact test? How and when can simulations help us when we do not use an exact test?</span></li>\n",
    "<li><span class=\"font-size-2\">What do you think makes a good data scientist?</span></li>\n",
    "<li><span class=\"font-size-2\">Do you think data science is an art or a science?</span></li>\n",
    "<li><span class=\"font-size-2\">What is the computational complexity of a good, fast clustering algorithm? What is a good clustering algorithm? How do you determine the number of clusters? How would you perform clustering on one million unique keywords, assuming you have 10 million data points - each one consisting of two keywords, and a metric measuring how similar these two keywords are? How would you create this 10 million data points table in the first place?</span></li>\n",
    "<li><span class=\"font-size-2\">Give a few examples of \"best practices\" in data science.</span></li>\n",
    "<li><span class=\"font-size-2\">What could make a chart misleading, difficult to read or interpret? What features should a useful chart have?</span></li>\n",
    "<li><span class=\"font-size-2\">Do you know a few \"rules of thumb\" used in statistical or computer science? Or in business analytics?</span></li>\n",
    "<li><span class=\"font-size-2\">What are your top 5 predictions for the next 20 years?</span></li>\n",
    "<li><span class=\"font-size-2\">How do you immediately know when statistics published in an article (e.g. newspaper) are either wrong or presented to support the author's point of view, rather than correct, comprehensive factual information on a specific subject? For instance, what do you think about the official monthly unemployment statistics regularly discussed in the press? What could make them more accurate?</span></li>\n",
    "<li><span class=\"font-size-2\">Testing your analytic intuition: <a href=\"http://www.analyticbridge.com/profiles/blogs/how-to-detect-a-pattern-problem-and-solution\" target=\"_blank\">look at these three charts</a>. Two of them exhibit patterns. Which ones? Do you know that these charts are called scatter-plots? Are there other ways to visually represent this type of data?</span></li>\n",
    "<li><span class=\"font-size-2\">You design a robust non-parametric statistic (metric) to replace correlation or R square, that (1) is independent of sample size, (2) always between -1 and +1, and (3) based on rank statistics. How do you normalize for sample size? Write an algorithm that computes all permutations of n elements. How do you sample permutations (that is, generate tons of <em>random</em> permutations) when n is large, to estimate the asymptotic distribution for your newly created metric? You may use this asymptotic distribution for normalizing your metric. Do you think that an exact theoretical distribution might exist, and therefore, we should find it, and use it rather than wasting our time trying to estimate the asymptotic distribution using simulations?&nbsp;</span></li>\n",
    "<li><span class=\"font-size-2\">More difficult, technical question related to previous one. There is an obvious one-to-one correspondence between permutations of n elements and integers between 1 and n! Design an algorithm that encodes an integer less than n! as a permutation of n elements. What would be the reverse algorithm, used to decode a permutation and transform it back into a number? <strong>Hint</strong>: An intermediate step is to use the <a href=\"http://en.wikipedia.org/wiki/Factorial_number_system\" target=\"_blank\">factorial number system</a>&nbsp;representation of an integer. Feel free to check this reference online to answer the question. Even better, feel free to browse the web to find the full answer to the question (this will test the candidate's ability to quickly search online and find a solution to a problem without spending hours reinventing the wheel). &nbsp;</span></li>\n",
    "<li><span class=\"font-size-2\">How many \"useful\" votes will a Yelp review receive? <strong>My answer</strong>:&nbsp;Eliminate bogus accounts (<a href=\"http://www.analyticbridge.com/forum/topics/how-do-you-estimate-the-proportion-of-bogus-accounts-on-facebook\" target=\"_blank\">read this article</a>), or competitor reviews (how to detect them: use taxonomy to classify users, and location - two Italian restaurants in same Zip code could badmouth each other and write great comments for themselves). Detect&nbsp;<a href=\"http://www.analyticbridge.com/profiles/blogs/invented-by-a-data-scientist-the-first-anti-scam\" target=\"_blank\">fake likes</a>: some companies (e.g.&nbsp;<a href=\"http://www.fanmenow.com/buy-twitter-followers/\" target=\"_blank\">FanMeNow.com</a>)&nbsp;will charge you to produce fake accounts and fake likes. Eliminate prolific users who like everything, those who hate everything. Have a blacklist of keywords to filter fake reviews. See if IP address or IP block of reviewer is in a blacklist such as \"Stop Forum Spam\". Create honeypot to catch fraudsters.&nbsp; Also watch out for disgruntled employees badmouthing their former employer. Watch out for 2 or 3 similar comments posted the same day by 3 users regarding a company that receives very few reviews. Is it a brand new company? Add more weight to trusted users (create a category of trusted users). &nbsp;Flag all reviews that are identical (or nearly identical) and come from same IP address or same user. Create a metric to measure distance between two pieces of text (reviews).&nbsp;<a href=\"http://www.bigdatanews.com/profiles/blogs/fast-clustering-algorithms-for-massive-datasets\" target=\"_blank\">Create a review or reviewer taxonomy</a>. Use&nbsp;<a href=\"http://www.analyticbridge.com/forum/topics/hidden-decision-trees-vs\" target=\"_blank\">hidden decision trees</a>&nbsp;to rate or score review and reviewers.</span></li>\n",
    "<li><span class=\"font-size-2\">What did you do today? Or what did you do this week / last week?</span></li>\n",
    "<li><span class=\"font-size-2\">What/when is the latest data mining book / article you read? What/when is the latest data mining conference / webinar / class / workshop / training you attended? What/when is the most recent programming skill that you acquired?</span></li>\n",
    "<li><span class=\"font-size-2\">What are your favorite data science websites? Who do you admire most in the data science community, and why? Which company do you admire most?</span></li>\n",
    "<li><span class=\"font-size-2\">What/when/where is the last data science blog post you wrote?&nbsp;</span></li>\n",
    "<li><span class=\"font-size-2\">In your opinion, what is data science? Machine learning? Data mining?</span></li>\n",
    "<li><span class=\"font-size-2\">Who are the best people you recruited and where are they today?</span></li>\n",
    "<li><span class=\"font-size-2\">Can you estimate and forecast sales for any book, based on Amazon public data? Hint: <a href=\"http://www.fonerbooks.com/surfing.htm\" target=\"_blank\">read this article</a>.</span></li>\n",
    "<li><span class=\"font-size-2\"><a href=\"http://www.analyticbridge.com/forum/topics/job-interview-question-what-is-wrong-with-this-picture\" target=\"_blank\">What's wrong with this picture?</a></span></li>\n",
    "<li><span class=\"font-size-2\">Should removing stop words be Step 1 rather than Step 3, <a href=\"http://www.datasciencecentral.com/profiles/blogs/building-better-search-tools-problems-and-solutions\" target=\"_blank\">in the search engine algorithm described here</a>? <strong>Answer</strong>: Have you thought about the fact that mine and yours could also be stop words? So in a bad implementation, data mining would become data mine after stemming, then data. In practice, you remove stop words before stemming. So Step 3 should indeed become step 1.&nbsp;</span></li>\n",
    "<li><span class=\"font-size-2\"><a href=\"http://www.analyticbridge.com/forum/topics/analytics-for-kids\" target=\"_blank\">Experimental design and a bit of computer science with Lego's</a></span></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>1. Answer from our data scientist</strong> (many of these questions are open questions):</p>\n",
    "<ol>\n",
    "<li>What is the life cycle of a data science project?</li>\n",
    "<li>How do you measure yield (over base line) resulting from a new or refined algorithm or architecture?</li>\n",
    "<li>What is cross-validation? How to do it right?</li>\n",
    "<li>Is it better to design robust or accurate algorithms?</li>\n",
    "<li>Have you written production code? Prototyped an algorithm? Created a proof of concept?</li>\n",
    "<li>What is the biggest data set you have worked with, in terms of training set size, and in terms of having your algorithm implemented in production mode to process billions of transactions per day / month / year?</li>\n",
    "<li>Name a few famous API's (for instance Google search). How would you create one?</li>\n",
    "<li>How to efficiently scrape web data, or collect tons of tweets?</li>\n",
    "<li>How to optimize algorithms (parallel processing and/or faster algorithm: provide examples for both)</li>\n",
    "<li>Examples of NoSQL architecture?</li>\n",
    "<li>How do you clean data?</li>\n",
    "<li>How do you define / select metrics? Have you designed and used compound metrics?</li>\n",
    "<li>Examples of bad and good visualizations?</li>\n",
    "<li>Have you been involved - as an adviser or architect - in the design of dashboard or alarm systems?</li>\n",
    "<li>How frequently an algorithm must be updated? What about lookup tables in real-time systems?</li>\n",
    "<li>Provide examples of machine-to-machine communication.</li>\n",
    "<li>Provide examples where you automated a repetitive analytical task.</li>\n",
    "<li>How do you assess the statistical significance of an insight?</li>\n",
    "<li>How to turn unstructured data into structured data?</li>\n",
    "<li>How to very efficiently cluster 100 billion web pages, for instance with a tagging or indexing algorithm?&nbsp;</li>\n",
    "<li>If you were interviewing a data scientist, what questions would you ask her?</li>\n",
    "</ol>\n",
    "<p><strong>2. This answer was posted on Quora by Jay Verkuilen</strong>:<a href=\"https://www.quora.com/What-are-20-questions-to-detect-fake-data-scientists\" target=\"_blank\" rel=\"nofollow\"><br /></a></p>\n",
    "<ol>\n",
    "<li>Explain what&nbsp;<em>regularization&nbsp;</em>is and why it is useful. What are the benefits and drawbacks of specific methods, such as ridge regression and LASSO?</li>\n",
    "<li>Explain what a&nbsp;<em>local optimum</em>&nbsp;is and why it is important in a specific context, such as&nbsp;<em>k</em>-means clustering. What are specific ways for determining if you have a local optimum problem? What can be done to avoid local optima?</li>\n",
    "<li>Assume you need to generate a&nbsp;<em>predictive model of a quantitative outcome variable using multiple regression</em>. Explain how you intend to validate this model.</li>\n",
    "<li>Explain what&nbsp;<em>precision&nbsp;</em>and&nbsp;<em>recall&nbsp;</em>are. How do they relate to the ROC curve?</li>\n",
    "<li>Explain what a&nbsp;<em>long tailed distribution</em>&nbsp;is and provide three examples of relevant phenomena that have long tails. Why are they important in classification and prediction problems?</li>\n",
    "<li>What is&nbsp;<em>latent semantic indexing</em>? What is it used for? What are the specific limitations of the method?</li>\n",
    "<li>What is the&nbsp;<em>Central Limit Theorem</em>? Explain it. Why is it important? When does it fail to hold?</li>\n",
    "<li>What is&nbsp;<em>statistical power</em>?</li>\n",
    "<li>Explain what&nbsp;<em>resampling methods</em>&nbsp;are and why they are useful. Also explain their limitations.</li>\n",
    "<li>Explain the differences between&nbsp;<em>artificial neural networks with softmax activation, logistic regression, and the maximum entropy classifier.</em></li>\n",
    "<li>Explain<em>&nbsp;selection bias&nbsp;</em>(with regards to a dataset, not variable selection).<em>&nbsp;</em>Why is it important? How can data management procedures such as missing data handling make it worse?</li>\n",
    "<li>Provide a simple example of how an&nbsp;<em>experimental</em>&nbsp;<em>design</em>&nbsp;can help answer a question about behavior. For instance, explain how an experimental design can be used to optimize a web page. How does experimental data contrast with observational data.</li>\n",
    "<li>Explain the difference between \"long\" and \"wide\" format data. Why would you use one or the other?</li>\n",
    "<li>Is&nbsp;<em>mean imputation of missing data</em>&nbsp;acceptable practice? Why or why not?</li>\n",
    "<li>Explain Edward Tufte's concept of \"chart junk.\"&nbsp;</li>\n",
    "<li>What is an&nbsp;<em>outlier</em>? Explain how you might screen for outliers and what you would do if you found them in your dataset. Also, explain what an&nbsp;<em>inlier</em>&nbsp;is and how you might screen for them and what you would do if you found them in your dataset.</li>\n",
    "<li>What is principal components analysis (PCA)? Explain the sorts of problems you would use PCA for. Also explain its limitations as a method.</li>\n",
    "<li>You have data on the duration of calls to a call center. Generate a plan for how you would code and analyze these data. Explain a plausible scenario for what the distribution of these durations might look like. How could you test (even graphically) whether your expectations are borne out?</li>\n",
    "<li>Explain what a&nbsp;<em>false positive&nbsp;</em>and a&nbsp;<em>false negative&nbsp;</em>are. Why is it important to differentiate these from each other? Provide examples of situations where (1) false positives are more important than false negatives, (2) false negatives are more important than false positives, and (3) these two types of errors are about equally important.</li>\n",
    "<li>Explain likely differences encountered between administrative datasets and datasets gathered from experimental studies. What are likely problems encountered with administrative data? How do experimental methods help alleviate these problems? What problems do they bring?</li>\n",
    "</ol>\n",
    "<p><strong>3. Kavita Ganesan offered this answer</strong>:</p>\n",
    "<ol>\n",
    "<li>What is a gold standard ?&nbsp;<br /> Believe it or not there are data scientists (even at very senior levels) who claim to know a hell lot about supervised machine learning and know nothing about what a gold standard is!</li>\n",
    "<li>What is the difference between supervised learning and unsupervised learning?&nbsp;- Give concrete examples.</li>\n",
    "<li>What does NLP stand for?<br /> Some data scientists claim to also do NLP.&nbsp;&nbsp;</li>\n",
    "<li>Write code to count the number of words in a document using any programming language. Now, extend this for bi-grams.<br /> I have seen a senior level data scientist who actually struggled to implement this.&nbsp;</li>\n",
    "<li>What are feature vectors?</li>\n",
    "<li>When would you use SVMs vs Random Forrest and Why?</li>\n",
    "<li>What is your definition of Big Data, and what is the largest size of data you have worked with? Did you parallelize your code?<br /> If their notion of big data is just volume - you may have a problem. Big Data is more than just volume of data. If the largest size of data they have worked with is 5MB - again you may have a problem.</li>\n",
    "<li>How do you work with large data sets?<br /> If the answer only comes out as hadoop it clearly shows that their view of solving problems is extremely narrow. Large data problems can be solved with:<br /> 1. efficient algorithms<br /> 2. multi-threaded applications<br /> 3. distributed programming<br /> 4. more...</li>\n",
    "<li>Write a mapper function to count word frequencies (even if its just pseudo code)</li>\n",
    "<li>Write a reducer function for counting word frequencies (even if its just pseudo code)</li>\n",
    "</ol>\n",
    "<li></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span class=\"font-size-3\"><strong>1. Spatial Models</strong></span></p>\n",
    "<p>Spatial dependency is the co-variation of properties within geographic space: characteristics at proximal locations appear to be correlated, either positively or negatively. Spatial dependency leads to the spatial auto-correlation&nbsp;problem in statistics since, like temporal auto-correlation, this violates standard statistical techniques that assume independence among observations</p>\n",
    "<p><span class=\"font-size-3\"><strong>2. Time Series</strong></span></p>\n",
    "<p>Methods for time series analyses may be divided into two classes: frequency-domain methods and time-domain methods. The former include spectral analysis and recently wavelet analysis; the latter include auto-correlation and cross-correlation analysis. In time domain, correlation analyses can be made in a filter-like manner using scaled correlation, thereby mitigating the need to operate in frequency domain.</p>\n",
    "<p>Additionally, time series analysis techniques may be divided into parametric and non-parametric methods. The parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using a small number of parameters (for example, using an autoregressive or moving average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process. By contrast, non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure.</p>\n",
    "<p>Methods of time series analysis may also be divided into linear and non-linear, and univariate and multivariate.</p>\n",
    "<p><strong><span class=\"font-size-3\">3. Survival Analysis</span></strong></p>\n",
    "<p>Survival analysis is a branch of statistics for analyzing the expected duration of time until one or more events happen, such as death in biological organisms and failure in mechanical systems. This topic is called reliability theory or reliability analysis in engineering, duration analysis or duration modelling in economics, and event history analysis in sociology. Survival analysis attempts to answer questions such as: what is the proportion of a population which will survive past a certain time? Of those that survive, at what rate will they die or fail? Can multiple causes of death or failure be taken into account? How do particular circumstances or characteristics increase or decrease the probability of survival? Survival models are used by actuaries and statisticians, but also by marketers designing churn and user retention models.</p>\n",
    "<p>Survival models are also used to predict time-to-event (time from becoming radicalized to turning into a terrorist, or time between when a gun is purchased and when it is used in a murder), or to model and predict decay (see section 4 <a href=\"http://www.datasciencecentral.com/profiles/blogs/top-30-dsc-blogs-based-on-new-scoring-technology\" target=\"_blank\">in this article</a>).</p>\n",
    "<p><span class=\"font-size-3\"><strong>4. Market Segmentation</strong></span></p>\n",
    "<p>Market segmentation, also called customer profiling, is a marketing strategy which involves dividing a broad target market into subsets of consumers,businesses, or countries that have, or are perceived to have, common needs, interests, and priorities, and then designing and implementing strategies to target them. Market segmentation strategies are generally used to identify and further define the target customers, and provide supporting data for marketing plan elements such as positioning to achieve certain marketing plan objectives. Businesses may develop product differentiation strategies, or an undifferentiated approach, involving specific products or product lines depending on the specific demand and attributes of the target segment.</p>\n",
    "<p><span class=\"font-size-3\"><strong>5. Recommendation Systems</strong></span></p>\n",
    "<p>Recommender systems or recommendation systems (sometimes replacing \"system\" with a synonym such as platform or engine) are a subclass of information filtering system that seek to predict the 'rating' or 'preference' that a user would give to an item.</p>\n",
    "<p><span class=\"font-size-3\"><strong>6. Association Rule Learning</strong></span></p>\n",
    "<p>Association rule learning is a method for discovering interesting relations between variables in large databases. For example, the rule { onions, potatoes } ==&gt; { burger } &nbsp;found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. In fraud detection, association rules are used to detect patterns associated with fraud. <strong>Linkage analysis</strong> is performed to identify additional fraud cases: if credit card transaction from user A was used to make a fraudulent purchase at store B, by analyzing all transactions from store B, we might find another user C with fraudulent activity.&nbsp;</p>\n",
    "<p><span class=\"font-size-3\"><strong>7. Attribution Modeling</strong></span></p>\n",
    "<p>An <a href=\"http://www.datasciencecentral.com/forum/topics/attribution-modeling-key-concept\" target=\"_blank\">attribution model</a>&nbsp;is the rule, or set of rules, that determines how credit for sales and conversions is assigned to touchpoints in conversion paths. For example, the Last Interaction model in Google Analytics assigns 100% credit to the final touchpoints (i.e., clicks) that immediately precede sales or conversions. Macro-economic models use long-term, aggregated historical data to assign, for each sale or conversion, an attribution weight to a number of channels. These models are also used for <strong>advertising mix optimization</strong>.</p>\n",
    "<p><strong><span class=\"font-size-3\">8. Scoring</span></strong></p>\n",
    "<p>Scoring model is a special kind of predictive models. Predictive models can predict defaulting on loan payments, risk of accident, client churn or attrition, or chance of buying a good. Scoring models typically use a logarithmic scale (each additional 50 points in your score reducing the risk of defaulting by 50%), and are based on logistic regression and decision trees, or a <a href=\"http://www.datasciencecentral.com/profiles/blogs/hidden-decision-trees-revisited\" target=\"_blank\">combination of multiple algorithms</a>. Scoring technology is typically applied to transactional data, sometimes in real time (credit card fraud detection, click fraud).</p>\n",
    "<p><span class=\"font-size-3\"><strong>9. Predictive Modeling</strong></span></p>\n",
    "<p>Predictive modeling leverages statistics to predict outcomes. Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place. They may also used for weather forecasting, to predict stock market prices, or to predict sales, incorporating time series or spatial models. Neural networks, linear regression, decision trees and naive Bayes are some of the techniques used for predictive modeling. They are associated with creating a training set, cross-validation, and model fitting and selection.</p>\n",
    "<p>Some predictive systems do not use statistical models, but are data-driven instead. See example <a href=\"http://www.datasciencecentral.com/profiles/blogs/black-box-confidence-intervals-excel-and-perl-implementations-det\" target=\"_blank\">here</a>.&nbsp;</p>\n",
    "<p><span class=\"font-size-3\"><strong>10. Clustering</strong></span></p>\n",
    "<p>Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, and bioinformatics.</p>\n",
    "<p>Unlike supervised classification (below), clustering does not use training sets. Though there are some hybrid implementations, called semi-supervised learning.</p>\n",
    "<p><span class=\"font-size-3\"><strong>11. Supervised Classification</strong></span></p>\n",
    "<p>Supervised classification, also called supervised learning, is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called label, class or category). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances.&nbsp;</p>\n",
    "<p>Examples, with an emphasis on big data,&nbsp;<a href=\"http://www.datasciencecentral.com/page/search?q=clustering\" target=\"_blank\">can be found on DSC</a>. Clustering algorithms are notoriously slow, though a very fast technique known as indexation or automated tagging will be described in Part II of this article.</p>\n",
    "<p><span class=\"font-size-3\"><strong>12. Extreme Value Theory</strong></span></p>\n",
    "<p>Extreme value theory or extreme value analysis (EVA) is a branch of statistics dealing with the extreme deviations from the median of probability distributions. It seeks to assess, from a given ordered sample of a given random variable, the probability of events that are more extreme than any previously observed. For instance, floods that occur once every 10, 100, or 500 years. These models have been performing poorly recently, to predict catastrophic events, resulting in massive losses for insurance companies. I prefer Monte-Carlo simulations, especially if your training data is very large. This will be described in Part II of this article.</p>\n",
    "<li></li>\n",
    "<p><strong>13. Simulations</strong></p>\n",
    "<p>Monte-Carlo simulations are used in many contexts: to produce high quality pseudo-random numbers, in complex settings such as multi-layer spatio-temporal hierarchical Bayesian models, to estimate parameters (see picture below), to compute statistics associated with very rare events, or even to generate large amount of data (for instance cross and auto-correlated time series) to test and compare various algorithms, especially for stock trading or in engineering.</p>\n",
    "<p><strong>14. Churn Analysis</strong></p>\n",
    "<p>Customer&nbsp;churn analysis&nbsp;helps you identify and focus on higher value customers, determine what actions typically precede a lost customer or sale, and better understand what factors influence customer retention. Statistical techniques involved include survival analysis (see Part I of this article) as well as Markov chains with four states: brand new customer, returning customer, inactive (lost) customer, and re-acquired customer, along with path analysis (including root cause analysis) to understand how customers move from one state to another, to maximize profit. Related topics: customer lifetime value, cost of user acquisition, user retention.</p>\n",
    "<p><strong>15. Inventory management</strong></p>\n",
    "<p>Inventory management is the overseeing and controlling of the ordering, storage and use of components that a company will use in the production of the items it will sell as well as the overseeing and controlling of quantities of finished products for sale. Inventory management is an <em>operations research</em> technique leveraging analytics (time series, seasonality, regression), especially for sales forecasting and optimum pricing - broken down per product category, market segment, and geography. It is strongly related to <em>pricing optimization</em> (see item #17). &nbsp;This is not just for brick and mortar operations: inventory could mean the amount of available banner ad slots on a publisher website in the next 60 days, with estimates of how much traffic (and conversions) each banner ad slot is expected to deliver to the potential advertiser. You don't want to over-sell or under-sell this virtual inventory, and thus you need good statistical models to predict the web traffic and conversions&nbsp;(to pre-sell the inventory), for each advertiser category.</p>\n",
    "<p><strong>16. Optimum Bidding</strong></p>\n",
    "<p>This is an example of automated, black-box, machine-to-machine communication system, sometimes working in real time, via various API's. It is backed by statistical models. Applications include detecting and purchasing the right keywords at the right price on Google AdWords, based on expected conversion rates for millions of keywords, most of them having no historical data; keywords are categorized using an indexation algorithm (see item #18 in this article) and aggregated into buckets (categories) to get some historical data with statistical significance, at the bucket level. This is a real problem for companies such as Amazon or eBay. Or it could be used as the core algorithm for automated high frequency stock trading.</p>\n",
    "<p><strong>17. Optimum Pricing</strong></p>\n",
    "<p>While at first glance it sounds like an econometric problem handled with efficiency curves, or even a pure business problem, it is highly statistical in nature. Optimum pricing takes into account available and predicted inventory, production costs, prices from competitors, and profit margins. Price elasticity models are often used to determine how high prices can be boosted before reaching strong resistance. Modern systems offer prices-on-demand, in real time, for instance when booking a flight or an hotel room. User-dependent pricing - a way to further optimize pricing, offering different prices based on user segment - is a controversial issue. It is accepted in the insurance industry: bad car drivers paying more than good ones for the same coverage, or smokers / women / old people paying a different fee for healthcare insurance (this is the only price discrimination allowed by Obamacare).&nbsp;</p>\n",
    "<p><strong>18. Indexation</strong></p>\n",
    "<p>Any system based on taxonomies use an indexation algorithm, created to build and maintain the taxonomy. For instance product reviews (both products and reviewers must be categorized using an indexation algorithm, then mapped onto each other), scoring algorithms to detect the top people to follow in a specific domain (<a href=\"http://www.datasciencecentral.com/profiles/blogs/top-algorithm-experts\" target=\"_blank\">click here</a> for details), digital content management (<a href=\"http://www.datasciencecentral.com/profiles/blogs/data-science-2-0-with-r-python-excel-perl-javascript-and-api-s\" target=\"_blank\">click here</a> for details, read part 2), and of course search engine technology. Indexation is a very efficient clustering algorithm, and the time used to index massive amounts of content grows linearly - that is very fast - with the size of your dataset. Basically, it relies on a few hundreds categories manually selected after parsing tons of documents, extracting billions of keywords, filtering them, producing a keyword frequency table, and focusing on top keywords. Indexation is also used in systems that provide related keywords associated with user-entered keywords, for instance <a href=\"http://www.analyticbridge.com/group/codesnippets/forum/topics/source-code-for-our-big-data-keyword-correlation-api\" target=\"_blank\">in this example</a>.</p>\n",
    "<p>Last but not least, an indexation algorithm can be used to automatically create an index for any document - report, article, blog, website, data repository, metadata, catalog, or book. Indeed, that's the origin of the word <em>indexation</em>. Surprisingly, publishers still pay people today for indexing jobs: you can find these jobs listed on the <a href=\"http://www.asindexing.org/become-an-indexer/so-you-want-to-be-an-indexer/\" target=\"_blank\">American Society for Indexing</a>&nbsp;website. This is an opportunity for data scientist entrepreneurs: offering publishers a software that does this job automatically, at a fraction of the cost.</p>\n",
    "<p><strong>19. Search Engines</strong></p>\n",
    "<p>Good search engine technology relies heavily on statistical modeling. Enterprise search engines help companies - for instance Amazon - sell their products, by providing users with an easy way to find them. Our own <a href=\"http://www.datasciencecentral.com/forum/topics/most-popular-data-science-keywords-on-dsc\" target=\"_blank\">Data Science Central search</a>&nbsp;is of high quality (superior to Google search), and one of the most used features on our website. The core algorithm used in any search engine is an <a href=\"http://www.datasciencecentral.com/page/search?q=indexation\" target=\"_blank\">indexation</a>&nbsp;(see item #19 in this article) or automated tagging system. Google search could be improved as follows: (1) eliminate page rank - this algorithm has been fooled by cheaters developing link farms and other web spam, (2) add new content more frequently in your index to make search results less static, less <em>frozen in time</em>, (3) show more relevant articles using better user / search keyword / landing page matching algorithms which ultimately means better indexation systems, and (4) use better attribution models to show the source of an article, not copies published on LinkedIn or elsewhere. (this could be as simple as putting more weights on small publishers, and identifying the first occurrence of an article, that is, time stamp detection and management).</p>\n",
    "<p><strong>20. Cross-Selling</strong></p>\n",
    "<p>Usually based on collaborative filtering algorithms, the idea is to find - especially in retail - which products to sell to a client based on recent purchases or interests. For instance, trying to sell engine oil to a customer buying gasoline. In banking, a company might want to sell several services: a checking account first, then a saving account, then a business account, then a loan and so on, to a specific customer segment. The challenge is to identify the correct order in which products must be promoted, the correct customer segments, and the optimum time lag between the various promotions. Cross-selling is different from up-selling.</p>\n",
    "<p><strong>21. Clinical trials</strong></p>\n",
    "<p>Clinical trials are experiments done in clinical research, usually involving small data. Such prospective biomedical or behavioral research studies on human participants are designed to answer specific questions about biomedical or behavioral interventions, including new treatments and known interventions that warrant further study and comparison. Clinical trials generate data on safety and efficacy. Major concerns include how test patients are sampled (especially if they are compensated), conflict of interests in these studies, and the lack of reproducibility.</p>\n",
    "<p><strong>22. Multivariate Testing</strong></p>\n",
    "<p>Multivariate testing is a technique for testing an hypothesis in which multiple variables are modified. The goal is to determine which combination of variations performs the best out of all of the possible combinations. Websites and mobile apps are made of combinations of changeable elements, that are optimized using multivariate testing. This involves careful design-of-experiment, and the tiny, temporary difference (in yield or web traffic) between two versions of a webpage might not have statistical significance. While ANOVA and tests of hypotheses are used by industrial or healthcare statisticians for multivariate testing, we have developed systems that are model-free, data-driven, based on data binning and model-free confidence intervals (<a href=\"http://www.datasciencecentral.com/profiles/blogs/black-box-confidence-intervals-excel-and-perl-implementations-det\" target=\"_blank\">click here</a>&nbsp;and <a href=\"http://www.datasciencecentral.com/profiles/blogs/is-data-science-a-sin-against-the-norms-of-statisticians\" target=\"_blank\">here</a>&nbsp;for details). Stopping a multivariate testing experiment (they usually last 14 days for web page optimization) as soon as the winning combination is identified, helps save a lot of money. Note that external events - for instance an holiday or some server outage - can impact the results of multivariate testing, and need to be addressed.</p>\n",
    "<p><strong>23. Queuing Systems</strong></p>\n",
    "<p>A queue management system is used to control queues. Queues of people form in various situations and locations in a queue area, for instance in a call center. The process of queue formation and propagation is defined as queuing theory. Arrival of people in a queue is typically modeled using a Poisson process, with time to serve a client modeled using an exponential distribution. While being a statistical problem, it is considered to be part of operations research.&nbsp;</p>\n",
    "<p><strong>24.&nbsp;Supply Chain Optimization</strong></p>\n",
    "<p>Supply chain optimization is the application of processes and tools to ensure the optimal operation of a manufacturing and distribution supply chain. This includes the optimal placement of inventory (see item #15 in this article) within the supply chain, minimizing operating costs (including manufacturing costs, transportation costs, and distribution costs). This often involves the application of mathematical modelling techniques such as graph theory to find optimum delivery routes (and optimum locations of warehouses), the simplex algorithm, and Monte Carlo simulations. Read&nbsp;<a href=\"http://www.datasciencecentral.com/profiles/blogs/20-data-science-systems-used-by-amazon-to-operate-its-business\" target=\"_blank\">21 data science systems used by Amazon to operate its business</a>&nbsp;for typical applications. Again, despite being heavily statistical in nature, this is considered to be an operations research problem.</p>\n",
    "<p><a href=\"http://api.ning.com/files/pP7lTQTNhY*iYO0Vkiuf7hqAEzIdzf*LkhwNX8E-IL7jKBfqjvnda3V*sIqQlhKskj6elL3BAjz3UT4zlQMQ0w89qQ*TR4yf/Capturebv.PNG\" target=\"_self\"><img class=\"align-center\" src=\"http://api.ning.com/files/pP7lTQTNhY*iYO0Vkiuf7hqAEzIdzf*LkhwNX8E-IL7jKBfqjvnda3V*sIqQlhKskj6elL3BAjz3UT4zlQMQ0w89qQ*TR4yf/Capturebv.PNG\" alt=\"\" width=\"412\" /></a></p>\n",
    "<p><em>Source for picture:&nbsp;<a href=\"https://climatesanity.wordpress.com/2008/09/05/applying-monte-carlo-simulation-to-sloans-and-wolfendales-use-of-forbush-decrease-data/\" target=\"_blank\">click here</a>&nbsp;(regression via Monte Carlo simulations)</em></p>\n",
    "<li></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

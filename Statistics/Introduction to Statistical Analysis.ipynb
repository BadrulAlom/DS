{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 align=\"center\">Statistics</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Contents</b>\n",
    "<br>[Probability Fundamentals](#Probability Fundamentals)\n",
    "<br>[Probability Distributions](#Probability Distributions)\n",
    "<br>[Measures of Fit](#Measures of Fit)\n",
    "<br>[Hypothesis Testing](#Hypothesis Testing)\n",
    "<br>[Confidence Intervals](#Confidence Intervals)\n",
    "<br>[Linear Regression](#Linear Regression)\n",
    "<br>[Generalized Linear Models](#GLM)\n",
    "<br>[Q&A](#Q&A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Fundamentals'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">1. Fundamentals</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LinearVsNonLinear'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Linear vs Non Linear</h4>\n",
    "\n",
    "<img src=\"../_img/stats/stats002.png\" width=\"500\"> \n",
    "\n",
    "* Linear models are linear in the parameters (i.e. co-efficients) which have to be estimated, but not necessarily in the independent variables. This explains why the middle of the three figures above shows a linear discrimination line between the two classes, although the line is not linear in the sense of a straight line \n",
    "\n",
    "* A line with lots of turns (i.e. higher order polynomials) is non-linear \n",
    "\n",
    "http://www.statistics4u.com/fundstat_eng/cc_linvsnonlin.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Probability'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Probability</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>List the fundamental rules of probability</H4>\n",
    "\n",
    "<b>1) UNION (OR) rule:</b> $$p(a \\ or \\ b) = p(a) + p(b) - p(both)$$  i.e. a union b = p(a) + p(b) - p(a intersect b)\n",
    "\n",
    "<b>2) Product/Joint probability rule:</b>\n",
    "$$p(a,b)=p(a|b)*p(b) = p(b|a)*p(a) $$\n",
    "\n",
    "i.e. Specific combination of a & b = prob. of one given the other * prob of the other\n",
    "\n",
    "<b>3) SUM rule / Marginal distribution:</b>\n",
    "$$p(a)=\\sum_{b} p(a,b) =\\sum_{b} p(a|b)*p(b) $$\n",
    "i.e. p(a) across all the possible values of b\n",
    "\n",
    "<b>4) Bayes Rule</b>\n",
    "$$ P(a \\mid b) = \\frac{P(b \\mid a) \\, P(a)}{P(b)}=\\frac{P(a,b) \\,}{P(b)} $$\n",
    "\n",
    "The denominator here can be extended using the Marginal distribution rule to become:\n",
    "\n",
    "$$ P(a \\mid b) =\\frac{P(b \\mid a) \\, P(a)}{\\sum_{b} p(b|a)*p(a)} $$\n",
    "\n",
    "\n",
    "Important to know: \n",
    "* P(a) is referred to as the prior\n",
    "* P(b $\\mid$ a) is referred to as the liklihood\n",
    "* P(a $\\mid$ b) is referred to as the posterior\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Some basic probability</b>\n",
    "\n",
    "\n",
    "* p(something happening at least once) = 1 - P(never happening)\n",
    "* p(something happening at least n times) = 1 - p(happening less than n times)\n",
    "    * if the thing in question is binary, like the flip of a coin, then can use the binomial formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> The Chain Rule: Aka The probability of lots of things happening one after the other</b>\n",
    "<br><br>\n",
    "* If a,b and c happen, in that order, then total $prob (a,b,c) = p(a) * p(b|a) * p(c|a \\& b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is a probability mass function?</h4>\n",
    "\n",
    "Ans: Same as a probabilty distribution. Can be represented as a chart or as a formula. Also known as the probability function, the frequency function, or probability density function. Sums up to 1.\n",
    "\n",
    "---------------------\n",
    "\n",
    "<h4>Expected value</h4>\n",
    "\n",
    "https://www.youtube.com/watch?v=OvTEhNL96v0\n",
    "\n",
    "* Expected value is <b>theoretical</b> mean of a distribution NOT the actual mean of observations\n",
    "* Expectation= Sum of [(each of the possible theoretical outcomes) × (the probability of the outcome occurring)].\n",
    "* For continuous: $$E(X) \\equiv \\int x p(x)  \\  dx $$\n",
    "or for discreet: $$E(X) \\equiv \\sum x p(x)$$\n",
    "* Think of it as the center of mass\n",
    "* Not relevent but usefl to know: E($\\beta$X+c) = E(X) * $\\beta$ + c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='Probability  Distributions'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">2. Probability Distributions</h3>\n",
    "\n",
    "<img src=\"../_img/distribution.png\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Summary of use</b>\n",
    "\n",
    "* Gaussian: Cases that follow this distribution are likely to involve a lot of data or exist in textbooks\n",
    "* Bernoulli: When you can condense the process you are analysing into a binary decision\n",
    "* Poisson: If you are analyzing frequency of something happening\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color:#616161;color:white\">Poisson Distribution</h4>\n",
    "\n",
    "<h5>Poisson PDF:</h5> \n",
    "The <b>Poisson</b> distribution is popular for modeling the number of times an event occurs in an interval of time or space.\n",
    "<br>\n",
    "<br>\n",
    "$$\\frac {\\lambda ^{\\sum Y}}{nY!}e^{-n \\lambda }$$ \n",
    "\n",
    "<br>\n",
    "* $\\lambda$: (sometimes referred to as mean $\\mu$) is the mean number of successes that occur in a specified period.\n",
    "* Y is the observed number of successes that occur in a specified interval.\n",
    "* e is a constant equal to approximately 2.71828. (Actually, e is the base of the natural logarithm system.)\n",
    "<br><br>\n",
    "\n",
    "Notes about the Poisson:\n",
    "* The variance is the Poisson = $\\lambda$\n",
    "* Assumptin 1: occurrences are independent, so that one occurrence neither diminishes nor increases the chance of another\n",
    "* Assumption 2: the average frequency of occurrence for the time period in question is known\n",
    "* Assumption 3: it is possible to count how many events have occurred, such as the number of times a firefly lights up in my garden in a given 5 seconds, some evening, but meaningless to ask how many such events have not occurred. \n",
    "* Is asymmetrical: given a rate $\\lambda$ = 3, the range of variation ends with zero on one side (you will never find \"minus one\" letter in your mailbox), but is unlimited on the other side (if the label machine gets stuck, you may find yourself some Tuesday with 4,573 copies of some magazine spilling all over your front yard - it's not likely, but you can't call it impossible). \n",
    "* The Poisson Distribution, as a data set or as the corresponding curve, is always skewed toward the right, but it is inhibited by the Zero occurrence barrier on the left. \n",
    "* The degree of skew diminishes as r becomes larger, and at some point the Poisson Distribution becomes, to the eye, about as symmetrical as the Normal Distribution. But much though it may come to resemble the Normal Distribution, to the eye of the person who is looking at a graph for, say, $\\lambda$ = 35, the Poisson is really coming from a different kind of world event. \n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color:#616161;color:white\">Bernoulli</h4>\n",
    "\n",
    "\n",
    "A <b>Bernoulli</b> distribution is when the outcome of an event is binary - success or not.\n",
    "<br>\n",
    "\n",
    "\n",
    "<h4 style=\"background-color:#616161;color:white\">Binomial</h4>\n",
    "\n",
    "A <b>Binomial</b> is like a Benoulli but repeated multiple times (e.g. flipping a coin 10 times). \n",
    "* To use a Binonial you have to have a fixed number of events, n and the same prob p across each of those events\n",
    "\n",
    "<br>\n",
    "P(k num of successes out of n trials) = $$\\binom{n}{k}* p(Success)^k*P(Not Success)^{n-k} =  \\frac{n!}{k!(n-k)!} * p^kq^{n-k}$$\n",
    "\n",
    "n = number of trials<br>\n",
    "k = number of successes\n",
    "nCr on calculator will perform the $\\binom{n}{k}$ section\n",
    "</p>\n",
    "\n",
    "<h4 style=\"background-color:#616161;color:white\">Negative Binomial</h4>\n",
    "\n",
    "* Typical choice for people that want a model for counts that has high variance \n",
    "\n",
    "\n",
    "<h4 style=\"background-color:#616161;color:white\">Exponential</h4>\n",
    "\n",
    "$$p(Y|\\lambda) = \\prod^n_{i=1} \\lambda e^{-\\lambda y_i} = \\lambda^n e^{-\\lambda \\sum y}$$\n",
    "\n",
    "where $\\lambda^{-1}$ is the mean and $\\sum$ y  means sum up any y observations you have\n",
    "\n",
    "<h4 style=\"background-color:#616161;color:white\">Gamma</h4>\n",
    "\n",
    "$\\frac {\\beta^{\\alpha}}{\\Gamma (\\alpha)} x^{\\alpha -1} e^{-\\beta x}$\n",
    "\n",
    "$\\alpha = E(X)^2 / Var[X] \\quad\\quad$    (Mean squared divided by variance = alpha)\n",
    "\n",
    "$\\beta = E(X)/Var[X] \\quad\\quad$  (Mean divided by variance = beta)\n",
    "\n",
    "$E(X) = \\frac{\\alpha}{\\beta}$\n",
    "$Var(X) = \\frac{\\alpha}{\\beta^2}$\n",
    "\n",
    "<h4 style=\"background-color:#616161;color:white\">Exponential</h4>\n",
    "\n",
    "$ Exponential: \\lambda e^{-\\lambda y} \\rightarrow \\lambda^n e^{-\\lambda \\sum y}$\n",
    "\n",
    "<h4 style=\"background-color:#616161;color:white\">Normal</h4>\n",
    "\n",
    "$ Normal = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}} \\rightarrow \\frac{1}{(\\sqrt{2\\pi \\sigma^2)}n}e^{-\\frac{\\sum( y-\\mu)^2}{2\\sigma^2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Functions'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">3. Functions</h3>\n",
    "\n",
    "<h4 style=\"background-color:#616161;color:white\">Logistic</h4>\n",
    "\n",
    "* Logistic Function \n",
    "\n",
    "Recommended reading: http://wmueller.com/precalculus/families/1_80.html\n",
    "\n",
    "$$f(x)=\\frac{L}{1+e^{-k(x-x_0)}}$$\n",
    "\n",
    "\n",
    "<h4 style=\"background-color:#616161;color:white\">Sigmoid</h4>\n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "* Logit Function\n",
    "\n",
    "$$\\theta = g^{-1}(n) \\equiv \\frac{1}{1+e^{-n}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='Hypothesis Testing'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">4. Hypothesis Testing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain Hypothesis testing</h4>\n",
    "\n",
    "Hypothesis testing is about determining whether some value occured by chance. You determine the probability distribution it would follow if something was random and by chance. This is the null hypothesis.\n",
    "\"A general statement or default position that there is no relationship between two measured phenomena, or no association among groups.\"\n",
    "\n",
    "You then measure the probability of seeing the values you are seeing under this distribution (the p-value). If the p-value is quite low, then you infer that there's a high chance it wasn't due to chance, that there was some underlying relationship/reason for it.\n",
    "\n",
    "If the p-value is high, you conclude that it does not disprove the null hypothesis (but it doesn't mean it proves it either)\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>How to test a hypothesis</b>\n",
    "\n",
    "* Construct a null hypothesis and an alternative.( (Typically the null $H_0$ is that everything is fine)\n",
    "* State the parameter of interest you are testing (e.g. $\\theta$ = 0.5)\n",
    "* State the probabolity distribution you are using and the parameters of that distribtion\n",
    "* State your criteria for rejecting $H_0$ in terms off the critical region $\\alpha$ and whether its a two-tailed test ($\\alpha/2)$ and $(1-\\alpha/2)$ or 1 tail (which is if you are explicity testing for one side only). \n",
    "\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is a Type I and Type II error</h4>\n",
    "\n",
    "Type I: Rejecting null hypothesis when it is true - The probability of our pv(X) being less than 0.05,\n",
    "<br>Type II error:  Failing to reject the null hypothesis when it is false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Measures of Fit'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">5. Measures of Fit</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Recommended reading:</b>\n",
    "    \n",
    "* https://robjhyndman.com/hyndsight/crossvalidation/\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The Q-Q plot</h4>\n",
    "\n",
    "<img src=\"../_img/stats/stats004.png\" height=\"200\" width=\"400\"> \n",
    "\n",
    "* A good way of visualizing seeing how well data fits a cumulative distribution\n",
    "* Can be used with any distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cross-Validation</h4>\n",
    "\n",
    "* Split your data in K disjoint subsets\n",
    "* For every data subset Dk, where k = 1, 2, …, K, use the remaining data for fitting (training) the model, and assess the RSS on the “test set” Dk.\n",
    "* Report as a score the model average Dk acrossfolds\n",
    "\n",
    "<h4>Deviance</h4>\n",
    "\n",
    "* Deviance is a measure of divergence between a model and a perfectly fitted one (aka the saturated model)\n",
    "* Lower deviance means better fit, but be aware that with deviance would expect a decrease of at least one unit for every new predictor your throw in, irrespective of whether it's any good\n",
    "* It is a generalization of the idea of using the sum of squares of residuals in ordinary least squares to cases where model-fitting is achieved by maximum likelihood.\n",
    "* The deviance is used to compare two models – in particular in the case of generalized linear models (GLM) where it has a similar role to residual variance from ANOVA in linear models (RSS)\n",
    "* More importantly, the ratio between the divergence score of two models built from the same data gives useful insight resulting in the log-liklihood ratio test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Pearsons Chi-squared test for Goodness of Fit</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Goodness of Fit</b>\n",
    "- It compares 'Expected' vs. 'Observed' outcomes where the expected outcome would be from some underlying probability model\n",
    "- It assesses the 'Goodness of fit' through several different algorithms:\n",
    "    - Kolmogorov–Smirnov test\n",
    "    - Pearsons Chi-squared test\n",
    "    - ..and others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is the T-Test?</h4>\n",
    "\n",
    "Ans: The t-test assesses whether the means of two groups are statistically different from each other. This analysis is appropriate whenever you want to compare the means of two groups, and especially appropriate as the analysis for the posttest-only two-group randomized experimental design.\n",
    "\n",
    "\n",
    "A one-sample location test of whether the mean of a population has a value specified in a null hypothesis.\n",
    "A two-sample location test of the null hypothesis such that the means of two populations are equal. All such tests are usually called Student's t-tests, though strictly speaking that name should only be used if the variances of the two populations are also assumed to be equal; the form of the test used when this assumption is dropped is sometimes called Welch's t-test. These tests are often referred to as \"unpaired\" or \"independent samples\" t-tests, as they are typically applied when the statistical units underlying the two samples being compared are non-overlapping.[8]\n",
    "A test of the null hypothesis that the difference between two responses measured on the same statistical unit has a mean value of zero. For example, suppose we measure the size of a cancer patient's tumor before and after a treatment. If the treatment is effective, we expect the tumor size for many of the patients to be smaller following the treatment. This is often referred to as the \"paired\" or \"repeated measures\" t-test:[8][9] see paired difference test.\n",
    "A test of whether the slope of a regression line differs significantly from 0.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is the Wald test?</h4>\n",
    "\n",
    "Where as the t-test is motivated by small, Gaussian samples, the Wald test uses the same statistic but assumes the sample size is large enough so that the central limit theorem kicks in.\n",
    "–Samples $X_{(i)}$can be of “any” distribution.\n",
    "–Hence the distribution of the statistic (let’s call it W, but it’s the same formula as T) is N(0, 1) now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Chi-Squared Test</b><br><br>\n",
    "If you had results 10,20,20,30,10,10\n",
    "and based on probabilities you expected it to be more like: 20,20,20,20,20,20 then:<br>\n",
    "Chi-Sq test = $\\sum\\frac{(x_i - E_i)^2}{E_i}$ = 20.\n",
    "<br>Look up p-value in Chi-Square table with N-1=5 degrees of freedom to find out what the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='CI'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">6. Confidence Intervals</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"../_img/CI.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What is a sampling distribution?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It's the distribution of a statistic that you get from taking lots of samples from a population. So you take a sample, work out your statistic (e.g. mean of your sample), throw the sample away, and repeat. Over-time you will build up a picture (i.e. distribution) of your statistic. The <b>mean</b> of that statistic, whether it's the mean of a mean or mean of something else, will get closer to the mean of the populaton statistic, with the more samples you have and the greater the sample size is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"../_img/samplingdist.png\" height=\"200\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> How can you compare the mean across two populations?</b><br>\n",
    "If you have two samples (a & b) which you believe ought to have the same underlying distribution and which to do a test to see if this is the case then:<br>\n",
    "a) You will want to test the difference in a test statistic (e.g. the mean) between a & b\n",
    "<br>b) Provided the samples are both large enough OR both samples are assumed to be from a normal distribution, then you will be able to assume that the difference you see between a and b would follow a normal distribution.\n",
    "<br>c) You can therefore test this difference by saying your null hypothesis is the difference is not statististically different from a difference of 0.\n",
    "<br>d) To work out the boundary (i.e. Confidence Interval) where you can say this you would compute:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you took just one sample from the population, you could either ask yourself how close are you to the true statistic, and if you think you are far, then continue sampling; or you could ask whether you are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "- Confidence Intervals can be constructed for any sample statistic, not just the mean\n",
    "- Confidence interval = sample statistic $\\pm$ Margin of error\n",
    "- Margin of error (ME) = Critical value * (either Standard deviation OR Standard error of sample statistic)\n",
    "\n",
    "<b>Finding the critical value</b><br>\n",
    "The central limit theorem states that the sampling distribution of a statistic will be nearly normal, if the sample size is large enough. As a rough guide, many statisticians say that a sample size of 30 is large enough when the population distribution is bell-shaped. But if the original population is badly skewed, has multiple peaks, and/or has outliers, researchers like the sample size to be even larger.\n",
    "\n",
    "- Obtain your alpha $\\alpha$, which is (1 - confidence level)\n",
    "- Divide alpha by 2 if doing a two-tail test and then use either the Z-table or the T-Table to get a Z or T score:\n",
    "    - If the population standard deviation is known, use the z-score.\n",
    "    - If the population standard deviation is unknown, use the t statistic.\n",
    "    - Another approach focuses on sample size.\n",
    "    - If the sample size is large, use the z-score. (The central limit theorem provides a useful basis for determining whether a sample is \"large\".) \n",
    "    - If the sample size is small, use the t statistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>How do you calculate a confidence interval around a mean from a sample of data?</b>\n",
    "<br>\n",
    "Margin of Error (ME) = $Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt(n)}$\n",
    "<br>\n",
    "CI = mean $\\pm$ ME\n",
    "<br>\n",
    "<img src=\"../_img/Alpha.png\" height=\"100\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What is the power of a test</b>\n",
    "<br>\n",
    "power = 1 – β.\n",
    "In plain English, statistical power is the likelihood that a study will detect an effect when there is an effect there to be detected. If statistical power is high, the probability of making a Type II error, or concluding there is no effect when, in fact, there is one, goes down.\n",
    "\n",
    "Useful links:\n",
    "<br>http://www.statsref.com/HTML/index.html?test_for_mean_when_standard_de_2.html\n",
    "<br>(http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_HypothesisTest-Means-Proportions/BS704_HypothesisTest-Means-Proportions3.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Interpret the following</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "a) The p-value on the F-statistic shows the the model was able to make results are statistically significant \n",
    "<br>b) As there's only one covariate we can see the same p-value in the co-efficients row showing horsepower was statistically significant and inversely correlated with the output\n",
    "<br>c) The residuals are farily well banced around the a median that's close to 0\n",
    "<br>d) The Residul standard error of 4.906 shows that the predicted output can vary from the true value by 4.906 on average\n",
    "<br>e) The R-squared of 0.6 can be considered good\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What does the distribution of P-Values look like?</b>\n",
    "P values have a uniform distribution if the null hypothesis is true. That is to say if the thing you are testing is truly random and there is no correlation then each p-value between 0 and 1 should have an equal chance of getting triggered.\n",
    "\n",
    "http://varianceexplained.org/statistics/interpreting-pvalue-histogram/\n",
    "    \n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is the problem with $R^2$ as a test statistic in regression?</h4>\n",
    "\n",
    "Ans: $R^2$ looks at total sum of squares error -- does not consider the variability of that error around the predicted. E.g. If you have a chart where really there's two distinct regions of data\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What are the names for the inputs and outputs of a lineear regression model?</b>\n",
    "<br><br>\n",
    "Input variables can be called:\n",
    "    - Covariates\n",
    "    - Inputs\n",
    "    - Regressors\n",
    "    - Independent variables (bad name!)\n",
    "<br>Output variable is called:\n",
    "    - Output\n",
    "    - Response\n",
    "    - Dependent variable (bad name!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> What is Heteroscedasticity</b>\n",
    "<br><br>\n",
    "In linear regression this means that your variance / error is not consistent due to sub-groups within your data. Example plotting the residuals may show variance increasing over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<q><b>What is collinearity?</b></q>\n",
    "<br><br>\n",
    "When two or more variables are closely correlated with one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Topics:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Non-parametric denisity estimations: \n",
    "1. Gaussian density estimate, empirical distribution, histograms\n",
    "2. Confidence bands\n",
    "3. Kernal Density Estimate\n",
    "4. Multicariate density estimations\n",
    "\n",
    "Building and estimating multivariate distributions\n",
    "1. Joint distributions\n",
    "2. Maximum liklihood\n",
    "3. Multivariate binomial\n",
    "4. Multivariate Gaussian\n",
    "5. Regularization\n",
    "6. Graphical lasso\n",
    "\n",
    "Dimensionality Reduction\n",
    "1. PCA\n",
    "\n",
    "Latent Variable Models and Clustering\n",
    "1. Latent Gaussian models\n",
    "2. Clustering\n",
    "3. K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Q: What is variance and covariance?\n",
    "\n",
    "* <b>Variance</b> is the amount of variablility around the Expectation:\n",
    "\n",
    "$$ var[f] = \\mathbf{E}[(f(x) - \\mathbf{E}[f(x)])^2]= \\mathbf{E}[f(x)^2] - \\mathbf{E}[f(x)]^2  $$\n",
    "\n",
    "\n",
    "This translates to: Variation = Mean of what you predicted$^2$ minus what you expected$^2$\n",
    "\n",
    "<br>\n",
    "* <b>Covariance</b>: Measures the joint-variability between two variables. \n",
    "\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Q: What is Expectation? </h4>\n",
    "\n",
    "* Another way of saying average\n",
    "* The average value of some function f(x) under a probability distribution p(x).\n",
    "\n",
    "$ \\mathbf{E}(f) = \\displaystyle \\sum p(x)f(x)  $ -- discreet case (Summing up all the individual probs * value)\n",
    "\n",
    "$ \\mathbf{E}(f) = \\displaystyle \\int p(x)f(x)dx  $ -- continuous case\n",
    "\n",
    "* Expectation can be estimated from a N samples drawn from a probabilty distribution function:\n",
    "\n",
    "$ \\mathbf{E}(f) \\simeq \\frac1 N \\sum f(x_n)$\n",
    "\n",
    "Note: Multiplying by \\frac1 N is the same as dividing by N. You'll see this a lot in machine learning formulas.\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Q: What does this show?\n",
    "\n",
    "<img src=\"../_img/covariance.png\" height=\"200\" width=\"400\">\n",
    "\n",
    "Ans: \n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Bivariate or multivariate Gaussian distributions (so like a 3d Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Suppose you are doing penalized curve fitting\n",
    "\n",
    "$$\\hat g= argmin \\left( \\sum^{n}_{i=1}(y^{(i)}-g(x^{(i)}))^2 + \\lambda \\int[g^m(x)]^2 dx \\right)$$\n",
    "\n",
    "where $g^{m}$ is the m-th derivative of g. Explain what happens when $\\lambda$ = 1 under m = 0 and m = 1.</h4>\n",
    "\n",
    "* When m=0, anything raised to the power of 0 =1 so [g^m(x)]^2 becomes $[g(x)]^2$\n",
    "* This is postive and so the penalty is infinite which will force all the co-efficients to be 0\n",
    "* The argmin will mean that the results of the function boils down to the average of y\n",
    "* This is the same if m=1\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Discuss different types of regulatizer terms</h4>\n",
    "\n",
    "L1\n",
    "* Term is: $\\sum abs(w)$\n",
    "* Heavy and small $w_i$ is penalized by the a constant proportional amount\n",
    "* Penalty can't be differentiated so requires special optmization routines\n",
    "* But will mean that only significant weights remain\n",
    "* In practice (e.g. deep learning) people don't worry about it being non-differentiable and just proceed with gradient based training as normal\n",
    "\n",
    "L2\n",
    "* Term is: $ w^T w$ or $\\sum_i{w^2_i}$\n",
    "* Penalizes large W more than small ones\n",
    "* Penalty is differentiable\n",
    "* Small weights will still persists\n",
    "\n",
    "Shapes for regularisation penalty for 2 weights $\\sum_i | w_i|^q$\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml006.png\" height=\"100\" width=\"600\">\n",
    "\n",
    "* As we decrease q towards 0 we get the ‘ideal’ regulariser that sets weights\n",
    "to 0 if they do not contribute.\n",
    "* For q = 1 the objective function for squared loss linear regression remains\n",
    "convex and easy to optimise\n",
    "* The objective function is complicated (non convex) for q < 1\n",
    "* As you increase the power you give the model more flexibility in what is sets the co-efficients to\n",
    "-------------------\n",
    "\n",
    "\n",
    "$l_0$ can give sparse solutions, but it is not computationally tractable. Regularisation boils down to\n",
    "zero/one penalties.\n",
    "\n",
    "$l_1$ can regularise and “sparsify” solutions in a single pass, being computationally convenient.\n",
    "– But this conflation of magnitude and sparsity is not ideal as\n",
    "they are not synonyms. Recall that a signal can be “large”\n",
    "and statistically non-significant, and “small” but significant.\n",
    "\n",
    "\n",
    "$l_2$ can give regularised solutions, but in general we will never see a sparse solution.\n",
    "– Informally we may think of thresholding, but this is not an\n",
    "optimal solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Logistic Regression'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">6. Logistic Regression</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id='GLM'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">7. Generalised Linear Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Useful viewing (after reading my explanation): \n",
    "* https://www.youtube.com/watch?v=hc06BXFYd_w\n",
    "* https://www.youtube.com/watch?v=G5xIFdLL5Ic\n",
    "* https://onlinecourses.science.psu.edu/stat504/node/216\n",
    "\n",
    "\n",
    "* In a normal linear regression model you had an equation y = mx + e  (where the mx could be multiple x's and multiple co-efficients)\n",
    "* That mx would give you a straight line, and you know that the true result is often spread around that straight line\n",
    "\n",
    "* What people don't tell you is that ther's an unwritten assumption here that whatever y value your model predicts, you know the true value is going to be somewhere around that prediction (\"I predict 100 give or take 10\"). That's you essentially saying that I assume a Gausssian distribution around my estimate.\n",
    "\n",
    "* Now GLM explicitly applies a distribution around that straight line model\n",
    "* It can apply different types of distributions (Gaussian, Binomial, etc.)\n",
    "* But to formulate the distribution you need to give it the parameters that define it - exactly how you define that can vary, but it will use your predicted value Y as a starting point\n",
    "\n",
    "Example:\n",
    "* If you are predicting the number of stop and searches based on ethnicity, you may first come up with the best linear model you can: $$y = \\beta_1 isAsian +  \\beta_2 isHisp +  \\beta_3 isWhite +   \\beta_4  isBlack + \\beta_5 TotalPop \\beta_0$$\n",
    "\n",
    "* You know that there's no way your model should ever predict 0 so you don't want to assume a Gaussian distribution. Instead you choose Poisson at that is not allowed to be less than 0\n",
    "\n",
    "<b>Important bit</b>\n",
    "* You define a link function to say that for any y output my model gives me, transform it into a Poisson probability distribution, using the y to help define that distribution\n",
    "* With this rule in place, you now have converted your y output into a distribution that tells you the probability of that y output occuring\n",
    "* You can then sum that up across many rows of training data in order to find the right set of parameters into your original linear model that maximizes the sum of the probabilities of your y outputs (i.e max liklihood)\n",
    "\n",
    "<b>The Link Function</b>\n",
    "<img src=\"../_img/stats/stats001.png\" height=\"200\" width=\"800\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='TestingFit'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Testing Fit</h4>\n",
    "\n",
    "Dispersion:\n",
    "* When many more points than expected lie outside the interval, it could be the result of a\n",
    "bad fit of the mean.\n",
    "* But it can also be the result of a bad choice of likelihood function. It is common to find count data where variance is higher than the mean. This is called overdispersion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miscellaneous notes:\n",
    "\n",
    "* Some minimization criteria are less stable (have high variance compared to others). Maximum Liklihood is has less variance than Least Squares minimization\n",
    "* KL Divergence is used for comparing two distributions from the same data sample space\n",
    "* Max Likilihood can be thought of as a Minimization of KL Divergence between your model and the empirical distribution\n",
    "\n",
    "\n",
    "* Unlike unpenalized least-squares regression, regression with shrinkage is scale-dependent.\n",
    "* Running penalized least-squares without considering the format of your data might give you suboptimal results.\n",
    "* Consider standardizing your data\n",
    "* It is hopeless to think we can learn a “true” model when the number of dimensions > rows and there are many relevant covariates\n",
    "    * What we recover is one of many possible models,\n",
    "one that can be statistically detected.\n",
    "– Training measures such as R2 will be useless here, but\n",
    "cross-validated predictions are meaningful.\n",
    "    * Prediction-wise, it is silly to throw variables away\n",
    "without looking at the data and without a\n",
    "theoretical justification, so we should welcome\n",
    "large p.\n",
    "    * Concerning what we learn about the world, we\n",
    "will be selecting promising explanatory variables\n",
    "as allowed by our data resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NonLinearModels'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">8. Non-Linear Models</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended: https://www.youtube.com/watch?v=n0LA2YhsWFU\n",
    "\n",
    "\n",
    "<h4> Generalized Additive Models</h4>\n",
    "\n",
    "Recommended: https://www.youtube.com/watch?v=2Lp2LYUUxhc\n",
    "\n",
    "$$Y = \\beta_0 + \\sum^p_{j=1}f_j(x_j) +  \\epsilon$$\n",
    "\n",
    "* Notice the summation here. j= column or dimension.\n",
    "\n",
    "* A nonparametric regression method. \n",
    "* The GAM uses a one-dimensional smoother to build a restricted class of nonparametric regression models. Because of this, it is less affected by the curse of dimensionality than e.g. a p-dimensional smoother. Furthermore, the AM is more flexible than a standard linear model, while being more interpretable than a general regression surface at the cost of approximation errors. Problems with AM include model selection, overfitting, and multicollinearity.\n",
    "* Additivity is certainly not the greatest assumption, but it can be as good as it gets in problems with many variables and not “that much” data.\n",
    "\n",
    "<b>The backfitting alorithm</b>\n",
    "\n",
    "* Keep adding adding a function of a variables until you can't find any that change the final outut much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='HighDimensionality'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">9. High Dimensionality</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The curse of dimensionality </h4>\n",
    "\n",
    "The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. The expression was coined by Richard E. Bellman when considering problems in dynamic optimization.[1][2]\n",
    "\n",
    "There are multiple phenomena referred to by this name in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining, and databases. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Column or Dimension selection</h4>\n",
    "* Do not throw away columns if you do not have a good motivation\n",
    "\n",
    "* Combinatorial search (aka. subset selection, Stepwise)\n",
    "    - Best-subset selection: Decide Go through analyzing all possible moels with 1 dimension, up to p dimensions\n",
    "    - Select the best one\n",
    "    \n",
    "* Shirinkage (aka. regualrization)\n",
    "* Dimensionality reduction: transform your input into a smaller set of feature space\n",
    "\n",
    "\n",
    "<b>Stepwise regression</b>\n",
    "\n",
    "* Forward stepwise regression: Start with no predictors and keep testing all and adding the best one in, until you can't find one that gives you any significant gain\n",
    "* Backwards stepwise regression: Start will all variables (or as many as you select) and remove any that have the least impact on results, and repeat until everything that remains has a significant impact if removed\n",
    "\n",
    "<b>PCA</b>\n",
    "\n",
    "* Scaling your variables is important, as PCA will blindly try to capture the most variance in your data, so columns with large values will be captured more\n",
    "* It is possible to directly pipeline PCA with regression to optimise parameters in a different way.\n",
    "    * See Principal Components Regression, Section 6.3 of ISLR.\n",
    "    * In the non-linear case, this is basically what a multilayer perceptron is.\n",
    "    \n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Unsupervised'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">10. Unsupervised</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Q&A'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">10. Q&A</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Probability'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Probability</h4>\n",
    "\n",
    "<b>Two fair dice are thrown. Let X be the smallest of the two numbers obtained\n",
    "(or the common value if the same number is obtained on both dice). Find the\n",
    "probability mass function (i.e. the distribution) of X. Find $P(X > 3)$.</b>\n",
    "<br>\n",
    "\n",
    "Ans:\n",
    "* 6x6 = 36 possibilities in total\n",
    "* Any two numbers has a 1 /36 chance\n",
    "* p(X>3) = 1 - p(x=1) + p(x=2) + p(x=3)\n",
    "\n",
    "Use a matrix here to help you determine\n",
    "* = 1 - (11/36) + (9/36) + (7/36) = 0.25 = 25%\n",
    "\n",
    "-----------------------\n",
    "\n",
    "<b>Let X be a random variable with expectation $\\mu$ Find the expectation of the random variable Y = X-$\\mu$ </b>\n",
    "\n",
    "* Recognize that X consists of a vector of x's\n",
    "* E(Y) = E(X - $\\mu$) = $\\int (x - \\mu) p(x) \\ dx $\n",
    "* Multiplying with p(x): $= \\int xp(x) - \\int \\mu p(x)\\  dx $\n",
    "* p(x) on cancel each other, leaving $E(x)- \\mu $ which we can infer equals 0\n",
    "\n",
    "---------------------------\n",
    "\n",
    "<h4>On a coral reef, S species of fish are present in proportions $p1, ... , pS$. A\n",
    "biologist wishes to take a sample of the fish, and wants to know how many\n",
    "species of fish she should expect to find in a sample of a given size.<br>\n",
    "<br>\n",
    "\n",
    "(a) Suppose a sample of size $n$ is taken. Let $X_i (i = 1, ... ,S) $ be a random variable taking the value 1 if species i occurs, 0 otherwise. \n",
    "\n",
    "Find an expression for the probability of at least one fish of type $X_i$.\n",
    "<br>(Assume that, first, the sample is small relative to the population\n",
    "of any fish species, so that taking the sample has a negligible effect on the\n",
    "proportions of fish remaining; second, species are distributed randomly so\n",
    "that successive fish in the sample can be regarded as independent draws\n",
    "from the population).\n",
    "<br><br>\n",
    "\n",
    "(b) Let Y be the number of fish species present in the sample. Express Y\n",
    "in terms of the $X_i$'s, and deduce that the expected number of species is\n",
    "$$ S-\\sum_{i=1}^{S}(1-p_i)^n$$\n",
    "Are you making any further assumptions in obtaining this result? Check\n",
    "that the formula gives the correct result for a couple of different sample\n",
    "sizes where the answer is \"obvious\".</b>\n",
    "<br><br></h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "a)\n",
    "* First sketch out the data so you are visualizing it\n",
    "<img src=\"../_img/stats/stats003.png\" height=\"200\" width=\"400\"> \n",
    "* $p(X_i)$ > =  1 = 1- $p(x_i) $= 0\n",
    "* = 1- (1 - $p(x_i)^n$)\n",
    "\n",
    "b) Y = $\\sum E[X_i] = \\sum_{i=1}^S 1-(1-p_i)^n = S - \\sum_{i=1}^S (1-p_i)^n $\n",
    "<br>(If you are not following the last step - just think of the middle terms as being the sum of 1's minus the sum of the probs).<br>\n",
    "<br>Assume uniform distribution of fish in population and S = 10 species. Check result when n= 1 where Y should = 1, and 10000 where Y = S. \n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "<b>\n",
    "In a promotion for a particular airline, customers and potential customers were given vouchers. 10% of these vouchers were for a free round-trip anywhere this airline flies. How many vouchers would an individual need to collect in order to have a 50% chance of winning at least one free trip?</b>\n",
    "<br><br>\n",
    "Trigger words here are 'at least'.<br>\n",
    "Probability of not winning at all = $0.9^S$ where s = number of tickets<br>\n",
    "<br>\n",
    "$p(winning) = 1- 0.9^S$\n",
    "<br>\n",
    "7 tickets need to be purchased to have a 50% chance of winning\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Karlyn Akimoto operates a small computer store. On a particular day she has three Hewlett-Packard and two Dell computers in stock. Suppose that Susan Spencer comes into the store to purchase two computers. Susan is not concerned about which brand she purchases—they all have the same operating specifications—so Susan selects the computers purely by chance: Any computer on the shelf is equally likely to be selected. What is the probability that Susan will purchase one Hewlett-Packard and one Dell computer?</b>\n",
    "<br><br>\n",
    "<code>p(HP and Dell) = P(HP|Dell first)*P(Dell first) + P(Dell|HP first)*P(HP first) = 3/4*2/5 + 2/4*3/5 = 0.6</code>\n",
    "\n",
    "------------------------\n",
    "\n",
    "<b>An urn contains 6 red marbles and 4 black marbles. Two marbles are drawn without replacement from the urn. What is the probability that both of the marbles are black?</b>\n",
    "<br>\n",
    "<br>\n",
    "`p(black & black) = p(black|black)* P(black) = 4/10 * 3/9 = 0.13 => 13%`\n",
    "\n",
    "-------------------\n",
    "\n",
    "<b>Five friends including Omar and Sarah go to a party. On their way out they each pick one of the 5 jackets randomly (without replacement.<br>\n",
    "What is the probability that:<br>\n",
    "a) Omar ends up with his own jacket?<br>\n",
    "b) everyone ends up with their own jacket?<br>\n",
    "c) both Omar and Sarah don't end up with their own jackets?<br>\n",
    "</b>\n",
    "<code>\n",
    "Assuming everyone left at exactly the same time then:<br>\n",
    "a) 1/5 0.2\n",
    "b) 1/5^5= 0.0003 (assuming they were each handed a jacket at the same time)\n",
    "c) 4/5*4/5 = 0.640\n",
    "\n",
    "</code>\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For each case below, state whether the binomial distribution is suitable. If not,\n",
    "give your reasons; if it would, state the values of parameters trials, n and probability p.<br>\n",
    "<br>(i) The number of sixes obtained in three successive throws of a fair die.\n",
    "<br>(ii) The number of girls in the families of British prime ministers.\n",
    "<br>(iii) The number of aces in a hand of four cards dealt from a standard pack of\n",
    "cards.\n",
    "<br>(iv) The number of students in a class of 40 whose birthday falls on a Sunday\n",
    "this year.\n",
    "<br>(v) The number of throws of a fair coin until the first head is obtained.</b>\n",
    "<br>\n",
    "<br>\n",
    "Binomial Distribution require number of trials n, and probabilty of success p, to remain fixed\n",
    "<br>1)Yes, n=3, p=1/6\n",
    "<br>2)No - n is not a constant (different families have different numbers of children)\n",
    "<br>3)No, prob of Aces is not constant (decreases over time)\n",
    "<br>4)Yes - p(Sunday in this year) is constant ~1/7 days, and k = 40\n",
    "<br>5)No - n is not fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Consider a jury trial in which it takes 8 out of the 12 jurors to convict. That is,\n",
    "in order for the defendant to be convicted, at least 8 of the 12 jurors must vote\n",
    "him guilty. Assume that jurors act independently and each makes the right\n",
    "decision with probability p. Let $\\alpha$ denote the probability that the defendant is\n",
    "guilty. What is the probability that the jury renders a correct decision?</b>\n",
    "\n",
    "`P(correct) = P(guilty verdict)*P(isGuilty) + P(innocent)*P(isInnocent)\n",
    "P(guilty verdict) = Binomoal prob. of 8,9,10,11,12, summed up\n",
    "P(innocent verdcit) = Binomial prob. of 1,2,3,4,5,6,7, summed up`\n",
    "\n",
    "$$P(Guilty Verdict)*P(isGuilty) = \\sum_{k=8} ^{12} \\binom{12}{k}* p^k*(1-P)^{12-k} * \\alpha $$\n",
    "$+$\n",
    "$$P(Innocent Verdict) * P(isInnocent) = \\sum_{k=0} ^{7} \\binom{12}{k}* p^k*(1-P)^{12-k} * (1-\\alpha) $$\n",
    "\n",
    "\n",
    "note: $\\binom{12}{k}$ means $\\frac {12!}{(12-k)!k!}$\n",
    "\n",
    "---------------------\n",
    "\n",
    "<b>An exam paper consists of ten multiple choice questions, each offering four\n",
    "choices of which only one is correct. If a candidate chooses his answers completely at random, what is the probability that<br>\n",
    "(i) he gets at least 8 questions right<br>\n",
    "(ii) the last of the ten questions is the eighth one he gets right <br>\n",
    "(iii) in six such exams, he gets at least 8 questions right in at most one exam?</b>\n",
    "<br>\n",
    "i) <br>\n",
    "p(k>=8) = p(k=8)+p(k=9)+p(k=10)\n",
    "<br>Using Binomal theorem where n=10, p=0.25:<br>\n",
    "p(k=8) = $ \\frac{10!}{8!(10-8)!} * 0.25^8 * 0.75^{2} = 0.0038 * $<br>\n",
    "p(k=9) = $ \\frac{10!}{9!(10-9)!} * 0.25^9 * 0.75^{1}$ = 0.00028<br>\n",
    "p(k=10) = $ \\frac{10!}{10!(10-10)!} * 0.25^{10} * 0.75^{0} = 0.00007$\n",
    "<br>= 0.004\n",
    "<br>In R:\n",
    "dbinom(10, size=10, prob=0.25)+dbinom(9, size=10, prob=0.25)+dbinom(8, size=10, prob=0.25) = 0.0004\n",
    "\n",
    "ii) = P(7/9 right) + 0.25\n",
    "ii) We know the answer to p(of getting at least 8) from part I. So this becomes a new binomial question with k = 1, n=6, and p=0.0004."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Andrew Whittaker, computer center manager, reports that his computer system experienced three component failures during the past 100 days. \n",
    "<br>a. What is the probability of no failures in a given day? \n",
    "<br>b. What is the probability of one or more component failures in a given day? \n",
    "<br>c. What is the probability of at least two failures in a 3-day period?\n",
    "\n",
    "</b>\n",
    "<br>\n",
    "Assuming the past 100 days is typical and component failures are independent of one another then we can computer the probability of failure usisng the Poisson distribution:<br>\n",
    "<br>\n",
    "$\\lambda (mean)$ = 3/100 = 0.03\n",
    "<br><br>a)$ p(x=0) = \\frac {0.03^0*e^-0.03} {0!} = 0.9075$\n",
    "<br>b) 1-p(x=0) = 0.0295\n",
    "<br>c)$\\lambda = 3*0.003 p(x>=2) = 1 - p(x=0)+p(x=1) = 1-0.9139+0.0822=0.003815$\n",
    "\n",
    "--------------------\n",
    "\n",
    "<b>A typist makes on average 2 mistakes per page. What is the probability of a particular page having no errors on it? </b>\n",
    "<br><br>\n",
    "We have an average rate here: $\\lambda$ = 2 errors per page.\n",
    "Using Poisson distribution. \n",
    "($\\lambda$) = (2 errors per page * 1 page) = 2.\n",
    "Hence P0 = $\\frac{2^0}{0!} * exp(-2) = 0.135$.\n",
    "\n",
    "-------------------\n",
    "\n",
    "<b>Components are packed in boxes of 20. The probability of a component being defective is 0.1. What is the probability of a box containing 2 defective components? </b>\n",
    "\n",
    "p(defective) = 0.1 and hence p(not defective) = 0.9. Hence, Binomial, with n = 20.  \n",
    "\n",
    "P(2 detective out of 20 trials) = $$  =  \\frac{20!}{2!(18)!} * 0.1^20.9^{18} = 190 * 0.002 = 0.285 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A coin is tossed 10 times. What is the probability that exactly 6 heads will occur.</b>\n",
    "\n",
    "`Using Binomial formula:\n",
    "Success = \"A head is flipped on a single coin\"\n",
    "p = 0.5\n",
    "q = 0.5\n",
    "n = 10\n",
    "x = 6\n",
    "P(x=6) = 10C6 * 0.5^6 * 0.5^4 = 210 * 0.015625 * 0.0625 = 0.205078125 `\n",
    "\n",
    "---------------\n",
    "\n",
    "<b>Say you toss 1,000 different coins once each, which you assume are identically\n",
    "distributed. Would you change your test for fairness? Would you think of ways in\n",
    "which the test could fail to behave as expected?</b>\n",
    "\n",
    "Assuming coins were independent (iid) the binomial could be used.\n",
    "If not independent (e.g. coins always landed tails if the year they were made was an odd number, and head otherwise) then you could end up with it not rejecting the null hypothesis when it ought to be rejected\n",
    "\n",
    "--------------\n",
    "\n",
    "<b>If you toss a coin 1,000 times and observe 570 heads, how would you assess the claim\n",
    "that the coin is fair?</b>\n",
    "<br>\n",
    "$H_0 = p(H) = 0.5$<br>\n",
    "Using Binomial: n = 1000, k = 570 \n",
    "$$\\frac{1000!}{570!(1000-570)!} * 500^k500^{1000-570}$$\n",
    "<br>This does not work on a calculator so use R instead:\n",
    "In R:   <code>binom.test(500,1000,0.5) </code>\n",
    "<code>\n",
    "Output:\n",
    "Data:  500 and 1000\n",
    "number of successes = 500, number of trials = 1000, p-value = 1\n",
    "alternative hypothesis: true probability of success is not equal to 0.5\n",
    "95 percent confidence interval:\n",
    " 0.4685492 0.5314508\n",
    "sample estimates:\n",
    "probability of success 0.5 \n",
    "</code>\n",
    "<br>95% CI = 469 - 531. 570 is outside of this range therefore reject null hypothesis.\n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ConfInt'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Confidence interval and hypothesis testing questions</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nine hundred (900) high school freshmen were randomly selected for a national survey. Among survey participants, the mean grade-point average (GPA) was 2.7, and the standard deviation was 0.4. What is the margin of error, assuming a 95% confidence level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The following table summarizes data from a\n",
    "double-blind experiment that aims at comparing particular drugs for nausea reduction\n",
    "against a placebo. Assume each patient is independently assigned one of the treatment\n",
    "groups, or the placebo.\n",
    "<img src=\"../_img/Placebo.png\" height=\"200\" width=\"400\"> \n",
    "<br>Test each drug versus the placebo at the 5 per cent level. Report what the result would\n",
    "be under a Bonferroni adjustment, and how the interpretation changes.</b>\n",
    "\n",
    "Each row of data can be considered an independent binomial test with n being num. of patients and k being incidence of Nausea. We can use the placebo to generate the true probaibility that we use as a binomial test for the other rows.<br>\n",
    "\n",
    "However as we are conducting multiple independent trials here the Bonferroni adjustment (which is to divide your $\\alpha$ by the number of tests you are doing could be used to reduce the risk of a Type I error (rejecting the null as over multiple experiments one result happened to be significant). \n",
    "\n",
    "-----------------\n",
    "\n",
    "<b>George Mendel bred four different types of peas, starting with round yellow seeds and wrinkled green seeds. Each pea could result in one of four categories: round yellow, wrinkled yellow, round green and wrinkled green. Mendel's theory dictates that these categories follow a\n",
    "discrete distribution with respective probabilities\n",
    "<br><br>\n",
    "$$p_0 = \\Bigg(\\frac9{16}\\frac3{16}\\frac3{16}\\frac1{16}\\Bigg)$$\n",
    "<br>\n",
    "His experiment had a sample size n = 556, where the observed counts were X =\n",
    "(315, 101, 108, 32). We want to test whether this data supports the theory or not.\n",
    "<br><br>For that, first calculate Pearson's \u001f$X^2$ statistic where $X_j$ is the count data for category $j$, and $E_j$ is the expected count under the null\n",
    "H0 : $p$ = $p_0$, with $p$ being the distribution parameter vector of the multinomial.\n",
    "If we have $k$ categories, T will have a chi-squared distribution with $k-1$ degrees of freedom\n",
    "(\"degrees of freedom\" is just the fancy name given to the parameter of the chi-squared).\n",
    "\n",
    "\n",
    "Describe how you would use this chi-squared statistic to test Mendel's theory.</b>\n",
    "<br>\n",
    "<br>\n",
    "Observed = 315, 101, 108, 32<br>\n",
    "Expected = 312.7, 104.2, 104.2, 34.75<br>\n",
    "$\\chi^2 = \\frac{5.06}{312.7}+ \\frac{10.56}{104.2}+ \\frac{14.06}{104.2}+\\frac{7.56}{34.75} = 0.47$\n",
    "<br><br>\n",
    "We then look for the critical region for for $\\chi^2$ with d.f of 3 (4-1) and confidence of 95%. Using R or an online calulator:\n",
    "http://www.socscistatistics.com/pvalues/chidistribution.aspx\n",
    "we find that the P-Value is 0.925431. Therefore the result is NOT significant at p < 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>You want to  conduct a survey in which you know the population mean to be 0 and need to determine the right sample size for your survey.\n",
    "<br>For a sample size n = 10, 50, 100, consider the test:<br> Null hypothesis $H_0 : \\mu$ (sample mean) = 0 (population mean)<br> vs. Alternative $H_1:\\mu$ (sample mean) <> 0 (population mean) <br>for a sample following a Gaussian distribution N($\\mu, \\sigma^2$), where \u001b$\\sigma^2$\n",
    "is known and the level of the test is $\\alpha$ = 0:05.<br><br> Write down the power of this test as a\n",
    "function of the true mean $\\mu_0$. <br>How would this change if $H_0$ was $\\mu$ <=0?</b>\n",
    "<br><br>\n",
    "a) \n",
    "To calculate a confidence interval you need:\n",
    "$$\\bar{X} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} $$\n",
    "$\\sigma = std. dev$\n",
    "<br>b) We want the test to be at the 95% Confidence Level. For a two tailed test at this level we know that Z=+/- 1.96 for an alpha of 0.05 (95% confidence) spread out across two tails \n",
    "<br><br>\n",
    "c) Therefore we can say our critical region (a fancy word for the remaining 5% not in the confidence interval) would be from $$CriticalRegion = \\bar{X} \\pm 1.96 \\frac{\\sqrt\\sigma}{\\sqrt{10}}$$\n",
    "\n",
    "According to Professor: \n",
    "The sample variance is a function of the data, defined as the sample average of $(x_i – x_bar)^2$, that is, $sum_i{(x_i – x_bar)^2} / n$. It also  gives an estimate of the variance of the data distribution. When n goes to infinite, the sample variance goes to $s^2$ (in most reasonable conditions).\n",
    " \n",
    "$s^2 / n$ is *not* the sample variance. It is the variance X_bar, which has its own distribution that is related to but *not* the same as the distribution of  X_1 or X_2 etc.. Consider the following. Remember from exercise sheet 1 that $Var(aX + b) = a^2 * Var(X)$, for constants a and b. Also, Var(X + Y) = Var(X) + Var(Y) if X and Y are independent.\n",
    " \n",
    "The following then shows how Var(X_bar) is derived from Var(X_1) = Var(X_2) = ... = Var(X_n) = s^2:\n",
    "1.       Var(X_bar) =  Var((X_ 1 + .. + X_n) /  n  = Var((X_1 + … + X_n)) / n^2 =\n",
    "2.       (Var(X_1) + … + Var(X_n)) / n^2 =\n",
    "3.       n * s^2  / n^2 =\n",
    "4.       s^2 / n.\n",
    "We can estimate s^2 / n by “sample variance” / n.\n",
    " \n",
    "Which is not the same as the variance of some other given statistic. For instance, if I take the sample median (sort the data ; pick point in the middle), its variance in general is *not* sigma^2 /n (it will be if the sample median and the sample mean are the same, but in general they are not) so “sample variance” / n might not make sense as an estimate of the variance of the sample median (hence, the bootstrap can help here). If I take the sample maximum (like in question 10 of exercise 3), its variance in general is not sigma^2  /n either.\n",
    " \n",
    "That is, Var(X_1) = … = Var(X_n) = s^2, which is different from Var(Sample average) which may be different from Var(Sample median) which may be different from Var(Sample Maximum) etc.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "George Samson is responsible for quality assurance at Integrated Electronics. Integrated Electronics has just signed a contract with a company in China to manufacture a control device that is a component of its manufacturing robotics products. Integrated Electronics wants to be sure that these new, lower-cost components meet its high-quality standards. George has asked you to establish a quality-monitoring process for checking shipments of control device A. The variability of the electrical resistance, measured in ohms, is critical for this device. Manufacturing standards specify a standard deviation of 3.6, and the population distribution of resistance measurements is normal when the components meet the quality specification. The monitoring process requires that a random sample of n = 6 observations be obtained from each shipment of devices and the sample variance be computed. Determine an upper limit for the sample variance such that the probability of exceeding this limit, given a population standard deviation of 3.6, is less than 0.05.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Describe the null hypotheses to which the p-values given in the table below correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.<br></b>\n",
    "<img src=\"../_img/Q4_1.png\" height=\"300\" width=\"500\"> \n",
    "\n",
    "Null hypothesis $H_0$: No linear relationship between the number of units sold and intercept, TV, radio, or newspaper.\n",
    "So for Intercept, TV, and radio, the small p-value means that there is evidence that\n",
    "these contributed to the behaviour of sales, while newpaper advertising did not\n",
    "(given advertising in other media). It cannot be concluded that spending money\n",
    "on newspaper advertising is useless, only that it seems not to contribute to sales\n",
    "when TV and radio are being used. It also does not mean that changing the way\n",
    "newspaper funds are used would continue not to show an effect on sales, only that\n",
    "the way done in the data does not seem to work.\n",
    "\n",
    "-----------\n",
    "\n",
    "<h4>Interpret the following</h4>\n",
    "<code>\n",
    "Call:\n",
    "lm(formula = mpg ~ horsepower, data = Auto)\n",
    "\n",
    "Residuals:\n",
    "     Min       1Q   Median       3Q      Max \n",
    "-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n",
    "\n",
    "Coefficients:\n",
    "             Estimate Std. Error t value Pr(>|t|)    \n",
    "(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\n",
    "horsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n",
    "\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "\n",
    "Residual standard error: 4.906 on 390 degrees of freedom\n",
    "  (5 observations deleted due to missingness)\n",
    "Multiple R-squared:  0.6059,\tAdjusted R-squared:  0.6049 \n",
    "F-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\n",
    " </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> What do these two charts show you?<b>\n",
    "\n",
    "<img src=\"../_img/ResidualPlot.png\" height=\"300\" width=\"500\"> \n",
    "<img src=\"../_img/LeveragePlot.png\" height=\"300\" width=\"500\"> \n",
    "\n",
    "- Top chart is a residual plot. Residual = the difference between you predicted Y and the actual Y, i.e. you are looking at the errors in your model. Very useful chart - you want your errors to be consistent across the predicted values (aka fitted valaues)\n",
    "- Bottom chart is a similar thing but when you are working in higher dimensions (to be completed...)\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>If your Linear regression model seemed to indicate “If we increase the TV budget by one thousand then, other things being equal, I will sell 400 hundred more units of my product, in expectation.” would this imply it was true?</b>\n",
    "<br><br>\n",
    "No - because there might be hidden causes that is causing both to happen. Example the data from the larger advertising budget came from more economically affluent areas where shoppers have more money to spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Suppose that the number of distinct uranium deposits in a given area is a\n",
    "Poisson random variable with parameter $\\lambda$ = 10. If, in a fixed period of\n",
    "time, each deposit is independently discovered with probability 1/50, find the\n",
    "probability that (i) exactly one, (ii) at least one and, (iii) at most one deposit\n",
    "is discovered during that time.</b>\n",
    "<br><br>\n",
    "i) We have a probability here, so we know a bimial probability distribution can be used.\n",
    "For Binomial:\n",
    "<br>P = 1/50\n",
    "<br>Q = 49/50\n",
    "<br>N = this is where you realize N = the Possion distribution(10) <br>\n",
    "<br>\n",
    "After this it gets complicated. Basically you are iterating the Poisson over the Binomial formula<full answer not shown><br>\n",
    "<img src=\"../_img/Q1_h.png\" width=800>\n",
    "<img src=\"../_img/Q1_h2.png\" width=700>\n",
    "\n",
    "----------------------\n",
    "\n",
    "<b>There is a theory that people can postpone their\n",
    "death until after an important event. Here are the numbers, in a particular year, of\n",
    "elderly Jewish and Chinese women who died just before and after the Chinese Harvest\n",
    "Moon Festival.\n",
    "<img src=\"../_img/Deaths.png\" height=\"100\" width=\"200\">\n",
    "<br>\n",
    "Compare the two mortality patterns using a hypothesis test, explaining your reasoning.\n",
    "</b>\n",
    "<br>\n",
    "Method 1:<br>\n",
    "First summarize the data into period 1 and period 2\n",
    "<img src=\"../_img/Deaths2.png\" height=\"100\" width=\"200\">\n",
    "\n",
    "We want to know whether the probability of deaths between the two groups are the same.\n",
    "$p_1$ = proportion of deaths for Chinese in P1\n",
    "$p_2$ = proportion of deaths for Jewish in P1\n",
    "<br>\n",
    "We can now think of this as two binomial distrbutions, one for Chinese and one for Jewish.  <br>\n",
    "<br>C ~ Binomial(198,$p_1$)  \n",
    "J ~ Binomial(586,$p_2$)\n",
    "<br>\n",
    "We want to know whether the two distributions have the same Proportions of people dying in Period 1. Our null hypothesis is no:\n",
    "<br>Null hypothesis H_0: $p_1$ = $p_2$\n",
    "<br>\n",
    "<br>We assume n is large enough such that the central limit theorem kicks in and that the average of p1 and p2 can be used \n",
    "<b>...fuck it I give up...Use goodness of fit instead</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>One method we have seen for finding confidence intervals is finding pivots such as\n",
    "    $Q(X, \\mu) \\equiv \\frac{(\\hat{\\mu}(X) - \\mu)}{\\hat{\\sigma}\u001b(X)}$, where we explicitly represented empirical average $\\hat{\\mu}$\n",
    "and empirical standard deviation $\\sigma$\u001b as functions of the data X. A pivot is not a\n",
    "statistic as it depends on the unknown parameter, but it has a distribution which\n",
    "does not (N(0, 1) in a typical example).<br>\n",
    "<br>Another way of building a confidence interval is by inverting a test statistic.\n",
    "For instance, suppose we have data $X^{(1)}...X^{(n)} ~ N(\\mu, \\sigma^2)$ from a known $\\sigma^2$ but\n",
    "unknown $\\mu$. Consider some test for H0 : $\\mu$ = $\\mu_0$ at a level $\\alpha$. Describe how the\n",
    "machinery behind this test can be converted into a 1 - $\\alpha$ confidence interval for $\\mu$.</b>\n",
    "\n",
    "First of all let's translate this to plain English.\n",
    "<br>Pivot - refers to anything around which we can build a confidence interval. It's true value will be unknown but it will have a probability distribution, typically a Gaussian one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Consider a study of police stops in 75 different precincts of New York\n",
    "City in a particular year, broken down by the ethnicity of the persons\n",
    "being stopped. There is a record for every combination of precinct\n",
    "and ethnicity. \n",
    "\n",
    "More specifically, each record contains: the precinct\n",
    "code precinct; the ethnicity eth, encoded non-numerically as a set of\n",
    "categorical levels, Black, Hispanic and White; the population of the\n",
    "precinct, pop; and the total number of stops for that year, stops.\n",
    "\n",
    "(a) Describe the likelihood function for a Poisson generalised linear\n",
    "model of the regression of stops on the other variables, justifying\n",
    "your choice of link function.\n",
    "\n",
    "(b) What would be the interpretation of the model if we fixed the\n",
    "coefficient for pop to 1? Would it make more sense to regress on\n",
    "pop or on the logarithm of pop? Explain.</h4>\n",
    "\n",
    "\n",
    "a)\n",
    "\n",
    "* Ok so a Poisson is $$\\frac{\\lambda^ke^{-\\lambda}}{k!}$$ where $\\lambda$ is replaced by our link function \n",
    "* Our liklihood function will be this summed up for all rows of data so:\n",
    "$$\\prod^{225}_{i=1} \\frac{\\lambda_i^{k_i}e^{-\\lambda_i}}{{k_i}!}$$\n",
    "\n",
    "* $\\lambda$ is what we derive from the the dataset, using the log-link function $exp(f(x_n))$\n",
    "\n",
    "* The log-link function is needed as for a Poisson you assume that $f(x) = log(\\theta)$\n",
    "\n",
    "* Finally one way of modelling f(x) is:\n",
    "$$f(x) = \\sum^{74}_{i=1} \\beta_i Precint_i + \\beta_{75}IsBlack + \\beta_{76}IsWhite + \\beta_{77}Population$$\n",
    "\n",
    "(b) Setting $\\beta_{77}$ = 1 would mean that there is an implied change of 1 unit of log(f(x))\n",
    "per extra person observed in the precint. \n",
    "\n",
    "This implies an exponential increase of f(x) per units of increase of pop. \n",
    "\n",
    "This does not seen adequate, so using the logarithm of pop instead as a covariate sounds more reasonable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "(c) Alternatively, what modelling issues would you have to face if the\n",
    "outcome variable was deemed to be the ratio of stops by pop?\n",
    "\n",
    "(d) Figure 1 is a plot where the horizontal axis is the predicted value\n",
    "of $\\hat y$ of stops for each data point, versus the residual $y - \\hat y$  on the\n",
    "vertical axis, where each y is the observed value of stops. The\n",
    "bottom and top dashed lines represent an approximate confidence\n",
    "interval of 95% under the Poisson family. Explain what this figure tells you about the fit of the model and which recommended change to the model you would make.\n",
    "\n",
    "(e) The data has 225 rows, since each of the 75 precincts is combined\n",
    "once with each of the three ethnicities. Which unwanted implica-\n",
    "tions this may have to your analysis, and what would you attempt\n",
    "to do to mitigate this?\n",
    "</h4>\n",
    "\n",
    "c. If our outcome Y is now given by stops/pop, this would keep the\n",
    "attractive interpretation of coefficients modelling directly the rate\n",
    "of stop per population. This adopts a default intuitive relationship\n",
    "between the two variables, avoiding the need to interpret what\n",
    "$\\beta_{77}$ might mean otherwise. However, this is now a continuous\n",
    "outcome variable. The Poisson model cannot be used anymore. A\n",
    "Gaussian model with independent error terms might be a problem\n",
    "if for some reason we need the variance to increase with the mean\n",
    "of the ratio stops/pop.\n",
    "<br><br>\n",
    "\n",
    "d. There seems to be a small but non-neglible number of points beyond what would be expected for a 95% interval. One possibility is that the variance implied by the Poisson is not large enough to\n",
    "model this data. Hence, I would recommend other model for this\n",
    "regression problem, such as the negative binomial, which allows\n",
    "for large variance/mean relationships.\n",
    "\n",
    "Another explanation could be that as the varince gets worse with higher number of stops it implies that it's for the larger populations that the model starts to break down. Hence why the stops per population ratio mentioned above may work better.\n",
    "<br><br>\n",
    "\n",
    "e. The model assumes that data points are independent, but since\n",
    "some points share information from the same sources (such as\n",
    "precints), the assumption of independence might be unrealistic.\n",
    "This leads, for instance, to optimistic confidence intervals (there\n",
    "is actually less information in the data than the independence\n",
    "assumption implies). One way of mitigating this is to use a model\n",
    "that allows for dependencies across some data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Key revision notes\n",
    "------------------\n",
    "\n",
    "* $P(Y=y) = \\sum_{n=0}^{\\infty} P(Y=y | N) P(N)$\n",
    "* Unlike unpenalized least-squares regression, regression with shrinkage (aka Regularization) is scale-dependent\n",
    "* When doing the GLM question, the liklihood function is your end result and simply your pdf function with either $\\int$ or $\\prod$ in front of it\n",
    "* The link function for a Exponential distribution is $y^{-1}$ and for Benie or Bi: $\\frac{Exp(y)} {1 + Exp(y)}$\n",
    "* Bias-Variance trade off in regulaization is what you already know - greater regularization (bias) means less \n",
    "\n",
    "* Memorise this for turning a y value into logistic regression output: $1/(1+e^{-y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-variate distributions:\n",
    "    \n",
    "* The pdf of a p-dimensional Gaussian would have a px1 mean vector $\\mu$ and a pxp symmetric co-variance matrix S\n",
    "* The co-variance martrix is added as a feature to allow covariates to be modelled\n",
    "\n",
    "* The mean determines where the center is, and the covariance will dermine whether the variance is equally spread out around the mean (co-variance of 0) or if one side is getting squashed so that a lot of the variance occurs in the other direction\n",
    "\n",
    "<img src=\"../_img/Stats/Stats005.png\" height=\"300\" width=\"700\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Probability</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<H4>Q: What  is a discreet random variable?</H4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: variable representing an uncertain variable based off a probability distribution. Given {X=x), X is the discreet random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H4>Q: What is a probability mass function?</H4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name given to the overall probabilty distribution. Can be represented as a chart or as a formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Q: List the fundamental rules of probability</H4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) UNION (OR) rule:</b> $$p(a \\ or \\ b) = p(a) + p(b) - p(both)$$  i.e. a union b = p(a) + p(b) - p(a intersect b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2) Product/Joint probability rule:</b>\n",
    "$$p(a,b)=p(a|b)*p(b) = p(b|a)*p(a) $$\n",
    "\n",
    "i.e. Specific combination of a & b = prob. of one given the other * prob of the other\n",
    "\n",
    "<b>3) SUM rule / Marginal distribution:</b>\n",
    "$$p(a)=\\sum_{b} p(a,b) =\\sum_{b} p(a|b)*p(b) $$\n",
    "i.e. p(a) across all the possible values of b\n",
    "\n",
    "<b>4) Bayes Rule</b>\n",
    "$$ P(a \\mid b) = \\frac{P(b \\mid a) \\, P(a)}{P(b)}=\\frac{P(a,b) \\,}{P(b)} $$\n",
    "The denominator here can be extended using the Marginal distribution rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5) Explain the chain rule</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(A,B,C) = P(A| B,C) P(B,C) = P(A|B,C) P(B|C) P(C)\n",
    "\n",
    "P(A, B, ..., Z) = P(A| B, ..., Z) P(B| C, ..., Z) P(Y|Z) P(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: Explain conditional independence</h4>\n",
    "\n",
    "Ans: If a and b are independent then p(a and b) = P(a) + p(b)\n",
    "Unfortuntely total independence is rare. Instead two variables may be independent under certain scenarios. \n",
    "Hence we can say, a and b are independent, given c.\n",
    "\n",
    "If a <i>independent</i> b | c, then p(a,b|c) = p(a|c)*p(c)\n",
    "\n",
    "i.e. If a and b are independent given c, then the probability of one of those, let's say a, is gonna be the probability of a given c, times the probability of c happening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: What is Expectation? </h4>\n",
    "\n",
    "* Another way of saying average\n",
    "* The average value of some function f(x) under a probability distribution p(x)\n",
    "\n",
    "$ \\mathbf{E}(f) = \\displaystyle \\sum p(x)f(x)  $ -- discreet case\n",
    "\n",
    "$ \\mathbf{E}(f) = \\displaystyle \\int p(x)f(x)dx  $ -- continuous case\n",
    "\n",
    "* Expeciation can be estimated from a N samples drawn from a probabilty distribution function:\n",
    "\n",
    "$ \\mathbf{E}(f) \\simeq \\frac1 N \\sum f(x_n)$\n",
    "\n",
    "Note: Multiplying by \\frac1 N is the same as dividing by N. You'll see this a lot in machine learning formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: What is a prior?</h4>\n",
    "\n",
    "Ans: A prior probability is the probability distribution based on a belief of something taking on a certain value.  So for example if I had to guess the age of \"a doctor\" my prior would be based on the fact they are likely to be between 26 and say 55."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: What is maximum liklihood estimation</h4>\n",
    "\n",
    "A method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters.\n",
    "\n",
    "In formulas:<br>\n",
    "$\\theta$ represents the parameter <br>\n",
    "$\\theta$ can be one more variables (e.g. mean and std. dev)<br>\n",
    "\n",
    "Max $p(data |  \\theta)$ across all possible values of $\\theta$<br>\n",
    "=Max $p(X_i =x_i|\\theta)$ across all possible values of $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro's:\n",
    "- Easy to compute & Interpret<br>\n",
    "- Asymptotically Consistent (converges towards to true solution as side of data, N, increases)\n",
    "- Lowest asymptotic variance (lowest possible error)\n",
    "- Invariant: Any transformation on the real $\\theta$ can also be applied to the MLE $\\theta$\n",
    "\n",
    "Cons:<br>\n",
    "- Point estimate - no indication of how much uncertainty there is\n",
    "- $\\theta$ may not be unique - could have more than 1 solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decision Theory</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Theory is concerned with making a decision based on probabilities and particularly Bayes Theorem.\n",
    "How this is applied to machine learning and classification is examined here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: What is a decision region, decision boundary?</h4>\n",
    "\n",
    "Ans: \n",
    "* Decision region - a subset of your solution space that has been labelled as one classification.\n",
    "* Decision boundary - the boundary between decision regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q: Describe minimizing the risk of misclassification</h4>\n",
    "    \n",
    "For classification we can either minimize the probability of misclassification or maximize the probability of correct classification. We can model the decision in terms of Bayes theorem and then pick the classicification based on the whichever option has the lower (minimization of risk) or higher (maximization of being correct) probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Q: What is a loss function?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Also known as cost function. \n",
    "    \n",
    "$$ \\mathbf{E}[L] = \\sum_k \\sum_j \\int_{R} L_{kj}p(x,C_k)dx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translates to: Expected Loss = Loss matrix (L_kj) * probability of it being (x and each possible Class).  This is expressed as a continuous solution space rather than discreet (so we are interested in the probability of a region, not a point) hence the integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

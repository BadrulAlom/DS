{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# <center>Applied Machine Learning Revision Q&A</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Topics covered in class</h3>\n",
    "\n",
    "<br>[<b>1. Linear Regression</b>](#Linear Regression)\n",
    "<br>[Least Squares Loss](#Loss)\n",
    "<br>[Auto Regressive Models](#AutoRegresiveModels)\n",
    "<br>[Radial Basis Functions](#Radial)  -- to do\n",
    "\n",
    "<br>[<b>2. Clustering and Classification</b>](#Clustering)\n",
    "<br>[Logistic Regression](#LogisticRegression)\n",
    "<br>[Maximum Liklihood](#MaxLiklihood)  -- to do\n",
    "<br>[Nearest  Neighbours](#Nearest Neighbours)\n",
    "<br>[Naieve Bayes](#NaieveBayes)\n",
    "<br>[SVMs](#SVM)  -- to do\n",
    "<br>[Decision Trees](#DecisionTrees)  -- to do\n",
    "<br>[Mixture Models](#MixtureModels) -- to do\n",
    "\n",
    "<br>[<b>3. Dimensionality Reduction</b>](#Dimensionality Reduction)\n",
    "<br>[PCA](#PCA)\n",
    "<br>[Non-Negative Matrix Factorization](#NNMF)\n",
    "<br>[ICA](#ICA)\n",
    "<br>[Fishers Linear Discrimnate & Canonical Variables](#Fishers)\n",
    "\n",
    "<br>[<b>4. Optimization</b>](#Optimization)\n",
    "<br>[Gradient Descent](#Gradient Descent)\n",
    "<br>[Line search & conjugate gradients](#ConjugateGradients)\n",
    "<br>[Higher order methods](#HigherOrder)\n",
    "\n",
    "<br>[<b>5. Large Scale Machine Learning</b>](#LargeScale)\n",
    "<br>[Stochastic Gradient Descent](#SGD)\n",
    "<br>[Sparsity](#SGD)\n",
    "<br>[Batch vs Online](#BatchOnline)\n",
    "\n",
    "<br>[<b>6. Neural Networks</b>](#NN)\n",
    "<br>[Auto Encoders](#AutoEncoders)\n",
    "<br>[CNNs](#CNNs)\n",
    "<br>[NLP](#NLP)\n",
    "<br>[RNNs](#RNNs)\n",
    "<br>[Gradient Decay/Explosion](#DecayExplosion)\n",
    "<br>[LSTM](#LSTM)\n",
    "<br>[SeqToSeq and Bidirectional](#Seq)\n",
    "<br>[Parameter Tying](#ParamTying)\n",
    "<br>[BackProp Through Time](#BPTime)\n",
    "<br>[Auto Differentiation](#AutoDiff)\n",
    "<br>[Initialization](#Initializtion)\n",
    "\n",
    "<br>[<b>7. Visualization</b>](#Visualization)\n",
    "<br>[T-SNE](#TSNE)\n",
    "\n",
    "<br>[<b>8. Fastest Nearest Neighbours</b>](#FastestNN)\n",
    "<br>[Orchard](#Orchard)\n",
    "<br>[AESA](#AESA)\n",
    "<br>[KD Trees](#KD Trees)\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Useful Latex Math symbols</h4>\n",
    "\n",
    "http://web.ift.uib.no/Teori/KURS/WRK/TeX/symALL.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Linear Regression'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">0. Key questions</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain Nearest Neighbours, Mahalanobis distance, and K-NN\n",
    "2. Explain Naieve Bayes\n",
    "3. [Explain the PCA algorithm](#PCA)\n",
    "4. d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Linear Regression'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">1. Linear Regression</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='LeastSquares'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Least Squares Loss</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain the issues of overfitting and describe what we add to the loss function to prevent it</h4>\n",
    "\n",
    "* Overfitting is when a statistical model describes random error or noise rather than the underlying relationship\n",
    "* Adding a regularization term such as $\\lambda w^2$ to penalise rapid changes in the output helps reduce this\n",
    "* Adding an L2 regularization (aka Ridge Regression) to the previous solution would make the w:\n",
    "$$w=\\left(\\sum_{n=1}{N}x^n(x^n)^T+\\lambda w^Tw\\right)^{-1}\\left(\\sum_{n=1}^{N}y^nx^n\\right)$$\n",
    "\n",
    "Shapes for regularisation penalty $\\sum_i | w_i|^q$\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml006.png\" height=\"100\" width=\"600\">\n",
    "\n",
    "* As we decrease q towards 0 we get the ‘ideal’ regulariser that selects weights\n",
    "which are 0 if they do not contribute.\n",
    "* For q = 1 the objective function for squared loss linear regression remains\n",
    "convex and easy to optimise\n",
    "* The objective function is complicated (non convex) for q < 1\n",
    "\n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is the difference between L1 and L2 regularization?</h4>\n",
    "\n",
    "L1\n",
    "* Term is: sum of abs values of w\n",
    "* Heavy and small $w_i$ is penalized by the a constant proportional amount\n",
    "* Penalty can't be differentiated so requires special optmization routines\n",
    "* But will mean that only significant weights remain\n",
    "* In practice (e.g. deep learning) people don't worry about it being non-differentiable and just proceed with gradient based training as normal\n",
    "\n",
    "L2\n",
    "* Term is: $ w^T w$ or $\\sum_i{w^2_i}$\n",
    "* Penalizes large W more than small ones\n",
    "* Penalty is differentiable\n",
    "* Small weights will still persists\n",
    "\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>For a dataset of inputs x and scalar outputs y, ${(x^n,y^n),n=1, ...,N}$, linear regression is the model\n",
    "\n",
    "$$y=w^Tx$$\n",
    "<br>\n",
    "i. The least squares criterion with regularizer term, sets w based on minimizing\n",
    "\n",
    "$$E(w) =\\sum^{N}_{n=1}(y^n-w^Tx^n)^2+\\lambda w^Tw$$\n",
    "\n",
    "Show that the optimal solution is given by\n",
    "\n",
    "$$w= \\left(\\sum_{n=1}^{N}x^n(x^n)^T + \\lambda I \\right)^{-1}\\left(\\sum_{n=1}^{N}y^nx^n\\right)$$</h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "Useful: http://www.statpower.net/Content/310/Summation%20Algebra.pdf\n",
    "<br><br>\n",
    "Starting with\n",
    "\n",
    "$$E(w) =\\sum^{N}_{n=1}(y^n-w^Tx^n)^2+\\lambda w^Tw$$\n",
    "\n",
    "1. Differentiate: \n",
    "$$\\frac{dE}{dw}=2\\sum^N_{n=1} (y^n-w^Tx^n)x^n + \\lambda w= 0$$\n",
    "\n",
    "\n",
    "2. Divide both sides by -2 then multiply out with $x^n$:\n",
    "$$\\sum^N_{n=1} (y^n)x^n - \\sum^N_{n=1}(w^Tx^n)x^n + \\lambda w = 0$$\n",
    "\n",
    "3. Using summation rule 2 to move the w out\n",
    "$$\\sum^N_{n=1} (y^nx^n) - w^T\\sum^N_{n=1}(x^n)(x^n)^T + \\lambda w = 0$$\n",
    "\n",
    "4. Note that the regularizer can also be written $w\\lambda$. Reverse multiply out with the w in the regularizing term (I gets introduced to keep $\\lambda$ in vector form once w disappears)\n",
    "$$ \\sum^N_{n=1} (y^nx^n) w \\left( \\sum^N_{n=1} ( x^n)(x^n)^T + \\lambda I \\right)  = 0$$\n",
    "\n",
    "5. Move yx to the right hand side\n",
    "$$ \\left(w^T\\sum^N_{n=1}x^n(x^n)^T \\right) + w \\lambda = \\sum^N_{n=1} (y^nx^n)$$\n",
    "\n",
    "6. Use inversion to divide\n",
    "\n",
    "$$w= \\left(\\sum_{n=1}^{N}y^nx^n\\right) \\left(\\sum_{n=1}^{N}x^n(x^n)^T + \\lambda I \\right)^{-1}$$</h4>\n",
    "\n",
    "<br>\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "<h4>Give also an expression for the computational complexity required to find $w_{opt}$ using this expression and explain why in practice it is not recommended to find w by using matrix inversion as above, explain an alternative procedure</h4>\n",
    "\n",
    "* You don't find the matrix inversion because it's an O($n^3$) calculation and you also have to store the hessian at every iteration which is also very expensive\n",
    "* So you just do gradient calculations as you don't need a $x^Tx$ calculation\n",
    "\n",
    "\n",
    "* Gaussian elimination is faster and numerically more stable\n",
    "\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Show that the least squares error criterion \n",
    "\n",
    "$$E(w) =\\sum^{N}_{n=1}(y^n-w^Tx^n)^2$$\n",
    "<br>\n",
    "can be written in the form \n",
    "<br>\n",
    "$$E(w) = w^TAw -2w^Tb+c$$\n",
    "<br>\n",
    "for suitably defined A,b,c\n",
    "</h4>\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='AutoRegressiveModels'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Auto Regressive Models</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Radial'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Radial Basis Functions</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Clustering'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">2. Clustering & Classification</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='LogisticRegression'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Logistic Regression</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is the difference between linear and logistic regression?</h4>\n",
    "\n",
    "* Logistic regression seeks to predict a class of data (binary or multi-class)\n",
    "* Like other linear regression inputs may be linear or categorical\n",
    "* The observed label in (binary) logistic regression is however a zero-or-one variable\n",
    "* The logistic regression estimates the odds, as a continuous variable\n",
    "* In some applications the odds are all that is needed. In others, a specific yes-or-no prediction is needed for whether the dependent variable is or is not a case; this categorical prediction can be based on the computed odds of a success, with predicted odds above some chosen cutoff value being translated into a prediction of a success.\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain this slide</h4>\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml003.png\" height=\"100\" width=\"600\">\n",
    "\n",
    "Top formula is saying\n",
    "* iterate through every data value and multiply up the estimated probability of it being in the class that its actually in\n",
    "* Within the iteration the exponentials are used to choose between the left or right probability as $c^n$ will be 1 for one of them and 0 for the other\n",
    "* The bottom formula is kinda the same thing - $w^Tx^n$ works out some value indicating prob but not likely to be between 0 and 1, +b is the overall average error constant, the sigmoid converts it to 0 to 1, and then we take the log of this\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Consider binary classification problems with class c $\\epsilon$ {0, 1}. Logistic Regression models\n",
    "the probability of input vector x being in class c =1 as \n",
    "$$p(c = 1 | x,w) = 6 (wTx)$$\n",
    "\n",
    "for input vector x and weight (parameter) vector w, where \n",
    "\n",
    "$$\\sigma(x)=\\frac{e^x}{1+e^x}$$\n",
    "\n",
    "The dataset is $D = {(x^n , c^n) ,n =1,...N}$,\n",
    "<br><br>\n",
    "a. Assuming that the data is independently and identically distributed, show that the\n",
    "gradient of the log likelihood is given by\n",
    "$$\\sum^N_{n=1}(2x^n-1)\\sigma(1-2c^n)x^n$$\n",
    "<br><br>\n",
    "</h4>\n",
    "* Liklihood is:\n",
    "$$\\prod^N_{n=1} p(c =1 | x^n,b,w)^{cn} (1-p(c= 1 | x^n,b,w))^{(1-c^n)}  \\quad (where \\  c^n = 0 \\  or \\  1)$$\n",
    "\n",
    "* Log-liklihood is:\n",
    "    $$L(w,b) = \\sum^N_{n=1}c^n log \\sigma (b+w^Tx^n)+ (1-c^n) log (1-\\sigma(b+w^Tx^n))$$\n",
    "\n",
    "to be completed...\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Consider training logistic regression in which the components of the input vector x\n",
    "can be on very different scales and also potentially highly correlated. Discuss how\n",
    "you might adapt the gradient ascent based approach to training logistic regression.\n",
    "<br><br>\n",
    "</h4>\n",
    "\n",
    "* Gradient ascent is easy to implement but slow to converge\n",
    "<br><br>\n",
    "* Since the surface has a single optimum, a Newton update $w^new = w^old + ηH^{−1}g$ (where H is the Hessian matrix as above and η is between 0 and 1) will typically converge much faster than gradient ascent.\n",
    "<br><br>\n",
    "* However, for large scale problems with dim (w) >> 1, the inversion of the Hessian is computationally demanding and limited memory BFGS or conjugate gradient methods are more practical alternatives.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Missing data is a common problem in practice. Describe how you might adapt logistic regression when there are missing elements in the input data vectors. Discuss the advantages and disadvantages of your approaches.\n",
    "</h4>\n",
    "<br><br>\n",
    "\n",
    "Can apply the conjugate gradient descent method\n",
    "<br><br>\n",
    "<h4>\n",
    "In classification we often have an associated 'loss' function for each class. For\n",
    "example it might be that it is important in a medical situation to detect cancer, even\n",
    "if some of the detections are actually false. We can use a loss function L(ctrue, cpd)\n",
    "to measure our loss when our predicted class is Cpred whereas the truth is ctrue.\n",
    "<h4>\n",
    "i. Explain how to adapt logistic regression to minimize expected loss L(w) and derive a gradient based training scheme.\n",
    "<br><br>\n",
    "https://www.youtube.com/watch?v=IxotEG3yWHs\n",
    "\n",
    "<br><br>\n",
    "Replace the quadratic loss function with a Cross-entropy loss function\n",
    "\n",
    "<br><br>\n",
    "\n",
    "f. Comment on the geometric structure of the objective function L(w) and discuss any potential computational issues involved in training this model.\n",
    "<br><br>\n",
    "</h4>\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='NaieveBayes'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Naieve Bayes</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is Naieve Bayes</h4>\n",
    "\n",
    "<H2>Naieve Bayes</H2>\n",
    "<br>\n",
    "<h4>The algorithm</h4>\n",
    "\n",
    "$${\\begin{aligned}p(Y_{k}\\vert x_{1},\\dots ,x_{n})&\\varpropto p(Y_{k})\\prod _{{i=1}}^{n}p(x_{i}\\vert Y_{k})\\,.\\end{aligned}}$$\n",
    "\n",
    "\n",
    "* The maths behind it is explained very well in the <a href=\"http://scikit-learn.org/stable/modules/naive_bayes.html\">Skikit-learn pages</a>\n",
    "\n",
    "* p(x) can consist of mutiple things .. i.e.  inputs $x_1$,$x_2$,$x_3$\n",
    "* if the inputs $x_1$,$x_2$,$x_3$ are \"naievely\" assumed to be independent of each other (more on this later), so they don't impact each others probability of y, then we can use the multiplicative rule or probability and simplify the top to: \n",
    "$$p(Y_k)\\prod_{{i=1}}^{n} p(x_{i}\\vert Y_{k})$$\n",
    "* We do not need a denominator for this as we can say that it's a constant, having analyzed a corpus of data and determined the p of each x. For purposes of classification we can therefore ignore it.\n",
    "\n",
    "\n",
    "<i>$\\varpropto$ means in proportion to</i>\n",
    "\n",
    "Notes:\n",
    "<br>\n",
    "* The ability to assume independence under a given class is a crucial part of applied Bayes. So the words 'great' and tremendous' are postively correlated in a movie review, but if you took it for granted that the movie review was positive, then the probability of 'great' is unlikely to correlate as strongly with 'tremendous' - the two may be equally likely for instance.\n",
    "\n",
    "* Naieve Bayes is very fast to train, deals with more than two classes, and can deal with missing data\n",
    "* In the case of low counts the method can be overconfident although the use of pseudocounts can help (see literature)\n",
    "<br>\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='NearestNeighbour'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Nearest Neighbour</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain Linear Separability</h4>\n",
    "\n",
    "* If all the data for class 1 lies on one side of a hyperplane, and for class 0 on the\n",
    "other, the data is said to be linearly separable.\n",
    "* We can map using a non-linear vector function ψ(x) to make it linearly separable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the Nearest Neighbour algorithm</h4>\n",
    "\n",
    "* New item x is classified based on the class of it's nearest neighbour in the dataset $x_1 ... x_n$\n",
    "* Distance is measured as the sum of the Euclidean squared distance across all dimensions D\n",
    "$$d(x,x') = \\sqrt{\\sum_{i=1}^{D}(x_i - x'_i)^2}$$\n",
    "* PCA can be used in cases of a high number of dimensions to improve calculation speed and improve results\n",
    "* It is not clear how to deal with missing data however or incorporate prior beliefs and domain knowledge\n",
    "\n",
    "<h4> What is Mahalanobis distance?</h4>\n",
    "This is where you multiply the distance by the covariance matrix of the input (from all classes) which rescales the input vector and thus large values dominate less\n",
    "\n",
    "$$d(x, x') = (x − x' ) S^{−1} (x − x')$$\n",
    "\n",
    "where S is the covariance matrix of the inputs (from all classes)\n",
    "\n",
    "This helps make the input vector more scale tolerant (inputting as mm vs cm should provide similar results)\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is KNN?</h4>\n",
    "\n",
    "* Extension of NN where you use the class of the K nearest neighbours not just the 1st nearest\n",
    "* We first work out the liklihood model for each class (across all the data of that class):\n",
    "$$ p(x|c=0) = \\frac{1}{N_{class0}} \\sum_n \\mathcal{N}(x|x^n, \\sigma^2I)$$\n",
    "$$ p(x|c=1) = \\frac{1}{N_{class1}} \\sum_n \\mathcal{N}(x|x^n, \\sigma^2I)$$\n",
    "\n",
    "\n",
    "* We work out the probability of a point being within a normal distribution of every other point\n",
    "* To deal with outliers we create aritifical points where the mean of the class is, and give it a very large variance. That way the outlier's class is not infuenced by whatever is closes and effectively reverts back to the prior\n",
    "\n",
    "To then classify a new data point we use Bayes rule:\n",
    "\n",
    "$$ p(c=0|x^{*}) = \\frac{p(x^* | c = 0)p(c=0)}{p(x^{*}|c=0)p(c=0)+p(x^{*}|c=1)p(c=1)}$$\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Dimensionality Reduction'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">3. Dimensionality Reduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='PCA'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">PCA</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Good resource: https://www.coursera.org/learn/machine-learning/lecture/ZYIPa/principal-component-analysis-algorithm\n",
    "http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
    "\n",
    "\n",
    "<h4>Explain the PCA algorithm</h4>\n",
    "\n",
    "<b>Pre-processing:</b>\n",
    "1. If the mean of the data is not 0 to begin with center the data by substracting the mean of each dimension, D\n",
    "2. If the different features have vastly different scales then (e.g. size of house, number of rooms) then normalize, typically done by dividng by the std. deviation\n",
    "\n",
    "<b>Main PCA algorithm:</b>\n",
    "1. Formulate the co-variance matrix, S, as a DxD matrix: $S=\\frac{1}{N} XX^T$\n",
    "\n",
    "2. If using Eigen-decomposition approach (theoretical way):\n",
    "    * Calculate the Eigenvectors of S such that they are each orthogonal to the ones previously\n",
    "    * B = top M eigenvectors\n",
    "    * Complexity based on eigen-decomposition is O($D^3$)\n",
    "\n",
    "3. In practice, it can be impractical to first compute the covariance matrix and then the eigenvalues. Instead we can use the SVD approach which works here as S is a positive semi-definite matrix. This gives a more stable result:\n",
    "    * the SVD of S gives you $UDV^T$\n",
    "    * We only care about the first M columns of U, which form the basis matrix B.\n",
    "\n",
    "4. Reduced dimensional representations is then $Y = B^T \\tilde X$.\n",
    "5. The approximate higher dimensional reconstruction back again is given by $\\tilde X$ ≡ BY + M where M = is a vector of means that you are adding back on (see pre-processing). Also note it's B not $B^T$ here.\n",
    "6. The error is calculated as the sum of the squares difference between, $\\tilde x$, and x\n",
    "\n",
    "Notes:\n",
    "\n",
    "* B is a D x H dimensional matrix (for every dimension you have a reduced H mapping)\n",
    "* B consists of vectors that are orthogonal to the original data points. But it does not take into account the rotation - the data that is projected back out could fit the same variance as the original but rotated in the dimension space.\n",
    "\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Principal Component Analysis (PCA) is a method to form a lower-dimensional representation of data. For datapoints $x^n,n=1,...N$, define the matrix\n",
    "\n",
    "$$X=[x^1,x2...x^N]$$\n",
    "\n",
    "That is, for datapoints x with dimension D, then X is DxN dimensional. The data is such that the mean is zero.\n",
    "\n",
    "The covariance matrix of the data S, has elements \n",
    "\n",
    "$$S_ij=\\frac{1}{N}\\sum^N_nx^n_ix^n_j$$\n",
    "\n",
    "1. Explain how to write S in terms of matrix multiplication X</h4>\n",
    "<br>\n",
    "S is a co-variance matrix therefore DxD so: $$S=\\frac{1}{N} XX^T$$\n",
    "\n",
    "<hr />\n",
    "\n",
    "<h4>2. PCA is based on the linear model of the data\n",
    "\n",
    "$$x^n\\approx My^n$$\n",
    "\n",
    "where M is a DxH dimensional matrix, and each $y^n$ is a H dimensional vector, with $H<D$. \n",
    "<br><br>\n",
    "With reference to an orthognal matrix\n",
    "<br>\n",
    "$$R^TR=I$$\n",
    "<br>\n",
    "explain why there is no unique setting in general for M and $y^n$. Use also a diagram to explain the geometric meaning of this result\n",
    "</h4>\n",
    "\n",
    "* The log-liklihood is the same even with an orthogonal rotation of B --> RB where $R^TR$ = I. \n",
    "* draw a line on a diagonal line on a grid with dots around it, rotate the line and show how the variance captured by the line can stay the same even though it has rotated\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Datapoints $x^n,n=1,...N$, define the matrix\n",
    "$$X=[x^1,x^2,...x^N]$$\n",
    "\n",
    "That is, for data points x with dimension D, then X is D $\\times$ N dimensional. The data is such that the mean is 0, that is:\n",
    "\n",
    "$$\\sum_{n=1}^{N}x^n=0$$\n",
    "\n",
    "K-dimensional PCA aims to find a representation:\n",
    "\n",
    "$$x^n\\equiv \\sum_{k=1}^{K}y^n_kb^k$$\n",
    "\n",
    "where $b^1,...b^K$ are 'basis' vectors and $y^n_k$ are coeffcients\n",
    "<br>\n",
    "<br>\n",
    "Explain how to efficiently compute the basis vectors and coefficients in order to minimize the squared loss between the approximation and each $x^n$, namely:\n",
    "\n",
    "$$\\sum_{n=1}{N}\\left(x^n-\\sum_{k=1}^{K}y^n_kb^k\\right)^2$$</h4>\n",
    "\n",
    "Ans:\n",
    "* Basis vectors can be calculated through finding Eigenvectors of the co-variance matrix or by performing Singular Value Decomposition of the co-variance matrix\n",
    "* In either case we pick the top M vectors to form the matrix B\n",
    "* The co-efficients are the lower dimensional representation of the data and can be found by:\n",
    "$Y = B^T X$.\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain why PCA is often used as a pre-processing step in machine learning and explain what the geometric meaning of PCA is</h4>\n",
    "\n",
    "http://visionandbeyond.blogspot.co.uk/2014/08/how-to-apply-pca.html\n",
    "\n",
    "* PCA is used as it reduces the data dimensions to something more manageable, and in particular by making the matrix more dense, so the algorithm is more efficient\n",
    "* The geometric meaning of is that PCA1 capture the greatest distance in the dimension space, so if the data as shaped like an oval in a 2d space, it would capture the diameter from one end to the other. PCA2 captures the the distance that is perpendicular to this (so the second greatest distance) and the process continues until principal components count = desired num of dimensions and <=D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain how to write the co-variance matrix S, in terms of the matrix multiplication of X</h4>\n",
    "* As we have assumed the mean is zero we can write: \n",
    "$$S = \\frac{1}{N-1}XX^T$$\n",
    "\n",
    "where N = num of observations\n",
    "<br>\n",
    "(Dummys note: so we are matrix-multiplying X with itself to find the strength of the correlation, and taking the average)\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Consider the situation in which the datapoints x n are very sparse - that is, only a few elements of each vector x n are non-zero, resulting also in a sparse matrix S. Describe a computationally efficient procedure to estimate the principal direction (the largest eigenvector of S) and explain why this is efficient.</h4>\n",
    "\n",
    "* One way of finding the principal eigenvector (the one with the larges value, PCA1) is to take any non-zero vector, and repeatedly multiply it with the covariance matrix\n",
    "* This process will converge to the principal eigenvector (eigenvector with the largest eigenvalue). \n",
    "* This is particularly efficient for the case that S is sparse since each matrix-vector product will be fast\n",
    "\n",
    "See: https://www.youtube.com/watch?v=fKivxsVlycs\n",
    "\n",
    "-----------------\n",
    "\n",
    "<h4> Continuing with the sparse dataset scenario, what would be a technique to perform full PCA (not just the principal eigenvector) efficiently?</h4>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Sparse_PCA\n",
    "\n",
    "*  Using the method described above one could find the princial eigenvector then deflate \n",
    "\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> How do you break rotational invariance in PCA</h4>\n",
    "\n",
    "* You find eigen vectors that are orthogonal to one another\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " <h4> Explain why the SVD method is related to the eigen decomposition of S and explain the computational complexity of this approach to performing PCA compared to directly computing the eigen-dcomposition of S</h4>\n",
    "* The SVD approach is equivalent to finding an eigen-decomposition of the sample covariance matrix and then taking the leading M eigenvectors and their correspoding eigenvalues $\\lambda_i$\n",
    "* $\\lambda_i = D^2_ii$ of S V D\n",
    "\n",
    "* Using the SVD to perform PCA makes much better sense numerically than forming the covariance matrix to begin with, since the formation of $XX^T$ can cause loss of precision\n",
    "\n",
    "* PCA requires calculating the eigenvalues and eigenvectors of the covariance matrix, which is the product $XX^⊤$, where X is the data matrix and when the mean is 0.\n",
    "\n",
    "* Since the covariance matrix is symmetric, the matrix is diagonalizable, and the eigenvectors can be normalized such that they are orthonormal\n",
    "\n",
    "* PCA corresponds to setting B = $U_M$ and the eigenvalues are the diagonal elements of $D_M$ squared.\n",
    "\n",
    "\n",
    "\n",
    " -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is the orthonormality constraint in PCA?</h4>\n",
    "* $B^TB$ must $= I$, so that the basis vectors are mutually orthogonal and of unit length.\n",
    "* Done using Larange multipliers so that the objective is to minimize:\n",
    "$$-trace(SBB^T)+trace (L(B^TB-I))$$\n",
    "\n",
    "-----------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain Canonical Variates</h4>\n",
    "\n",
    "A form of dimensionality reduction in supervised learning that seeks to encode class distinction in the basis vectors\n",
    "\n",
    "1. Compute the between and within class scatter matrices A, and B.\n",
    "2. Compute the Cholesky factor $\\tilde B$ of B.\n",
    "3. Compute the L principal eigenvectors [$e_1 , . . . , e_L$ ] of $\\tilde B^{-T} A \\tilde B^{−1}$.\n",
    "4. Return W = [$e_1 , . . . , e_L$] as the projection matrix.\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='NNMF'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Non Negative Maxtrix Factorization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>PCA can be considered a form of matrix factorisation. An alternative matrix factorisation method is probabilistic latent semantic analysis (PLSA) (also called non\n",
    "negative matrix factorisation). This takes a positive matrix X whose entries all sum to 1:\n",
    "    \n",
    "$$\\sum_{ij}X_{ij}=1, \\quad 0\\leq X_{ij} \\leq 1$$\n",
    "\n",
    "and forms an approximation based on\n",
    "\n",
    "$$X_ij \\approx  \\sum^H_{k=1}U_{ik}V_{kj}$$\n",
    "\n",
    "for matrices U and V non-negative entries and $\\sum_iU_{ik} = 1 \\quad and \\quad \\sum_kV_{kj}=1$\n",
    "<br><br>\n",
    "1. Explain what are the typical characteristics of the 'eigenfaces'in PCA compared with the 'plsa' faces in Matrix Factorization.\n",
    "<br><br>\n",
    "2. Derive an algorithm to find U and V based on an interpretation of X, U and V in terms of probability distributions.\n",
    "\n",
    "</h4>\n",
    "\n",
    "1. The PSLA faces are more localised and one of them, for example, might be clearly\n",
    "emphasising aspects of the chin. The eigenfaces are more general and emphasise broader\n",
    "aspects of the image. (Note though that overall, PLSA must have a higher reconstruction\n",
    "error since it has more constraints (positive bases).)\n",
    "\n",
    "2.\n",
    "** I am only 20% sure this answer is aanywehre close to being correct **\n",
    "* We can transform X,U and V into probability distributions using $p = \\frac {CountOfElement }{SumOfElement Count}$\n",
    "* Let p = p(X,Y): This is the true probability\n",
    "* Let $\\tilde p$ = p(X,UV) \n",
    "* Let z = p(U) and y = p(V)\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml001.png\" height=\"100\" width=\"300\">\n",
    "\n",
    "Notes:\n",
    "* Line 3 - is keeping track of the p(z|x,y) in the last iteration\n",
    "* Line 4 - is updating p(x|z) using chain rule to match p(x, y) * previous weightings\n",
    "* Line 5 - is doing the same but in the output end, p(y|z)\n",
    "* Loops until the difference between old & new p(z | x,y) is minimal\n",
    "\n",
    "* Now derive z from the conditional probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Optimization'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">4. Optimization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Show that $-log \\sigma(x)$  is convex where $\\sigma$ represents the sigmoid function</h4>\n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{1}{\\sigma(x)}\\sigma(1-\\sigma) = \\sigma(x) -1$$\n",
    "$$\\frac{dfdx}{dx} = \\sigma(x) (1- \\sigma(x)$$\n",
    "\n",
    "Because sigma lies between 0-1 we, this function will also always lie between 0 and 1, threfore > 0 and hence convex\n",
    "\n",
    "Notes:\n",
    "* derivative of a log(x) is the $\\frac{1}{log(x)}$\n",
    "* derivative of a sigmoid is sigmoid(x)*(1-sigmoid(x))\n",
    "* in an exam the x they give you might be more complicated. Don't fall for it - just say z= .... and repeat steps above\n",
    "\n",
    "------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain the concept of gradient descent optimization for minimizing the least-squared loss E(w) and explain why this is, in general, an inefficient procedure for the problem.</h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "* Gradient descent is an iterative method that continues until f(x) is 0 or very close to 0\n",
    "* It works by determining the new value of x as: $f(x_k+1)\\approx f(x_k)+(x_{k+1}-x_k)^T \\triangledown f(x_k)$ or new weight = old weigtht + chage in weight * change in f(x)\n",
    "* Simply following the curve, one small step change at a time may take a long time to converge. If the step-change is too great it may overshoot the optimal, of it's too small it will take a long time to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain gradient descent</h4>\n",
    "\n",
    "* We wish to find an x that minimies f(x), and thre is no closed-form solution to this problen that applies in all cases, hence we need to use an iterative method\n",
    "* We can say that at the minimum, $f(x_k)$ and $f(x_{k+1})$ would give approximately the same result for some small change to x which we call $\\epsilon$  (aka learning rate)\n",
    "\n",
    "* The general formula is $x_{k+1} = x_k - \\epsilon \\triangledown f(x_k)$\n",
    "* Except we write it in the more correct matrix notation:\n",
    "$x_{k+1} = x_k - \\epsilon C^{-1}g$\n",
    "<br><br>\n",
    "\n",
    "* Note 1: The gradient part of this formula tells us how much each change to $\\epsilon$ will increase f(x) by, hence by negating this we go in the opposite direction, until $f(x_{k+1}) \\approx f(x_k)$\n",
    "\n",
    "* Note 2: The C in the matrix notation is a positive definite matrix and means the $\\epsilon$ gets converted into a vector\n",
    "\n",
    "* Note 3: If the learning rate is too small, you may reach the start of the minimum and end up stopping there as it would approximate to 0. So you may need to test out different learning rates at the end once you get to the minimum.\n",
    "\n",
    "* Note 4: If the learning rate is too large it will not converge on the minimum - in fact you could start going uphill due to the fact k was on one side of the gradient and k+1 is on the other\n",
    "\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the concept of line search optimization and show that for a line going through the point $w_k$ and direction $p_k$,\n",
    "\n",
    "$w=w_k + \\lambda p_k$\n",
    "<br>\n",
    "the optimal point on the line to minimize the squared error E(w) is given when:\n",
    "<br><br>\n",
    "$$\\lambda = \\frac{(b-Ap_k)^Tp_k}{p^T_kAp_k}$$</h4>\n",
    "\n",
    "** No idea where I got this question from **\n",
    "\n",
    "* Line search optimization is when you choose to only change one dimension within the weight vector and optimize along that direction before repeating for other dimensions until convergence is found\n",
    "\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain convergence rate for convex funtions</h4>\n",
    "\n",
    "* Assuming a convex function with $x_1$ to $x_T$ steps the gradience descent algorithm will take\n",
    "* and there is an optimal finite x^*\n",
    "* Then the gradient has a constant, L, that means the Hessian maximum Eigenvalue is less than or equal to L\n",
    "$$H(x) \\succeq LI$$ ($\\succeq$ means that $H(x_i) \\leq LI $ for every index i)\n",
    "\n",
    "* In this case we can say that the convergence rate is of order O(1/T) where T is the number of iterations\n",
    "* $$f(x_T) - f(x^*) \\leq \\frac{1}{2 \\epsilon T}(x_1-x^*)^2$$\n",
    "\n",
    "(the amount you are above the optimal by will be <=  ...)\n",
    "\n",
    "* In practice the results may be better than this\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the meaning of the 'conjugate direction' and why this means that the optimum of E(w) can be found by optimizing along each conjugate direction independently</h4>\n",
    "\n",
    "Ans\n",
    "* This is the idea of moving along one dimension to the find the optimal,then moving along another dimension that is conjugate to the first one. This way the change in the new dimension will not impact the change from the first dimension.\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain Momentum</h4>\n",
    "\n",
    "* Taking a moving average of the update \n",
    "* Reduces zig-zagging behaviour and help push past saddle points (local minima followe by a local maxima)\n",
    "* It is one way of updating the moving average without re-computing everything\n",
    "* $$x_k + \\tilde g_{k+1}$$ \n",
    "\n",
    "* Momentum can increase the speed of convergence since, for smooth objectives, as we get close to the minimum the gradient decreases and standard gradient descent would start to slow down\n",
    "* If the learning rate is too large, standard gradient descent may oscillate, but momentum may reduce oscillations by going in the average direction.\n",
    "* However, the momentum parameter $\\mu$ may need to be reduced with the iteration count to ensure convergence.\n",
    "* Particularly useful when the gradient is noisy. By averaging over previous gradients, the noise ‘averages’ out and the moving average direction can be much less noisy.\n",
    "* Momentum is also useful to avoid saddles (a point where the gradient is zero, but the objective function is not a minimum, such as the function x 3 at the origin) since typically the momentum will carry you over the saddle.\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain Nestorov's accelerated gradient'</h4>\n",
    "\n",
    "* New weights = Prev weights + Previous update - LearningRate*(derivative of \"new weights if you stick to previous update\" w.r.t  old weights)\n",
    "\n",
    "$$x_k = x_k-1 + V_k$$ \n",
    "where $$v_k = v_k-1 - \\epsilon\\frac{\\delta}{\\delta x}f(x_{k-1} + v_{k-1}*dampening factor)$$\n",
    "\n",
    "* So you are in effect moving the previous update in the optimal direction\n",
    "* Nestorov also added a factor to slow down the momentum as the solution approached the minimum so that it would converge. The final equation for v is:\n",
    "where $$v_{k-1} = \\mu v_{k-1} - \\epsilon\\frac{\\delta}{\\delta x}f(x_{k-1} + \\mu_{k-1}v_{k-1}*dampening factor)$$\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='CJ'></a>\n",
    "<h4> Explain Conjugate Gradient Descent</h4>\n",
    "https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\n",
    "\n",
    "* Conjugate gradient descent is a way of doing gradient descent by optimizing along one dimension at a time\n",
    "* We have a set of weights, x that we wish to update with each iteration until it reaches the optimal\n",
    "$$x_k+1 = x_k + \\alpha_kp_k$$\n",
    "\n",
    "* Imagine an n dimensional problem that is convex such as this quadratic: \n",
    "$$f(x) = \\frac{1}{2}x^T Ax - b^Tx $$\n",
    "\n",
    "\n",
    "Example:\n",
    "<img src=\"../_img/aml_3.jpg\" height=\"100\" width=\"400\">\n",
    "\n",
    "* Note 1: We wish to find the optimal weights such that it minimizes the error, so x represents our NN weights here\n",
    "* Note 2: Within NN optimization, A represents the thing you are multiplying your weights with (either input data or output from the prevous layer)\n",
    "\n",
    "\n",
    "\n",
    "* Conjugate gradient descent says that if you wish to optimize one x dimension at a time, you need to replace x with $P$, such that $P^TAP$  is a diagonal matrix.\n",
    "* If x has 10 dimensions then we would do 10 iterations, and each time a new p vector would be added into P. Each p vector would be orthogonal to all previous p vectors so that it is not impacting previous dimensions\n",
    "* You can imagine this in the bowl as a two vetors that are orthogonal (right-angled) from each other - that way  you can optimize one without worrying about the effect it will have on the other\n",
    "* In a quadratic problem this means that this:\n",
    "$$f(x)=\\frac{1}{2}x^TAx-b^Tx $$\n",
    "becomes this:\n",
    "$$f(x)=\\sum_{i=1}^{n} \\left(\\frac{1}{2} \\alpha_i^2p_i^TAp_i-\\alpha_ib^Tp_i \\right ) $$\n",
    "\n",
    "\n",
    "* We can find each new p using the Polak-Ribière formula: which uses the gradients to find the next conjugate vector, which then allow us to make the update within each iteration:\n",
    "$$x_k+1 = x_k + \\alpha_kp_k$$\n",
    "\n",
    "where each p is a diagonal only for k (so when you move to the next direction it becomes 0's)\n",
    "\n",
    "* Note: In a non quadratic problem no such method exists\n",
    "\n",
    "Finally we get to the following algorithm:\n",
    "\n",
    "1. k = 1\n",
    "2. Choose $x_1$\n",
    "3. $p_1$ = $−g_1$  # pick any gradient\n",
    "4. while $g_k \\neq 0$ do\n",
    "    \n",
    "    * $\\alpha_k$ = argmin $f(x_k + \\alpha_k p_k) $  &nbsp;&nbsp;&nbsp;&nbsp; # Do a line search\n",
    "    * $x{_k+1} := x_k + \\alpha_k p_k$    &nbsp;&nbsp;&nbsp;&nbsp; # Update x based on p\n",
    "    * $\\beta_k := g_{k+1}^Tg_{k+1}/(g^T_kg_k)$  &nbsp;&nbsp;&nbsp;&nbsp; # Find new conjugate direction\n",
    "    * $p{_k+1} =  -g{_k+1} + \\beta_kp_k$     &nbsp;&nbsp;&nbsp;&nbsp; # Update next p\n",
    "    * k = k +1\n",
    "10. end while\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is the Taylor series and why is it important?</h4>\n",
    "\n",
    "http://www.mathsisfun.com/algebra/taylor-series.html\n",
    "\n",
    "Ans:\n",
    "\n",
    "1) The Taylor series is the idea that you can rewrite any smooth function (one who's shape can be described by a single formula) into a series of simpler \"generaized\" polynomials . Since (almost) all functions you encounter have a Taylor series, all functions can be thought of as \"generalized\" polynomials!\n",
    "\n",
    "\n",
    "<img src=\"../_img/aml_4.jpg\" height=\"100\" width=\"500\">\n",
    "\n",
    "2) For example we can write a function as a power series with center $x_0$ \n",
    "\n",
    "3) Power series are easy to differentiate and integrate. No more techniques of integration, if one is satisfied with writing an integral as a power series!\n",
    "\n",
    "4) In finding integrals and solving differential equations, one often faces the problem that the solutions can't be \"found\", just because they do not have a name, i.e., they cannot be written down by combining the familiar function names and the familiar mathematical notation. The functions describing the motion of a \"simple\" pendulum are important examples. Power series open the door to explore even functions like these! \n",
    "\n",
    "---------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain Newtons Method</h4>\n",
    "\n",
    "* At a high level Newton's method finds the minimum of a function by drawling a line at the gradient of any point, and moving to the point where this intersects the with the x-axis. At this point it then sets $x_k+1$ to this value and repeats. The result is that f(x) converges on the minimum\n",
    "\n",
    "* f(x+ change) = previous f(x) + (change $*$ gradient) + (half $*$ change$^2 *$ Hessian)\n",
    "\n",
    "-------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> For input-output training points $(xn,yn), n = 1; ... ,N$, where each input $x^n$ is a vector\n",
    "and each output yn is a scalar, the squared loss of a linear regression model is\n",
    "\n",
    "$$E(w) = \\frac{1}{N} \\sum^N_{n=1}(y^n-w^Tx^n)^2$$\n",
    "<br><br>\n",
    "1. Compute the gradient and Hessian of this objective function and show that E (w) is convex.</h4>\n",
    "<br><br>\n",
    "\n",
    "$\\frac{\\delta E}{\\delta\\theta_i} = 2\\sum_n(y^n-\\theta^Tx^n)x^n_i $\n",
    "\n",
    "$\\frac{\\delta E}{\\delta\\theta_ij} = 2\\sum_n(x^n)x^n_i = 2 \\sum x^n_jx^n_i   $\n",
    "\n",
    "Or in matrix form $= 2X^TX$\n",
    "<br><br>\n",
    "This will always be > 0 and is therefore convex\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='LargeScale'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">5. Large Scale Machine Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> For input-output training points $(xn,yn), n = 1; ... ,N$, where each input $x^n$ is a vector\n",
    "and each output $y^n$ is a scalar, the squared loss of a linear regression model is </h4>\n",
    "\n",
    "$$E(w) = \\frac{1}{N} \\sum^N_{n=1}(y^n-w^Tx^n)^2$$\n",
    "\n",
    "<h4>Explain what Stochastic Gradient Descent is and how it could be used to find the w that minimises E(w)</h4>\n",
    "\n",
    "* In normal ('Batch') gradient descent you analyze all the data before making first W update\n",
    "\n",
    "* In SGD you update W after each single row of data\n",
    "\n",
    "* This means that while you are performing more updates, and may not reach the true optimal, you converge approximately to the optimal faster precisely because you are doing more updates so each new one update gains from the fact lots of other ones have happened before it\"\n",
    "<br>\n",
    "* This is useful in situations where that dataset is too big to store it all in memory and/or it will take a long time to update\n",
    "\n",
    "\n",
    "<h4>In the case that the input vectors are sparse (only a fraction f of the elements\n",
    "of each x n are non-zero), explain what computational savings this has when\n",
    "implementing gradient descent.</h4>\n",
    "\n",
    "<h4>Explain how Conjugate Gradients could be used to find the w that minimises\n",
    "E(w) and what computational savings can be made when the input vectors $x^n$ are sparse.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='AutoDiff'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Auto Differentiation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Describe Forward Automatic Differentiation (AutoDiff) and give two procedures\n",
    "(one exact and the other an approximation) that compute the gradient of a subroutine\n",
    "f(x) with respect to its arguments x, giving time complexities of the approaches.</h4>\n",
    "\n",
    "https://justindomke.wordpress.com/2009/02/17/automatic-differentiation-the-most-criminally-underused-tool-in-the-potential-machine-learning-toolbox/\n",
    "\n",
    "Ans:\n",
    "* AutoDiff takes a function f(x) and calculates the gradient\n",
    "* It does this without having to calculate the gradient for each element of x\n",
    "* AutoDiff exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically and accurately to working precision\n",
    "* Forward autodiff, takes the bottoms up approach by calculating the inner differentials first (the ones with respect to independent variables) before moving up the chain.\n",
    "* The complexity of forward autodiff is equivalent to the original function for which it is calculating the gradient\n",
    "\n",
    "There are two approaches to Forwar-Auto Diff\n",
    "* Complex Arithmethic - gives an approximate answer only\n",
    "* Dual Arithmetic - gives an exact answer but it not efficient \n",
    "\n",
    "Both these appraches introduce an imaginary number $\\epsilon$\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain reverse mode Auto-Diff:</h4>\n",
    "\n",
    "* Reverse autodiff takes the top down approach, calculating the outer derivatives first and working inwards. \n",
    "* This requires storing all the intermediate calculations in memory as you go along, which may cause problems, unless checkpointing is used in which certain portions would have to reevaluated at the end\n",
    "* The complexity of reverse autodiff is ...(twice that of the original function)??\n",
    "* Forward-mode is efficient for functions taking one input and producing many outputs. Reverse-mode is efficient for functions taking many inputs and producing a single output. (That output would be a “loss function” in machine learning)\n",
    "\n",
    "\n",
    "<img src=\"../_img/aml_1.jpg\" height=\"100\" width=\"100\">\n",
    "\n",
    "-----------------\n",
    "<br><br>\n",
    "$$\\frac{df}{dy}=\\frac{\\delta f}{\\delta x}+\\frac{\\delta f}{ \\delta g}\\frac{\\delta g}{\\delta x}$$\n",
    "\n",
    "Notes:\n",
    "1. d = total derivative, $\\delta$ = partial\n",
    "2. Because f is impacted by x in two different ways, the total derivative of f w.r.t x is given by summing boths paths (https://en.wikipedia.org/wiki/Sum_rule_in_differentiation)\n",
    "3. The second path is broken down by the chain rule (https://en.wikipedia.org/wiki/Chain_rule)\n",
    "\n",
    "-----------------\n",
    "\n",
    "<h4> Write out the reverse mode differentiation for this:\n",
    "<img src=\"../_img/aml_2.jpg\" height=\"100\" width=\"300\"></h4>\n",
    "\n",
    "\n",
    "1. $\\frac{df}{dx} = (2x + gh) + (g^2xg) + (2x2gxxg) + (2xxh)$\n",
    "2. Substitute f and g to get to $2x + 8x^7$\n",
    "\n",
    "Note: If you are asked to create a graph for a function, then first differentiate that function then create the graph, building up each step of the calculation as an $f_1$ , $f_2$ etc as you go\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> What is the Reverse Mode Differentiation algorithm </h4>\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml005.png\" width=\"500\">\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<q4> Explain how to use Reverse AutoDiff to efficiently calculate the gradient with re\n",
    "spect to $\\theta_1 and \\theta_2$ of [insert nested complicated function of your choice]\n",
    "Your computation graph should have nodes representing elementary functions. Annotate your graph\n",
    "suitably and define the forward and backward passes explicitly.\n",
    "\n",
    "Ans:\n",
    "* To solve this put the wrt at the top of the chart (in this case two nodes of $\\theta_1$ and $\\theta_2$\n",
    "* then draw arrows down to show the tranistions, labelling the next node $f_1, f_2$ and so on\n",
    "* on the RHS explain what each f represents\n",
    "\n",
    "Now put the following explanation below\n",
    "* Reverse Mode Auto-Diff first does a forward pass to build up the above graph\n",
    "* Then go through the nodes in reverse order and calculate the derivative of each one, recursiely traversing through the child nodes in order to do so and remembering the child derivatives on the way back up\n",
    "* This is more efficient as you calculate common components just once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain how we can calculate the quantity Hessian-vector product $Hv$ using the autodiff framework:</h4>\n",
    "\n",
    "----------------\n",
    "\n",
    "We use the fact that $D_v(\\frac{\\partial E}{\\partial \\theta_i}) = [Hv]_i$ and then use the usual rule of reverse differentiation from autodiff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Consider a time series prediction problem in which, given a sequence of inputs\n",
    "Xl,X2\", \"Xt, we make a prediction)it for the output at time t. To do this we define:\n",
    "    \n",
    "$$h_1=x_1$$\n",
    "$$h_t=f(x_t, h{_t-1}, A)  \\quad t>1$$\n",
    "$$\\tilde y_t = g(h_t,B)$$\n",
    "\n",
    "where A and B are parameters and f and g are some (unspecified) functions. The\n",
    "objective is to find parameters A and B that minimise the loss\n",
    "\n",
    "$$\\sum^T_{t=1}(y_t-\\tilde Y_t)^2$$\n",
    "\n",
    "Explain how to use Reverse AutoDiff to efficiently calculate the gradient of this loss\n",
    "function with respect to A and B.\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='NN'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">6. Neural Networks</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='AutoEncoders'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Auto Encoders</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>\n",
    "Explain how auto-encoders can be used to find low dimensional representations of data and explain how PCA relates to an Autoencoder.</h4>\n",
    "\n",
    "\n",
    "Ans:\n",
    "\n",
    "* Auto-encoders use neural nets to learn to represent data using. Data is fed into a neural net with one or more hidden layers. The hidden layer with the smallest number of weights effective becomes the lower-level representation of the data. The outputs are then scaled back up to map the output back in the form of the original data. \n",
    "\n",
    "* The loss function then checks the difference between what was generated and the original input.\n",
    "\n",
    "* PCA is equivalent to an Autoencoder with a single hidden layer with a linear trasfer function. In this case PCA is a more efficient way of calculating the optimal.\n",
    "-----------------------\n",
    "\n",
    "We can break an autoencoder into two parts -- an encoder from the input to the bottleneck layer and a decoder from the bottleneck to the reconstruction. Both encoder and decoder can contain multiple layers. Let's write h=f(x) for the encoder part and x'=g(h) for the decoder.   If the decoder is a linear function, then the autoencoder cannot beat PCA. Otherwise it can.  Essentially this is what we showed in the lectures.\n",
    "\n",
    "Remember that f and g can themselves represent multiple layers. Consider an autoencoder x->h1->h2->h3->h4->h5->x' in which h3 is the bottleneck. Then even if (for example) h5->x' is linear, then the overall decoder h3->h4->h5->x' can still be a non-linear function of h3, provided either h3->h4 or h4->h5 is non-linear. Hence this autoencoder could beat PCA even with a linear output layer.\n",
    "\n",
    "\n",
    "<h4>\n",
    "Consider and Auto-encoder with structure x$\\rightarrow$ h $\\rightarrow \\tilde x$, trained to minimize the squared loss:\n",
    "\n",
    "$$\\sum_{n=1}{N}\\left(\\tilde x - x^n \\right)^2$$ \n",
    "with $h^n=f(Ax^n)$ and $\\tilde x^n = Bh^n$ for matrices A,B and a non linear function f.\n",
    "\n",
    "For k-dimensional h, is this non-linear procedure in principle more powerful than K-dimensional PCA, in the sense that it has lower squared loss? Explain fully your answer.\n",
    "</h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "------------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain what is meant by an auto-encoder neural network</h4>\n",
    "\n",
    "* An auto-encoder NN tried to find a lower dimensional representation of data by setting the output to to match the input dimensions, with reduction of dimension in 1 or more hidden layers in between\n",
    "\n",
    "* The hidden layer with the smaller dimensions (bottleneck layer) is the lower dimensional representation of the input data\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='RNNS'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">RNNs</h3>\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml004.png\" height=\"100\" width=\"200\">\n",
    "\n",
    "* Each h in this digram represents 1 or more hidden layers of that same time step\n",
    "* RNNs are used in timeseries applications\n",
    "* The basic idea is that the hidden units at time $h_t$ (and possibly output $y_t$ ) depend on the previous state of the network $h_t−1 , x_t−1 , y_t−1$ for inputs $x_t$ and outputs $y_t$ .\n",
    "* The above network is ‘unrolled the net through time’ to give a standard NN diagram.\n",
    "* Potential links from $x _t−1$ , $y_t−1$ to $h_t$  have been omitted\n",
    "\n",
    "There are two ways to train an RNN:\n",
    "* RTRL (which is a single forward pass in time but has high storage cost) \n",
    "* BPTT (which is a forward and backward pass with more modest storage cost). RTRL is straightforward, but BPTT requires an understanding of parameter tying (or more generally AutoDiff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What is Parameter tying?</h4>\n",
    "\n",
    "* Parameter tying addresses the issue of how to differentiate netsted functions in which there is a common paramter $\\theta$ (not to be confused with input x)\n",
    "* One way would have been to do the chain rule\n",
    "* But another way is to treat the two as two separate functions and differentiate them individually\n",
    "* Then join the two unconstrained derivatives together (sum) and apply a constraint that $\\theta_1 = \\theta_2$\n",
    "\n",
    "1. Treat all parameters as independent and calculate the gradient with respect to\n",
    "each independent parameter.\n",
    "2. Sum all the resulting independent gradients together.\n",
    "3. Evaluate the expression by setting all the independent parameters to the same\n",
    "value.\n",
    "Note that this is a general result and can be used to deal with parameter tying\n",
    "in any objective, not just Deep Learning and Neural Nets.\n",
    "(Of course, this is again just a special case of AutoDiff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='LSTM'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">LSTM</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### An input-output time-series ($X_t,Y_t$), t = 1...T can be modelled by a recurrent LSTM (Long ShortTerm Memory) network. Explain the essential components of an LSTM network and what difficulties it tries to overcome (compared to standard recurrent networks).\n",
    "\n",
    "Ans:\n",
    "\n",
    "* AN LSTM tried to overcome the limitations of an RNN when it comes to storing long-term dependencies in memory. With a standard RNN this becomes computationally inefficient.\n",
    "* It does this by introducing the concept of a memory gates that can affect the memory state running through the function\n",
    "* The forget gate takes in the current input ($C_t$) and the previous output and  and uses them to decide whic parts of the current state should be reduced (forgotten)\n",
    "* The input gate also takes in the current input ($C_t$) and the previous output and decides whether anything should be added to the memory. This then gets added to the result of the forget step\n",
    "* The output gate then decides whether to activate an output based on the results at this timestep. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Initialization'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Initialization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "http://cs231n.github.io/neural-networks-2/#datapre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Visualization'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">7. Visualization\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain SNE and T-SNE</h4>\n",
    "\n",
    "<b>SNE</b>\n",
    "\n",
    "* Stochastic Neighbourhood Embedding was invented by Hinton and Rowels\n",
    "* It represents any high dimensional object, i, as the probability of it being close to every other neighbour,j\n",
    "$$p_{ij} = \\frac{exp(-d^2_ij)}{\\sum_{k \\neq i } exp(-d^2_{ik})}$$\n",
    "* The distance measure can be anything but Hinton proposes Eucliden distance $-(x_i - x_j)^2/(2\\sigma^2_i) $\n",
    "* This should give each point its own Gaussian distribution. Remeber that i and k here represent the dimension and could be large\n",
    "* The next step is to define a probability distribution that is a similar shape but it based on a lower level of dimensions\n",
    "$$q{j|i} = \\frac{exp(-(y_i-y_j)^2)}{\\sum_{j \\neq i} exp(-y_i-y_j)^2)}$$\n",
    "* When determining whether the two probabilities are similar SNE uses KL divergance\n",
    "* However this isn't symmetric - it maintains local structure but is in accurate on global structure\n",
    "* The Gaussian distribution also means that points far away have almost no impact in determining q\n",
    "\n",
    "<b>T-SNE</b>\n",
    "* Uses symmetric loss function when optimizing q\n",
    "* Uses a Students-T distribution for q\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml002.png\" height=\"100\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='FastestNN'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">8. Fastest Nearest Neighbours</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# <center>Applied Machine Learning Q&A</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Topics covered in class</h3>\n",
    "\n",
    "<br>[<b>1. Linear Regression</b>](#Linear Regression)\n",
    "<br>[Least Squares Loss](#Loss)\n",
    "<br>[Auto Regressive Models](#AutoRegresiveModels)\n",
    "<br>[Radial Basis Functions](#Radial)  -- to do\n",
    "\n",
    "<br>[<b>2. Clustering and Classification</b>](#Clustering)\n",
    "<br>[Logistic Regression](#LogisticRegression)\n",
    "<br>[Maximum Liklihood](#MaxLiklihood)  -- to do\n",
    "<br>[Nearest  Neighbours](#Nearest Neighbours)\n",
    "<br>[Fastest Nearest Neighbours](#FastestNN)  --- not complete\n",
    "\n",
    "<br>[Naieve Bayes](#NaieveBayes)\n",
    "<br>[SVMs](#SVM)  -- not on any past exam\n",
    "<br>[Decision Trees](#DecisionTrees)  -- to do\n",
    "<br>[Mixture Models](#MixtureModels) -- to do\n",
    "\n",
    "<br>[<b>2b. Ensemble Models (ISL only)</b>](#Clustering)\n",
    "<br>[Adaboost](#AdaBoost)\n",
    "<br>[Random Forest](#RandomForest)\n",
    "\n",
    "<br>[<b>3. Dimensionality Reduction</b>](#Dimensionality Reduction)\n",
    "<br>[PCA](#PCA)\n",
    "<br>[Non-Negative Matrix Factorization](#NNMF)\n",
    "<br>[ICA](#ICA)\n",
    "<br>[Fishers Linear Discrimnate & Canonical Variables](#Fishers)\n",
    "\n",
    "<br>[<b>4. Optimization</b>](#Optimization)\n",
    "<br>[Gradient Descent method](#GradientDescent)\n",
    "<br>[Newtons Method](#NewtonsMethod)\n",
    "<br>[Conjugate Gradients Method](#CG)\n",
    "<br>[Line search & conjugate gradients](#ConjugateGradients)\n",
    "<br>[Higher order methods](#HigherOrder)\n",
    "\n",
    "<br>[<b>5. Large Scale Machine Learning</b>](#LargeScale)\n",
    "<br>[Stochastic Gradient Descent](#SGD)\n",
    "<br>[Sparsity](#SGD)\n",
    "<br>[Batch vs Online](#BatchOnline)\n",
    "\n",
    "<br>[<b>6. Neural Networks</b>](#NN)\n",
    "<br>[Auto Encoders](#AutoEncoders)\n",
    "<br>[CNNs](#CNNs)\n",
    "<br>[NLP](#NLP)\n",
    "<br>[RNNs](#RNNs)\n",
    "<br>[Gradient Decay/Explosion](#DecayExplosion)\n",
    "<br>[LSTM](#LSTM)\n",
    "<br>[SeqToSeq and Bidirectional](#Seq)\n",
    "<br>[Parameter Tying](#ParamTying)\n",
    "<br>[BackProp Through Time](#BPTime)\n",
    "<br>[Auto Differentiation](#AutoDiff)\n",
    "<br>[Initialization](#Initializtion)\n",
    "\n",
    "<br>[<b>7. Visualization</b>](#Visualization)\n",
    "<br>[T-SNE](#TSNE)\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Useful Latex Math symbols</h4>\n",
    "\n",
    "http://web.ift.uib.no/Teori/KURS/WRK/TeX/symALL.html\n",
    "\n",
    "------------------------\n",
    "\n",
    "<h4>Guide</h4>\n",
    "\n",
    "* If I say some external resource is 'recommended' it means it's a useful resource that will help cement the idea\n",
    "* If I say it's 'essential' it means drop everything you're doing and view it instead of reading my notes (e.g. https://www.youtube.com/watch?v=TEB2z7ZlRAw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Linear Regression'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">0. Key questions</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. Explain the issues of overfitting and describe what we add to the loss function to prevent it\n",
    "2. Discuss the shapes of different regularization functions\n",
    "3. Talk about non-squared loss\n",
    "4. Explain Nearest Neighbours, Mahalanobis distance, and K-NN\n",
    "2. Explain Naieve Bayes\n",
    "3. What is the log liklihood function?\n",
    "4. [Explain the PCA algorithm](#PCA)\n",
    "5. Explain why the SVD method is related to the eigen decomposition of S\n",
    "6. Compute the Gradient and Hessian of $$E(w) = \\frac{1}{N} \\sum^N_{n=1}(y^n-w^Tx^n)^2$$\n",
    "7. Explain what Stochastic Gradient Descent is and how it could be used to find\n",
    "the w that minimises E(w)\n",
    "8. Explain Conjugate Gradients\n",
    "9. Describe Forward Auto-Diff\n",
    "10. Describe Reverse Auto-Diff\n",
    "11. Explain Auto-Encoders and compare them to PCA\n",
    "12. Explain Fishers Linear Discrimnant\n",
    "13. Explain the relationship between Sparse vectors and Conjugate gradients\n",
    "Optimization:\n",
    "14. [Explain Gradient descent for minimizing least squared loss & adv/disadv](#GradientDescent)\n",
    "15. [Explain Conjugate Vectors method for minimizing least squared loss & adv/disadv](#CG)\n",
    "16. [Explain Newtons method for minimizing least squared loss & adv/disadv](#NewtonsMethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Linear Regression'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">1. Linear Regression</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain Linear Regression</h4>\n",
    "\n",
    "$$y_i = \\beta x^T +\\epsilon$$\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml010.png\" height=\"100\" width=\"300\">\n",
    "\n",
    "* Y is called the the regressand, endogenous variable, response variable, measured variable, criterion variable, or dependent variable\n",
    "* X is called the regressors, exogenous variables, explanatory variables, covariates, input variables, predictor variables, or independent variables \n",
    "* $\\beta$ is the parameter vector. <b>You will often see it being denoted by W, a weight vector.</b> Elements of this are interpreted as the partial derivatives of the dependent variable with respect to the various independent variables.\n",
    "* $\\epsilon$ is called the error term, disturbance term, or noise. This variable captures all other factors which influence the dependent variable yi other than the regressors xi. The relationship between the error term and the regressors, for example whether they are correlated, is a crucial step in formulating a linear regression model, as it will determine the method to use for estimation.\n",
    "* Linear regression has a number of assumptions (https://en.wikipedia.org/wiki/Linear_regression#Extensions) some of which are addressed by the many extensions to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4 style=\"background-color:#616161;color:white\">Measuring loss</h4>\n",
    "\n",
    "A large number of procedures have been developed for parameter estimation and inference in linear regression. \n",
    "\n",
    "<h4>Linear or Ordinary Least Squares (OLS)</h4>\n",
    "\n",
    "* Least squares problems fall into two categories: linear or ordinary least squares and non-linear least squares, depending on whether or not the residuals are linear in all unknowns. \n",
    "* The linear least-squares problem occurs in statistical regression analysis; and has a closed-form solution. \n",
    "* The non-linear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.\n",
    "\n",
    "Given N rows of data, the OLS loss is\n",
    "\n",
    "$$Loss(\\beta) = \\sum^N_{i=1} (y^i - \\beta^Tx^i)^2$$\n",
    "\n",
    "or\n",
    "\n",
    "$$E(w) = \\sum^N_{i=1} (y^i - w^Tx^i)^2$$\n",
    "\n",
    "* While this has a closed form solution it is computationally very expensive and not practical for a large matrix.\n",
    "* In this situation, using an iterative method is much more computationally efficient than using the closed form solution to the least squares problem. \n",
    "\n",
    "<h4>Generalised Least Squares (GLS)</h4>\n",
    "\n",
    "* An extension of OLS that allows estimation when heteroscedasticity (no constant variance), or correlations are present among the error terms of the model; as long as the form of heteroscedasticity and correlation is known independently of the data.\n",
    "\n",
    "<h4>Total least squares (TLS)</h4>\n",
    "An approach to least squares estimation of the linear regression model that treats the covariates and response variable in a more geometrically symmetric manner than OLS. It is one approach to handling the \"errors in variables\" problem, and is also sometimes used even when the covariates are assumed to be error-free.\n",
    "\n",
    "<h4>Maximum-likelihood estimation and related techniques</h4>\n",
    "\n",
    "- Maximum likelihood estimation can be performed when the distribution of the error terms is known to belong to a certain parametric family ƒθ of probability distributions. \n",
    "\n",
    "- Ridge regression and other forms of penalized estimation such as Lasso regression deliberately introduce bias into the estimation of $\\beta$ in order to reduce the variability of the estimate. The resulting estimators generally have lower mean squared error than the OLS estimates, particularly when multicollinearity is present or when overfitting is a problem. They are generally used when the goal is to predict the value of the response variable y for values of the predictors x that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='LRIterativeMethod'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Iterative Methood for Linear Regression</h4>\n",
    "\n",
    "* The Gradient Descent procedure is .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Overfitting'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Dealing with Overfitting</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain the issues of overfitting and describe what we add to the loss function to prevent it</h4>\n",
    "\n",
    "* Overfitting is when a statistical model describes random error or noise rather than the underlying relationship\n",
    "* Adding a regularization term such as $\\lambda w^2$ to penalise rapid changes in the output helps reduce this\n",
    "* Adding an L2 regularization (aka Ridge Regression) to the previous solution would make the w:\n",
    "$$w=\\left(\\sum_{n=1}{N}x^n(x^n)^T+\\lambda w^Tw\\right)^{-1}\\left(\\sum_{n=1}^{N}y^nx^n\\right)$$\n",
    "\n",
    "Shapes for regularisation penalty for 2 weights $\\sum_i | w_i|^q$\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml006.png\" height=\"100\" width=\"600\">\n",
    "\n",
    "* As we decrease q towards 0 we get the ‘ideal’ regulariser that selects weights\n",
    "which are 0 if they do not contribute.\n",
    "* For q = 1 the objective function for squared loss linear regression remains\n",
    "convex and easy to optimise\n",
    "* The objective function is complicated (non convex) for q < 1\n",
    "\n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is the difference between L1 and L2 regularization?</h4>\n",
    "\n",
    "L1\n",
    "* Term is: sum of abs values of w\n",
    "* Heavy and small $w_i$ is penalized by the a constant proportional amount\n",
    "* Penalty can't be differentiated so requires special optmization routines\n",
    "* But will mean that only significant weights remain\n",
    "* In practice (e.g. deep learning) people don't worry about it being non-differentiable and just proceed with gradient based training as normal\n",
    "\n",
    "L2\n",
    "* Term is: $ w^T w$ or $\\sum_i{w^2_i}$\n",
    "* Penalizes large W more than small ones\n",
    "* Penalty is differentiable\n",
    "* Small weights will still persists\n",
    "\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Talk about non-squared loss</h4>\n",
    "\n",
    "* It’s worth noting that some regression problems have non-squared loss, for\n",
    "example the summed absolute loss\n",
    "* In which case optimal predictor won’t necessarily be simply the average of the prediction distribution (getting some predictions right may matter more than others in this case).\n",
    "* It’s very common to train models using the squared loss. However, if the task performance is measured by some other loss, it often makes more sense to train the model using the correct loss. \n",
    "* One often sees users train a model using one kind of loss, but evaluate it using a very different loss – seems a bad idea in general.\n",
    "* It’s also worth noting that the loss can heavily effect how easy it is to train a\n",
    "model. For example, in classification, using the logistic regression model gives a convex optimisation problem for θ. Using a squared loss gives a non-convex problem\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>For a dataset of inputs x and scalar outputs y, ${(x^n,y^n),n=1, ...,N}$, linear regression is the model\n",
    "\n",
    "$$y=w^Tx$$\n",
    "<br>\n",
    "i. The least squares criterion with regularizer term, sets w based on minimizing\n",
    "\n",
    "$$E(w) =\\sum^{N}_{n=1}(y^n-w^Tx^n)^2+\\lambda w^Tw$$\n",
    "\n",
    "Show that the optimal solution is given by\n",
    "\n",
    "$$w= \\left(\\sum_{n=1}^{N}x^n(x^n)^T + \\lambda I \\right)^{-1}\\left(\\sum_{n=1}^{N}y^nx^n\\right)$$</h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "Useful: http://www.statpower.net/Content/310/Summation%20Algebra.pdf\n",
    "<br><br>\n",
    "Starting with\n",
    "\n",
    "$$E(w) =\\sum^{N}_{n=1}(y^n-w^Tx^n)^2+\\lambda w^Tw$$\n",
    "\n",
    "1. Differentiate: \n",
    "$$\\frac{dE}{dw}=2\\sum^N_{n=1} (y^n-w^Tx^n)x^n + \\lambda w= 0$$\n",
    "\n",
    "\n",
    "2. Divide both sides by -2 then multiply out with $x^n$:\n",
    "$$\\sum^N_{n=1} (y^n)x^n - \\sum^N_{n=1}(w^Tx^n)x^n + \\lambda w = 0$$\n",
    "\n",
    "3. Using summation rule 2 to move the w out\n",
    "$$\\sum^N_{n=1} (y^nx^n) - w^T\\sum^N_{n=1}(x^n)(x^n)^T + \\lambda w = 0$$\n",
    "\n",
    "4. Note that the regularizer can also be written $w\\lambda$. Reverse multiply out with the w in the regularizing term (I gets introduced to keep $\\lambda$ in vector form once w disappears)\n",
    "$$ \\sum^N_{n=1} (y^nx^n) w \\left( \\sum^N_{n=1} ( x^n)(x^n)^T + \\lambda I \\right)  = 0$$\n",
    "\n",
    "5. Move yx to the right hand side\n",
    "$$ \\left(w^T\\sum^N_{n=1}x^n(x^n)^T \\right) + w \\lambda = \\sum^N_{n=1} (y^nx^n)$$\n",
    "\n",
    "6. Use inversion to divide\n",
    "\n",
    "$$w= \\left(\\sum_{n=1}^{N}y^nx^n\\right) \\left(\\sum_{n=1}^{N}x^n(x^n)^T + \\lambda I \\right)^{-1}$$</h4>\n",
    "\n",
    "<br>\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "<h4>Give also an expression for the computational complexity required to find $w_{opt}$ using this expression and explain why in practice it is not recommended to find w by using matrix inversion as above, explain an alternative procedure</h4>\n",
    "\n",
    "* You don't find the matrix inversion because it's an O($n^3$) calculation and you also have to store the hessian at every iteration which is also very expensive\n",
    "* So you just do gradient calculations as you don't need a $x^Tx$ calculation\n",
    "\n",
    "\n",
    "* Gaussian elimination is faster and numerically more stable\n",
    "\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Show that the least squares error criterion \n",
    "\n",
    "$$E(w) =\\sum^{N}_{n=1}(y^n-w^Tx^n)^2$$\n",
    "<br>\n",
    "can be written in the form \n",
    "<br>\n",
    "$$E(w) = w^TAw -2w^Tb+c$$\n",
    "<br>\n",
    "for suitably defined A,b,c\n",
    "</h4>\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='AutoRegressiveModels'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Auto Regressive Models</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Radial'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Radial Basis Functions</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Clustering'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">2. Clustering & Classification</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='LogisticRegression'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Logistic Regression</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is the difference between linear and logistic regression?</h4>\n",
    "\n",
    "* Logistic regression seeks to predict a class of data (binary or multi-class)\n",
    "* Like other linear regression inputs may be linear or categorical\n",
    "* The observed label in (binary) logistic regression is however a zero-or-one variable\n",
    "* The logistic regression estimates the odds, as a continuous variable\n",
    "* In some applications the odds are all that is needed. In others, a specific yes-or-no prediction is needed for whether the dependent variable is or is not a case; this categorical prediction can be based on the computed odds of a success, with predicted odds above some chosen cutoff value being translated into a prediction of a success.\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain this slide</h4>\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml003.png\" height=\"100\" width=\"600\">\n",
    "\n",
    "Top formula is saying\n",
    "* iterate through every data value and multiply up the estimated probability of it being in the class that its actually in\n",
    "* Within the iteration the exponentials are used to choose between the left or right probability as $c^n$ will be 1 for one of them and 0 for the other\n",
    "* The bottom formula is kinda the same thing - $w^Tx^n$ works out some value indicating prob but not likely to be between 0 and 1, +b is the overall average error constant, the sigmoid converts it to 0 to 1, and then we take the log of this\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Consider binary classification problems with class c $\\epsilon$ {0, 1}. Logistic Regression models\n",
    "the probability of input vector x being in class c =1 as \n",
    "$$p(c = 1 | x,w) = 6 (wTx)$$\n",
    "\n",
    "for input vector x and weight (parameter) vector w, where \n",
    "\n",
    "$$\\sigma(x)=\\frac{e^x}{1+e^x}$$\n",
    "\n",
    "The dataset is $D = {(x^n , c^n) ,n =1,...N}$,\n",
    "<br><br>\n",
    "a. Assuming that the data is independently and identically distributed, show that the\n",
    "gradient of the log likelihood is given by\n",
    "$$\\sum^N_{n=1}(2x^n-1)\\sigma(1-2c^n)x^n$$\n",
    "<br><br>\n",
    "</h4>\n",
    "* Liklihood is:\n",
    "$$\\prod^N_{n=1} p(c =1 | x^n,b,w)^{cn} (1-p(c= 1 | x^n,b,w))^{(1-c^n)}  \\quad (where \\  c^n = 0 \\  or \\  1)$$\n",
    "\n",
    "* Log-liklihood is:\n",
    "    $$L(w,b) = \\sum^N_{n=1}c^n log \\sigma (b+w^Tx^n)+ (1-c^n) log (1-\\sigma(b+w^Tx^n))$$\n",
    "\n",
    "to be completed...\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>\n",
    "Consider training logistic regression in which the components of the input vector x\n",
    "can be on very different scales and also potentially highly correlated. Discuss how\n",
    "you might adapt the gradient ascent based approach to training logistic regression.\n",
    "<br><br>\n",
    "</h4>\n",
    "\n",
    "* Gradient ascent is easy to implement but slow to converge\n",
    "<br><br>\n",
    "* Since the surface has a single optimum, a Newton update $w^new = w^old + ηH^{−1}g$ (where H is the Hessian matrix as above and η is between 0 and 1) will typically converge much faster than gradient ascent.\n",
    "<br><br>\n",
    "* However, for large scale problems with dim (w) >> 1, the inversion of the Hessian is computationally demanding and limited memory BFGS or conjugate gradient methods are more practical alternatives.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>\n",
    "Missing data is a common problem in practice. Describe how you might adapt logistic regression when there are missing elements in the input data vectors. Discuss the advantages and disadvantages of your approaches.\n",
    "</h4>\n",
    "<br><br>\n",
    "\n",
    "Can apply the conjugate gradient descent method\n",
    "<br><br>\n",
    "<h4>\n",
    "In classification we often have an associated 'loss' function for each class. For\n",
    "example it might be that it is important in a medical situation to detect cancer, even\n",
    "if some of the detections are actually false. We can use a loss function L(ctrue, cpd)\n",
    "to measure our loss when our predicted class is Cpred whereas the truth is ctrue.\n",
    "<h4>\n",
    "i. Explain how to adapt logistic regression to minimize expected loss L(w) and derive a gradient based training scheme.\n",
    "<br><br>\n",
    "https://www.youtube.com/watch?v=IxotEG3yWHs\n",
    "\n",
    "<br><br>\n",
    "Replace the quadratic loss function with a Cross-entropy loss function\n",
    "\n",
    "<br><br>\n",
    "\n",
    "f. Comment on the geometric structure of the objective function L(w) and discuss any potential computational issues involved in training this model.\n",
    "<br><br>\n",
    "</h4>\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='NaieveBayes'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Naieve Bayes</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is Naieve Bayes</h4>\n",
    "\n",
    "<H2>Naieve Bayes</H2>\n",
    "<br>\n",
    "<h4>The algorithm</h4>\n",
    "\n",
    "$${\\begin{aligned}p(Y_{k}\\vert x_{1},\\dots ,x_{n})&\\varpropto p(Y_{k})\\prod _{{i=1}}^{n}p(x_{i}\\vert Y_{k})\\,.\\end{aligned}}$$\n",
    "\n",
    "\n",
    "* The maths behind it is explained very well in the <a href=\"http://scikit-learn.org/stable/modules/naive_bayes.html\">Skikit-learn pages</a>\n",
    "\n",
    "* p(x) can consist of mutiple things .. i.e.  inputs $x_1$,$x_2$,$x_3$\n",
    "* if the inputs $x_1$,$x_2$,$x_3$ are \"naievely\" assumed to be independent of each other (more on this later), so they don't impact each others probability of y, then we can use the multiplicative rule or probability and simplify the top to: \n",
    "$$p(Y_k)\\prod_{{i=1}}^{n} p(x_{i}\\vert Y_{k})$$\n",
    "* We do not need a denominator for this as we can say that it's a constant, having analyzed a corpus of data and determined the p of each x. For purposes of classification we can therefore ignore it.\n",
    "\n",
    "\n",
    "<i>$\\varpropto$ means in proportion to</i>\n",
    "\n",
    "Notes:\n",
    "<br>\n",
    "* The ability to assume independence under a given class is a crucial part of applied Bayes. So the words 'great' and tremendous' are postively correlated in a movie review, but if you took it for granted that the movie review was positive, then the probability of 'great' is unlikely to correlate as strongly with 'tremendous' - the two may be equally likely for instance.\n",
    "\n",
    "* Naieve Bayes is very fast to train, deals with more than two classes, and can deal with missing data\n",
    "* In the case of low counts the method can be overconfident although the use of pseudocounts can help (see literature)\n",
    "<br>\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='NearestNeighbour'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Nearest Neighbour</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain Linear Separability</h4>\n",
    "\n",
    "* If all the data for class 1 lies on one side of a hyperplane, and for class 0 on the\n",
    "other, the data is said to be linearly separable.\n",
    "* We can map using a non-linear vector function ψ(x) to make it linearly separable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the Nearest Neighbour algorithm</h4>\n",
    "\n",
    "* New item x is classified based on the class of it's nearest neighbour in the dataset $x_1 ... x_n$\n",
    "* Distance is measured as the sum of the Euclidean squared distance across all dimensions D\n",
    "$$d(x,x') = \\sqrt{\\sum_{i=1}^{D}(x_i - x'_i)^2}$$\n",
    "* PCA can be used in cases of a high number of dimensions to improve calculation speed and improve results\n",
    "* It is not clear how to deal with missing data however or incorporate prior beliefs and domain knowledge\n",
    "\n",
    "<h4> What is Mahalanobis distance?</h4>\n",
    "This is where you multiply the distance by the covariance matrix of the input (from all classes) which rescales the input vector and thus large values dominate less\n",
    "\n",
    "$$d(x, x') = (x − x' ) S^{−1} (x − x')$$\n",
    "\n",
    "where S is the covariance matrix of the inputs (from all classes)\n",
    "\n",
    "This helps make the input vector more scale tolerant (inputting as mm vs cm should provide similar results)\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is KNN?</h4>\n",
    "\n",
    "* Extension of NN where you use the class of the K nearest neighbours not just the 1st nearest\n",
    "* We first work out the liklihood model for each class (across all the data of that class):\n",
    "$$ p(x|c=0) = \\frac{1}{N_{class0}} \\sum_n \\mathcal{N}(x|x^n, \\sigma^2I)$$\n",
    "$$ p(x|c=1) = \\frac{1}{N_{class1}} \\sum_n \\mathcal{N}(x|x^n, \\sigma^2I)$$\n",
    "\n",
    "\n",
    "* We work out the probability of a point being within a normal distribution of every other point\n",
    "* To deal with outliers we create aritifical points where the mean of the class is, and give it a very large variance. That way the outlier's class is not infuenced by whatever is closes and effectively reverts back to the prior\n",
    "\n",
    "To then classify a new data point we use Bayes rule:\n",
    "\n",
    "$$ p(c=0|x^{*}) = \\frac{p(x^* | c = 0)p(c=0)}{p(x^{*}|c=0)p(c=0)+p(x^{*}|c=1)p(c=1)}$$\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='FastestNN'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Fastest Nearest Neighbours</h4>\n",
    "\n",
    "* K-NN has the problem of being computationally expensive for large datasets. It takes O(D) complexity as for every data point you wish to classify you have to examine every other data point\n",
    "\n",
    "* There are several extensions that try and over come this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Orchard's Algorithm</h4>\n",
    "\n",
    "\n",
    "* Creates a one-time ordered list of the distance between each data point that you can use to to speed up the calculation of your nearest neighbours\n",
    "* Utilizes this rule\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml014.png\" height=\"100\" width=\"500\">\n",
    "\n",
    "Notes:\n",
    "* Precomputation of distance matrix: O DN$^2$\n",
    "* Evaluating each member in the list: O (D)\n",
    "* Need to examine M < N members in the lists.\n",
    "* Orchard’s algorithm can work well in low dimensional cases, avoiding the calculation of many distances.\n",
    "* It requires however a potentially very time consuming one-time calculation of all point to point distances.\n",
    "* The storage of this inter-point distance matrix can be prohibitive.\n",
    "\n",
    "\n",
    "<h4>AESA</h4>\n",
    "\n",
    "The triangle inequality can be used to form a lower bound\n",
    "$$d(q,x^j ) \\geq d(q, x^i ) − d(x^i , x^j )$$\n",
    "\n",
    "Define I to be the set of datapoints for which d(q, x^i ), i \\element I has already\n",
    "been computed. One can then maximise the lower bounds to find the tightest\n",
    "lower bound on all other d(q, x^j )\n",
    "\n",
    "$$d(q, x^j ) \\geq max [d(q, x^i ) − d(x^i , x^j )]$$\n",
    "where i∈I\n",
    "\n",
    "* All datapoints x j whose lower bound is greater than the current best nearest\n",
    "neighbour distance can then be eliminated. \n",
    "* One may then select the next (non-eliminated) candidate datapoint x j corresponding to the lowest bound and continue, updating the bound and eliminating.\n",
    "\n",
    "Notes:\n",
    "* Precomputation of distance matrix: O (DN^2)\n",
    "* Evaluating the bound for all M remaining datapoints: O (M (N − M ))\n",
    "\n",
    "Need to compute M < N bounds.\n",
    "\n",
    "-----------------------\n",
    "\n",
    "* Both Orchard’s algorithm and AESA can significantly reduce the number of\n",
    "distance calculations required.\n",
    "\n",
    "* However, we pay an O N 2 storage cost. For very large datasets, this storage cost is likely to be prohibitive.\n",
    "* (More strictly, we can actually compute the distances d(x i , x j ) ‘on the fly’ when they are required. For a single query this may be effective; however for many different queries, we would end up recomputing these distances – hence we may as well store them.)\n",
    "\n",
    "* Given the difficulty in storing d i,j , an alternative is to consider the distances between the training points and a smaller number of strategically placed ‘buoys’ (also called ‘pivots’ or ‘basis vectors’)\n",
    "\n",
    "* These buoys can be either a subset of the original datapoints, or new positions.\n",
    "\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "<h4>KD Trees</h4>\n",
    "\n",
    "* K-dimensional trees are a way to form a hierarchical partition of the space that can be used to help speed up search.\n",
    "* This is a form of ‘spatial data structure’\n",
    "* An advantage over the Orchard and AESA approaches is that the storage cost of the tree is only O(N).\n",
    "* Also, only in the worst case are O (N) operations required for a new query.\n",
    "* Before introducing the tree, we’ll discuss the basic idea on which the potential speed-up is based."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='SVMs'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">SVMs</h3>\n",
    "\n",
    "* Basic concept is that we can linearly separate what appears to be linearly inseparable data by addining new dimensions to it until we can find a linear seperation that works (think going from 2d to 3d). This transformation is known as a Kernel.\n",
    "* However as technically transforming all the data like that is computationally in efficient there is a 'Kernel trick' that allows us to find the separation without having to transform the data\n",
    "\n",
    "http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Dimensionality Reduction'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">3. Dimensionality Reduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='PCA'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">PCA</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Good resource: https://www.coursera.org/learn/machine-learning/lecture/ZYIPa/principal-component-analysis-algorithm\n",
    "http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
    "\n",
    "\n",
    "<h4>Explain the PCA algorithm</h4>\n",
    "\n",
    "<b>Pre-processing:</b>\n",
    "1. If the mean of the data is not 0 to begin with center the data by substracting the mean of each dimension, D\n",
    "2. If the different features have vastly different scales then (e.g. size of house, number of rooms) then normalize, typically done by dividng by the std. deviation\n",
    "\n",
    "<b>Main PCA algorithm:</b>\n",
    "1. Formulate the co-variance matrix, S, as a DxD matrix: $S=\\frac{1}{N} XX^T$\n",
    "\n",
    "2. If using Eigen-decomposition approach (theoretical way):\n",
    "    * Calculate the Eigenvectors of S such that they are each orthogonal to the ones previously\n",
    "    * B = top M eigenvectors\n",
    "    * Complexity based on eigen-decomposition is O($D^3$)\n",
    "\n",
    "3. In practice, it can be impractical to first compute the covariance matrix and then the eigenvalues. Instead we can use the SVD approach which works here as S is a positive semi-definite matrix. This gives a more stable result:\n",
    "    * the SVD of S gives you $UDV^T$\n",
    "    * We only care about the first M columns of U, which form the basis matrix B.\n",
    "\n",
    "4. Reduced dimensional representations is then $Y = B^T \\tilde X$.\n",
    "5. The approximate higher dimensional reconstruction back again is given by $\\tilde X$ ≡ BY + M where M = is a vector of means that you are adding back on (see pre-processing). Also note it's B not $B^T$ here.\n",
    "6. The error is calculated as the sum of the squares difference between, $\\tilde x$, and x\n",
    "\n",
    "Notes:\n",
    "\n",
    "* B is a D x H dimensional matrix (for every dimension you have a reduced H mapping)\n",
    "* B consists of vectors that are orthogonal to the original data points. But it does not take into account the rotation - the data that is projected back out could fit the same variance as the original but rotated in the dimension space.\n",
    "\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Principal Component Analysis (PCA) is a method to form a lower-dimensional representation of data. For datapoints $x^n,n=1,...N$, define the matrix\n",
    "\n",
    "$$X=[x^1,x2...x^N]$$\n",
    "\n",
    "That is, for datapoints x with dimension D, then X is DxN dimensional. The data is such that the mean is zero.\n",
    "\n",
    "The covariance matrix of the data S, has elements \n",
    "\n",
    "$$S_ij=\\frac{1}{N}\\sum^N_nx^n_ix^n_j$$\n",
    "\n",
    "1. Explain how to write S in terms of matrix multiplication X</h4>\n",
    "<br>\n",
    "S is a co-variance matrix therefore DxD so: $$S=\\frac{1}{N} XX^T$$\n",
    "\n",
    "<hr />\n",
    "\n",
    "<h4>2. PCA is based on the linear model of the data\n",
    "\n",
    "$$x^n\\approx My^n$$\n",
    "\n",
    "where M is a DxH dimensional matrix, and each $y^n$ is a H dimensional vector, with $H<D$. \n",
    "<br><br>\n",
    "With reference to an orthognal matrix\n",
    "<br>\n",
    "$$R^TR=I$$\n",
    "<br>\n",
    "explain why there is no unique setting in general for M and $y^n$. Use also a diagram to explain the geometric meaning of this result\n",
    "</h4>\n",
    "\n",
    "* The log-liklihood is the same even with an orthogonal rotation of B --> RB where $R^TR$ = I. \n",
    "* draw a line on a diagonal line on a grid with dots around it, rotate the line and show how the variance captured by the line can stay the same even though it has rotated\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Datapoints $x^n,n=1,...N$, define the matrix\n",
    "$$X=[x^1,x^2,...x^N]$$\n",
    "\n",
    "That is, for data points x with dimension D, then X is D $\\times$ N dimensional. The data is such that the mean is 0, that is:\n",
    "\n",
    "$$\\sum_{n=1}^{N}x^n=0$$\n",
    "\n",
    "K-dimensional PCA aims to find a representation:\n",
    "\n",
    "$$x^n\\equiv \\sum_{k=1}^{K}y^n_kb^k$$\n",
    "\n",
    "where $b^1,...b^K$ are 'basis' vectors and $y^n_k$ are coeffcients\n",
    "<br>\n",
    "<br>\n",
    "Explain how to efficiently compute the basis vectors and coefficients in order to minimize the squared loss between the approximation and each $x^n$, namely:\n",
    "\n",
    "$$\\sum_{n=1}{N}\\left(x^n-\\sum_{k=1}^{K}y^n_kb^k\\right)^2$$</h4>\n",
    "\n",
    "Ans:\n",
    "* Basis vectors can be calculated through finding Eigenvectors of the co-variance matrix or by performing Singular Value Decomposition of the co-variance matrix\n",
    "* In either case we pick the top M vectors to form the matrix B\n",
    "* The co-efficients are the lower dimensional representation of the data and can be found by:\n",
    "$Y = B^T X$.\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain why PCA is often used as a pre-processing step in machine learning and explain what the geometric meaning of PCA is</h4>\n",
    "\n",
    "http://visionandbeyond.blogspot.co.uk/2014/08/how-to-apply-pca.html\n",
    "\n",
    "* PCA is used as it reduces the data dimensions to something more manageable, and in particular by making the matrix more dense, so the algorithm is more efficient\n",
    "* The geometric meaning of is that PCA1 capture the greatest distance in the dimension space, so if the data as shaped like an oval in a 2d space, it would capture the diameter from one end to the other. PCA2 captures the the distance that is perpendicular to this (so the second greatest distance) and the process continues until principal components count = desired num of dimensions and <=D\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain how to write the co-variance matrix S, in terms of the matrix multiplication of X</h4>\n",
    "* As we have assumed the mean is zero we can write: \n",
    "$$S = \\frac{1}{N-1}XX^T$$\n",
    "\n",
    "where N = num of observations\n",
    "<br>\n",
    "(Dummys note: so we are matrix-multiplying X with itself to find the strength of the correlation, and taking the average)\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Consider the situation in which the datapoints x n are very sparse - that is, only a few elements of each vector x n are non-zero, resulting also in a sparse matrix S. Describe a computationally efficient procedure to estimate the principal direction (the largest eigenvector of S) and explain why this is efficient.</h4>\n",
    "\n",
    "* One way of finding the principal eigenvector (the one with the larges value, PCA1) is to take any non-zero vector, and repeatedly multiply it with the covariance matrix\n",
    "* This process will converge to the principal eigenvector (eigenvector with the largest eigenvalue). \n",
    "* This is particularly efficient for the case that S is sparse since each matrix-vector product will be fast\n",
    "\n",
    "See: https://www.youtube.com/watch?v=fKivxsVlycs\n",
    "\n",
    "-----------------\n",
    "\n",
    "<h4> Continuing with the sparse dataset scenario, what would be a technique to perform full PCA (not just the principal eigenvector) efficiently?</h4>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Sparse_PCA\n",
    "\n",
    "*  Using the method described above one could find the princial eigenvector then deflate \n",
    "\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> How do you break rotational invariance in PCA</h4>\n",
    "\n",
    "* You set you principal eigenvector to be the one that maximizes the variance (corresponds to the largest eigenvalue of S)\n",
    "* All subsequent vectors will have to be orthogonal to preceding ones which together means your solution will not be rotationally invariant\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " <h4> Explain why the SVD method is related to the eigen decomposition of S and explain the computational complexity of this approach to performing PCA compared to directly computing the eigen-dcomposition of S</h4>\n",
    "* The SVD approach is equivalent to finding an eigen-decomposition of the sample covariance matrix and then taking the leading M eigenvectors and their correspoding eigenvalues $\\lambda_i$\n",
    "* $\\lambda_i = D^2_ii$ of S V D\n",
    "\n",
    "* Using the SVD to perform PCA makes much better sense numerically than forming the covariance matrix to begin with, since the formation of $XX^T$ can cause loss of precision\n",
    "\n",
    "* PCA requires calculating the eigenvalues and eigenvectors of the covariance matrix, which is the product $XX^⊤$, where X is the data matrix and when the mean is 0.\n",
    "\n",
    "* Since the covariance matrix is symmetric, the matrix is diagonalizable, and the eigenvectors can be normalized such that they are orthonormal\n",
    "\n",
    "* PCA corresponds to setting B = $U_M$ and the eigenvalues are the diagonal elements of $D_M$ squared.\n",
    "\n",
    "\n",
    "\n",
    " -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is the orthonormality constraint in PCA?</h4>\n",
    "* $B^TB$ must $= I$, so that the basis vectors are mutually orthogonal and of unit length.\n",
    "* Done using Larange multipliers so that the objective is to minimize:\n",
    "$$-trace(SBB^T)+trace (L(B^TB-I))$$\n",
    "\n",
    "-----------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain PCA with missing data</h4>\n",
    "\n",
    "* The idea that if you have a fixed b, then you can optimize on Y instead (still doing using x to measure the loss)\n",
    "* This results in finding a Y that results in filling in the missing data\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain Canonical Variates</h4>\n",
    "\n",
    "A form of dimensionality reduction in supervised learning that seeks to encode <class distinction in the basis vectors so that the the classifications aren't lost when doing PCA\n",
    "\n",
    "1. Compute the between and within class scatter matrices A, and B.\n",
    "2. Compute the Cholesky factor $\\tilde B$ of B.\n",
    "3. Compute the L principal eigenvectors [$e_1 , . . . , e_L$ ] of $\\tilde B^{-T} A \\tilde B^{−1}$.\n",
    "4. Return W = [$e_1 , . . . , e_L$] as the projection matrix.\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='NNMF'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Non Negative Maxtrix Factorization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>PCA can be considered a form of matrix factorisation. An alternative matrix factorisation method is probabilistic latent semantic analysis (PLSA) (also called non\n",
    "negative matrix factorisation). This takes a positive matrix X whose entries all sum to 1:\n",
    "    \n",
    "$$\\sum_{ij}X_{ij}=1, \\quad 0\\leq X_{ij} \\leq 1$$\n",
    "\n",
    "and forms an approximation based on\n",
    "\n",
    "$$X_ij \\approx  \\sum^H_{k=1}U_{ik}V_{kj}$$\n",
    "\n",
    "for matrices U and V non-negative entries and $\\sum_iU_{ik} = 1 \\quad and \\quad \\sum_kV_{kj}=1$\n",
    "<br><br>\n",
    "1. Explain what are the typical characteristics of the 'eigenfaces'in PCA compared with the 'plsa' faces in Matrix Factorization.\n",
    "<br><br>\n",
    "2. Derive an algorithm to find U and V based on an interpretation of X, U and V in terms of probability distributions.\n",
    "\n",
    "</h4>\n",
    "\n",
    "1. The PSLA faces are more localised and one of them, for example, might be clearly\n",
    "emphasising aspects of the chin. The eigenfaces are more general and emphasise broader\n",
    "aspects of the image. (Note though that overall, PLSA must have a higher reconstruction\n",
    "error since it has more constraints (positive bases).)\n",
    "\n",
    "2.\n",
    "** I am only 20% sure this answer is anywehre close to being correct **\n",
    "* We can transform X,U and V into probability distributions using $p = \\frac {CountOfElement }{SumOfElement Count}$\n",
    "* Let p = p(X,Y): This is the true probability\n",
    "* Let $\\tilde p$ = p(X,UV) \n",
    "* Let z = p(U) and y = p(V)\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml001.png\" height=\"100\" width=\"300\">\n",
    "\n",
    "Notes:\n",
    "* Line 3 - is keeping track of the p(z|x,y) in the last iteration\n",
    "* Line 4 - is updating p(x|z) using chain rule to match p(x, y) * previous weightings\n",
    "* Line 5 - is doing the same but in the output end, p(y|z)\n",
    "* Loops until the difference between old & new p(z | x,y) is minimal\n",
    "\n",
    "* Now derive z from the conditional probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Optimization'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">4. Optimization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='OptimizationMethods'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Optimization Methods</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='GradientDescent'></a>\n",
    "<h4>Explain Gradient descent optimization for minimizing the least-squared loss E(w)</h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "Essential viewing: https://www.youtube.com/watch?v=TEB2z7ZlRAw\n",
    "\n",
    "* Gradient descent is an iterative method that seeks to minimize some function f(x) in cases where no closed form solution exists\n",
    "* It works by determining the new value of f(x) as: <br>\n",
    "$$=NewFunctionValue \\approx OldFunctionValue + VectorChange * GradientOfFunction$$\n",
    "\n",
    "$$= f(x_{k+1})\\approx f(x_k)+(x_{k+1}-x_k)^T \\triangledown f(x_k)$$ \n",
    "\n",
    "* We can infer that the value that maximizes the change in the function is going to be when the VectorChange (can be thought of as the direction of change) overlaps the most it is in the same direction as the gradient.\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml013.png\" height=\"100\" width=\"300\">\n",
    "\n",
    "* The VectorChange is worked out as: \n",
    "$$New \\ x = previous \\ x - LearningRate* GradientOfFunction^2$$\n",
    "\n",
    "$$x_{k+1} = x_k - \\epsilon \\triangledown f(x_k)^2$$\n",
    "\n",
    "Notes:\n",
    "* It is the 'previous x minus' that causes it to go to do gradient descent rather than ascent\n",
    "\n",
    "\n",
    "* We can aso write this out in Matrix notation:\n",
    "\n",
    "$$x_{k+1} = x_k - \\epsilon C^{-1}g$$\n",
    "\n",
    "where the C is a positive definite matrix so that $\\epsilon$ gets converted into a vector\n",
    "\n",
    "<b>Adv/Dis-adv:</b>\n",
    "* Will always converge to the optimal solution for a convex problem provided the learning rate is small enough\n",
    "* If the LearningRate is too great it may overshoot the optimal and hence zig-zag\n",
    "* However if it's too small it will take a long time to converge\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='NewtonsMethod'></a>\n",
    "<h4> Explain Newtons Optimization Method</h4>\n",
    "\n",
    "* The Taylor series expansion tells us that any function can be written as a series of gradients on top of each other\n",
    "\n",
    "* Newtons methods makes use of this to write $f(x + \\triangle)$ up to the second order, where the second order of the gradient, is called the Hessian matrix, H\n",
    "\n",
    "* Written in that way (formula not shown), we can use differentiation to find f has the lowest value by the Hessian divided by the gradient:\n",
    "$x_{k+1} = x_k - \\epsilon H_f^{-1} \\nabla f$\n",
    "\n",
    "(remember $^{-1}$ is how  you divide in Matrix multiplication)\n",
    "\n",
    "* This means Newtons method can converge in one step (for $\\epsilon$ = 1), although generally one uses a value < 1 to avoid overshooting effects.\n",
    "\n",
    "<b>Adv/Dis-adv</b>\n",
    "\n",
    "* Adv: The decrease of the objective function is independent of the coordinate system (for linear transformations of the coordinates)\n",
    "* However we cant' guarantee we will go downhild unless the learning rate is small and H is a positive definite\n",
    "* More significantly storing the Hessian and solving the linear system $H{^−1}_f \\nabla f$ is very computationally very expensive\n",
    "* So instead we can use Conjugate gradient to minimize $H\\triangle = \\nabla f$ instead\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='CG'></a>\n",
    "<h4>Explain the Conjugate Gradient descent method for minimizing the least-squared loss E(w)</h4>\n",
    "\n",
    "https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\n",
    "\n",
    "* Conjugate gradient descent is a way of doing gradient descent by optimizing along one dimension at a time\n",
    "* We have a set of weights, x that we wish to update with each iteration until it reaches the optimal\n",
    "$$x_k+1 = x_k + \\alpha_kp_k$$\n",
    "\n",
    "* Imagine an n dimensional problem that is convex such as this quadratic: \n",
    "\n",
    "    <img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml011.png\" height=\"100\" width=\"300\">\n",
    "<br>\n",
    "* A represents the thing you are multiplying your weights with (so in NN either input data or output from the prevous layer)\n",
    "\n",
    "* We wish to find the optimal weights (x in this formula) such that it minimizes the error\n",
    "\n",
    "* Conjugate gradient descent says that we get the zig-zag behaviour because A is not diagonal (it has values everywhere and therefore you cannot optimize for just one dimension without impacting the others\n",
    "\n",
    "\n",
    "* So we break up our weight vector x into two components $\\alpha$ and P such that $P^TAP$  is a diagonal matrix \n",
    "\n",
    "* In other words P becomes a filter that diagonalizes the problem for any given iteration leaving us free to do a line search for the optinal alpha in that iteration\n",
    "\n",
    "* We can now optimize along each dimension i, and each time the $p_i$ vector is updated such that it is orthogonal to all previous p vectors in order for this filtering to work\n",
    "\n",
    "* You can imagine this in the bowl as a two vectors that are orthogonal (right-angled) from each other - that way  you can optimize one without worrying about the effect it will have on the other\n",
    "\n",
    " <img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml012.png\" height=\"100\" width=\"200\">\n",
    " \n",
    "* In a quadratic problem this means that this:\n",
    "$$f(x)=\\frac{1}{2}x^TAx-b^Tx $$\n",
    "becomes this (i.e. x replaced by $\\alpha$ p):\n",
    "\n",
    "$$f(x)=\\frac{1}{2}p^TAp-b^Tp $$\n",
    "\n",
    "$$f(x)=\\sum_{i=1}^{n} \\left(\\frac{1}{2} \\alpha_i^2p_i^TAp_i-\\alpha_ib^Tp_i \\right ) $$\n",
    "\n",
    "\n",
    "* We can find each new p using the Polak-Ribière formula: which uses the gradients to find the next conjugate vector, which then allow us to make the update within each iteration:\n",
    "$$x_k+1 = x_k + \\alpha_kp_k$$\n",
    "\n",
    "where each p is a diagonal only for k (so when you move to the next direction it becomes 0's)\n",
    "\n",
    "* Note: In a non quadratic problem no such method exists\n",
    "\n",
    "Finally we get to the following algorithm:\n",
    "\n",
    "1. k = 1\n",
    "2. Choose $x_1$\n",
    "3. $p_1$ = $−g_1$  # pick any gradient\n",
    "4. while $g_k \\neq 0$ do  # while not at the minimum\n",
    "    \n",
    "    * $\\alpha_k$ = argmin $f(x_k + \\alpha_k p_k) $  &nbsp;&nbsp;&nbsp;&nbsp; # Do a line search to find the optimal $\\alpha$\n",
    "    * $x_{k+1} := x_k + \\alpha_k p_k$    &nbsp;&nbsp;&nbsp;&nbsp; # Update x based on p\n",
    "    * $\\beta_k := g_{k+1}^Tg_{k+1}/(g^T_kg_k)$  &nbsp;&nbsp;&nbsp;&nbsp; # Find new conjugate direction\n",
    "    * $p_{k+1} =  -g_{k+1} + \\beta_kp_k$     &nbsp;&nbsp;&nbsp;&nbsp; # Update next p\n",
    "    * k = k +1\n",
    "10. end while\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Convexity'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Convexity</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Show that $-log \\sigma(x)$  is convex where $\\sigma$ represents the sigmoid function</h4>\n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{1}{\\sigma(x)}\\sigma(1-\\sigma) = \\sigma(x) -1$$\n",
    "$$\\frac{dfdx}{dx} = \\sigma(x) (1- \\sigma(x)$$\n",
    "\n",
    "Because sigma lies between 0-1 we, this function will also always lie between 0 and 1, threfore > 0 and hence convex\n",
    "\n",
    "Notes:\n",
    "* derivative of a log(x) is the $\\frac{1}{log(x)}$\n",
    "* derivative of a sigmoid is sigmoid(x)*(1-sigmoid(x))\n",
    "* in an exam the x they give you might be more complicated. Don't fall for it - just say z= .... and repeat steps above\n",
    "\n",
    "------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the concept of line search optimization and show that for a line going through the point $w_k$ and direction $p_k$,\n",
    "\n",
    "$w=w_k + \\lambda p_k$\n",
    "<br>\n",
    "the optimal point on the line to minimize the squared error E(w) is given when:\n",
    "<br><br>\n",
    "$$\\lambda = \\frac{(b-Ap_k)^Tp_k}{p^T_kAp_k}$$</h4>\n",
    "\n",
    "** No idea where I got this question from **\n",
    "\n",
    "* Line search optimization is when you choose to only change one dimension within the weight vector and optimize along that direction before repeating for other dimensions until convergence is found\n",
    "\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain convergence rate for convex funtions</h4>\n",
    "\n",
    "* Assuming a convex function with $x_1$ to $x_T$ steps the gradience descent algorithm will take\n",
    "* and there is an optimal finite x^*\n",
    "* Then the gradient has a constant, L, that means the Hessian maximum Eigenvalue is less than or equal to L\n",
    "$$H(x) \\succeq LI$$ ($\\succeq$ means that $H(x_i) \\leq LI $ for every index i)\n",
    "\n",
    "* In this case we can say that the convergence rate is of order O(1/T) where T is the number of iterations\n",
    "* $$f(x_T) - f(x^*) \\leq \\frac{1}{2 \\epsilon T}(x_1-x^*)^2$$\n",
    "\n",
    "(the amount you are above the optimal by will be <=  ...)\n",
    "\n",
    "* In practice the results may be better than this\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml007.png\" height=\"100\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the meaning of the 'conjugate direction' and why this means that the optimum of E(w) can be found by optimizing along each conjugate direction independently</h4>\n",
    "\n",
    "Ans\n",
    "* This is the idea of moving along one dimension to the find the optimal,then moving along another dimension that is conjugate to the first one. This way the change in the new dimension will not impact the change from the first dimension.\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain Momentum</h4>\n",
    "\n",
    "* Taking a moving average of the update \n",
    "* Reduces zig-zagging behaviour and help push past saddle points (local minima followe by a local maxima)\n",
    "* It is one way of updating the moving average without re-computing everything\n",
    "* $$x_k + \\tilde g_{k+1}$$ \n",
    "\n",
    "* Momentum can increase the speed of convergence since, for smooth objectives, as we get close to the minimum the gradient decreases and standard gradient descent would start to slow down\n",
    "* If the learning rate is too large, standard gradient descent may oscillate, but momentum may reduce oscillations by going in the average direction.\n",
    "* However, the momentum parameter $\\mu$ may need to be reduced with the iteration count to ensure convergence.\n",
    "* Particularly useful when the gradient is noisy. By averaging over previous gradients, the noise ‘averages’ out and the moving average direction can be much less noisy.\n",
    "* Momentum is also useful to avoid saddles (a point where the gradient is zero, but the objective function is not a minimum, such as the function x 3 at the origin) since typically the momentum will carry you over the saddle.\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain Nestorov's accelerated gradient</h4>\n",
    "\n",
    "* New weights = Prev weights + Previous update - LearningRate*(derivative of \"new weights if you stick to previous update\" w.r.t  old weights)\n",
    "\n",
    "$$x_k = x_k-1 + V_k$$ \n",
    "where $$v_k = v_k-1 - \\epsilon\\frac{\\delta}{\\delta x}f(x_{k-1} + v_{k-1}*dampening factor)$$\n",
    "\n",
    "* So you are in effect moving the previous update in the optimal direction\n",
    "* Nestorov also added a factor to slow down the momentum as the solution approached the minimum so that it would converge. The final equation for v is:\n",
    "where $$v_{k-1} = \\mu v_{k-1} - \\epsilon\\frac{\\delta}{\\delta x}f(x_{k-1} + \\mu_{k-1}v_{k-1}*dampening factor)$$\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> For input-output training points $(xn,yn), n = 1; ... ,N$, where each input $x^n$ is a vector\n",
    "and each output yn is a scalar, the squared loss of a linear regression model is\n",
    "\n",
    "$$E(w) = \\frac{1}{N} \\sum^N_{n=1}(y^n-w^Tx^n)^2$$\n",
    "<br><br>\n",
    "1. Compute the gradient and Hessian of this objective function and show that E (w) is convex.</h4>\n",
    "<br><br>\n",
    "\n",
    "$\\frac{\\delta E}{\\delta w_i} = 2\\sum_n(y^n-w^Tx^n)x^n_i $\n",
    "\n",
    "$\\frac{\\delta E}{\\delta w_ij} = 2\\sum_n(x^n)x^n_i = 2 \\sum x^n_jx^n_i   $\n",
    "\n",
    "Or in matrix form $= 2X^TX$\n",
    "<br><br>\n",
    "This will always be > 0 and is therefore convex\n",
    "\n",
    "-----------------------\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml009.png\" height=\"100\" width=\"500\">\n",
    "\n",
    "-----------------\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml008.png\" height=\"100\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='LargeScale'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">5. Large Scale Machine Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> For input-output training points $(xn,yn), n = 1; ... ,N$, where each input $x^n$ is a vector\n",
    "and each output $y^n$ is a scalar, the squared loss of a linear regression model is </h4>\n",
    "\n",
    "$$E(w) = \\frac{1}{N} \\sum^N_{n=1}(y^n-w^Tx^n)^2$$\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain what Stochastic Gradient Descent (SGD) is and how it could be used to find the w that minimises E(w)</h4>\n",
    "\n",
    "* In normal ('Batch') gradient descent you analyze all the data before making first W update\n",
    "\n",
    "* In SGD you update W after each single row of data\n",
    "\n",
    "* This means that while you are performing more updates, and may not reach the true optimal, you converge approximately to the optimal faster precisely because you are doing more updates so each new one update gains from the fact lots of other ones have happened before it\"\n",
    "<br>\n",
    "* This is useful in situations where that dataset is too big to store it all in memory and/or it will take a long time to update\n",
    "\n",
    "\n",
    "<h4>In the case that the input vectors are sparse (only a fraction f of the elements\n",
    "of each x n are non-zero), explain what computational savings this has when\n",
    "implementing gradient descent.</h4>\n",
    "\n",
    "<h4>Explain how Conjugate Gradients could be used to find the w that minimises\n",
    "E(w) and what computational savings can be made when the input vectors $x^n$ are sparse.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='AutoDiff'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Auto Differentiation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Describe Forward Automatic Differentiation (AutoDiff) and give two procedures\n",
    "(one exact and the other an approximation) that compute the gradient of a subroutine\n",
    "f(x) with respect to its arguments x, giving time complexities of the approaches.</h4>\n",
    "\n",
    "https://justindomke.wordpress.com/2009/02/17/automatic-differentiation-the-most-criminally-underused-tool-in-the-potential-machine-learning-toolbox/\n",
    "\n",
    "Ans:\n",
    "* AutoDiff takes a function f(x) and calculates the gradient\n",
    "* Crucially it does this without having to calculate the gradient for each element of x, so it scales more efficiently\n",
    "* AutoDiff exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically and accurately to working precision\n",
    "* Forward autodiff, takes the bottoms up approach by calculating the inner differentials first (the ones with respect to independent variables) before moving up the chain.\n",
    "* The complexity of forward autodiff is equivalent to the original function for which it is calculating the gradient\n",
    "\n",
    "There are two approaches to Forward-Auto Diff\n",
    "* Complex Arithmetic - gives an approximate answer only\n",
    "* Dual Arithmetic - gives an exact answer but it not efficient \n",
    "\n",
    "Both these appraches introduce an imaginary number $\\epsilon$\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain reverse mode Auto-Diff:</h4>\n",
    "\n",
    "* Reverse autodiff takes the top down approach, calculating the outer derivatives first and working inwards. \n",
    "* This requires storing all the intermediate calculations in memory as you go along, which may cause problems, unless checkpointing is used in which certain portions would have to reevaluated at the end\n",
    "* The complexity of reverse autodiff is will at best be greater than the original function, and depend upon the number of different paths the function takes\n",
    "* Forward-mode is efficient for functions taking one input and producing many outputs. Reverse-mode is efficient for functions taking many inputs and producing a single output. (That output would be a “loss function” in machine learning)\n",
    "\n",
    "\n",
    "<img src=\"../_img/aml_1.jpg\" height=\"100\" width=\"100\">\n",
    "\n",
    "-----------------\n",
    "<br><br>\n",
    "$$\\frac{df}{dy}=\\frac{\\delta f}{\\delta x}+\\frac{\\delta f}{ \\delta g}\\frac{\\delta g}{\\delta x}$$\n",
    "\n",
    "Notes:\n",
    "1. d = total derivative, $\\delta$ = partial\n",
    "2. Because f is impacted by x in two different ways, the total derivative of f w.r.t x is given by summing boths paths (https://en.wikipedia.org/wiki/Sum_rule_in_differentiation)\n",
    "3. The second path is broken down by the chain rule (https://en.wikipedia.org/wiki/Chain_rule)\n",
    "\n",
    "-----------------\n",
    "\n",
    "<h4> Write out the reverse mode differentiation for this:\n",
    "<img src=\"../_img/aml_2.jpg\" height=\"100\" width=\"300\"></h4>\n",
    "\n",
    "\n",
    "1. $\\frac{df}{dx} = (2x + gh) + (g^2xg) + (2x2gxxg) + (2xxh)$\n",
    "2. Substitute f and g to get to $2x + 8x^7$\n",
    "\n",
    "Note: If you are asked to create a graph for a function, then first differentiate that function then create the graph, building up each step of the calculation as an $f_1$ , $f_2$ etc as you go\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is the Reverse Mode Differentiation algorithm </h4>\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml005.png\" width=\"500\">\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<q4> Explain how to use Reverse AutoDiff to efficiently calculate the gradient with re\n",
    "spect to $\\theta_1 and \\theta_2$ of [insert nested complicated function of your choice]\n",
    "Your computation graph should have nodes representing elementary functions. Annotate your graph\n",
    "suitably and define the forward and backward passes explicitly.\n",
    "\n",
    "Ans:\n",
    "* To solve this put the wrt at the top of the chart (in this case two nodes of $\\theta_1$ and $\\theta_2$\n",
    "* then draw arrows down to show the tranistions, labelling the next node $f_1, f_2$ and so on\n",
    "* on the RHS explain what each f represents\n",
    "\n",
    "Now put the following explanation below\n",
    "* Reverse Mode Auto-Diff first does a forward pass to build up the above graph\n",
    "* Then go through the nodes in reverse order and calculate the derivative of each one, recursiely traversing through the child nodes in order to do so and remembering the child derivatives on the way back up\n",
    "* This is more efficient as you calculate common components just once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain how we can calculate the quantity Hessian-vector product $Hv$ using the autodiff framework:</h4>\n",
    "\n",
    "----------------\n",
    "\n",
    "We use the fact that $D_v(\\frac{\\partial E}{\\partial \\theta_i}) = [Hv]_i$ and then use the usual rule of reverse differentiation from autodiff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Consider a time series prediction problem in which, given a sequence of inputs\n",
    "Xl,X2\", \"Xt, we make a prediction)it for the output at time t. To do this we define:\n",
    "    \n",
    "$$h_1=x_1$$\n",
    "$$h_t=f(x_t, h{_t-1}, A)  \\quad t>1$$\n",
    "$$\\tilde y_t = g(h_t,B)$$\n",
    "\n",
    "where A and B are parameters and f and g are some (unspecified) functions. The\n",
    "objective is to find parameters A and B that minimise the loss\n",
    "\n",
    "$$\\sum^T_{t=1}(y_t-\\tilde Y_t)^2$$\n",
    "\n",
    "Explain how to use Reverse AutoDiff to efficiently calculate the gradient of this loss\n",
    "function with respect to A and B.\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='NN'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">6. Neural Networks</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='AutoEncoders'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Auto Encoders</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>\n",
    "Explain how auto-encoders can be used to find low dimensional representations of data and explain how PCA relates to an Autoencoder.</h4>\n",
    "\n",
    "\n",
    "Ans:\n",
    "\n",
    "* Auto-encoders use neural nets to learn to represent data using. Data is fed into a neural net with one or more hidden layers. The hidden layer with the smallest number of weights effective becomes the lower-level representation of the data. The outputs are then scaled back up to map the output back in the form of the original data. \n",
    "\n",
    "* The loss function then checks the difference between what was generated and the original input.\n",
    "\n",
    "* PCA is equivalent to an Autoencoder with a single hidden layer with a linear trasfer function. In this case PCA is a more efficient way of calculating the optimal.\n",
    "-----------------------\n",
    "\n",
    "We can break an autoencoder into two parts -- an encoder from the input to the bottleneck layer and a decoder from the bottleneck to the reconstruction. Both encoder and decoder can contain multiple layers. Let's write h=f(x) for the encoder part and x'=g(h) for the decoder.   If the decoder is a linear function, then the autoencoder cannot beat PCA. Otherwise it can.  Essentially this is what we showed in the lectures.\n",
    "\n",
    "Remember that f and g can themselves represent multiple layers. Consider an autoencoder x->h1->h2->h3->h4->h5->x' in which h3 is the bottleneck. Then even if (for example) h5->x' is linear, then the overall decoder h3->h4->h5->x' can still be a non-linear function of h3, provided either h3->h4 or h4->h5 is non-linear. Hence this autoencoder could beat PCA even with a linear output layer.\n",
    "\n",
    "\n",
    "<h4>\n",
    "Consider and Auto-encoder with structure x$\\rightarrow$ h $\\rightarrow \\tilde x$, trained to minimize the squared loss:\n",
    "\n",
    "$$\\sum_{n=1}{N}\\left(\\tilde x - x^n \\right)^2$$ \n",
    "with $h^n=f(Ax^n)$ and $\\tilde x^n = Bh^n$ for matrices A,B and a non linear function f.\n",
    "\n",
    "For k-dimensional h, is this non-linear procedure in principle more powerful than K-dimensional PCA, in the sense that it has lower squared loss? Explain fully your answer.\n",
    "</h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "------------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain what is meant by an auto-encoder neural network</h4>\n",
    "\n",
    "* An auto-encoder NN tried to find a lower dimensional representation of data by setting the output to to match the input dimensions, with reduction of dimension in 1 or more hidden layers in between\n",
    "\n",
    "* The hidden layer with the smaller dimensions (bottleneck layer) is the lower dimensional representation of the input data\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='RNNS'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">RNNs</h3>\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml004.png\" height=\"100\" width=\"200\">\n",
    "\n",
    "* Each h in this digram represents 1 or more hidden layers of that same time step\n",
    "* RNNs are used in timeseries applications\n",
    "* The basic idea is that the hidden units at time $h_t$ (and possibly output $y_t$ ) depend on the previous state of the network $h_t−1 , x_t−1 , y_t−1$ for inputs $x_t$ and outputs $y_t$ .\n",
    "* The above network is ‘unrolled the net through time’ to give a standard NN diagram.\n",
    "* Potential links from $x _t−1$ , $y_t−1$ to $h_t$  have been omitted\n",
    "\n",
    "There are two ways to train an RNN:\n",
    "* RTRL (which is a single forward pass in time but has high storage cost) \n",
    "* BPTT (which is a forward and backward pass with more modest storage cost). RTRL is straightforward, but BPTT requires an understanding of parameter tying (or more generally AutoDiff).\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is Parameter tying?</h4>\n",
    "\n",
    "* Parameter tying addresses the issue of how to differentiate netsted functions in which there is a common paramter $\\theta$ (not to be confused with input x)\n",
    "* One way would have been to do the chain rule\n",
    "* But another way is to treat the two as two separate functions and differentiate them individually\n",
    "* Then join the two unconstrained derivatives together (sum) and apply a constraint that $\\theta_1 = \\theta_2$\n",
    "\n",
    "1. Treat all parameters as independent and calculate the gradient with respect to\n",
    "each independent parameter.\n",
    "2. Sum all the resulting independent gradients together.\n",
    "3. Evaluate the expression by setting all the independent parameters to the same\n",
    "value.\n",
    "Note that this is a general result and can be used to deal with parameter tying\n",
    "in any objective, not just Deep Learning and Neural Nets.\n",
    "(Of course, this is again just a special case of AutoDiff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='LSTM'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">LSTM</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### An input-output time-series ($X_t,Y_t$), t = 1...T can be modelled by a recurrent LSTM (Long ShortTerm Memory) network. Explain the essential components of an LSTM network and what difficulties it tries to overcome (compared to standard recurrent networks).\n",
    "\n",
    "Ans:\n",
    "\n",
    "* AN LSTM tried to overcome the limitations of an RNN when it comes to storing long-term dependencies in memory. With a standard RNN this becomes computationally inefficient.\n",
    "* It does this by introducing the concept of a memory gates that can affect the memory state running through the function\n",
    "* The forget gate takes in the current input ($C_t$) and the previous output and  and uses them to decide whic parts of the current state should be reduced (forgotten)\n",
    "* The input gate also takes in the current input ($C_t$) and the previous output and decides whether anything should be added to the memory. This then gets added to the result of the forget step\n",
    "* The output gate then decides whether to activate an output based on the results at this timestep. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Initialization'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Initialization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "http://cs231n.github.io/neural-networks-2/#datapre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Visualization'></a>\n",
    "<h2 style=\"background-color:#616161;color:white\">7. Visualization\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain SNE and T-SNE</h4>\n",
    "\n",
    "<b>SNE</b>\n",
    "\n",
    "* Stochastic Neighbourhood Embedding was invented by Hinton and Rowels\n",
    "* It represents any high dimensional object, i, as the probability of it being close to every other neighbour,j\n",
    "$$p_{ij} = \\frac{exp(-d^2_ij)}{\\sum_{k \\neq i } exp(-d^2_{ik})}$$\n",
    "* The distance measure can be anything but Hinton proposes Eucliden distance $-(x_i - x_j)^2/(2\\sigma^2_i) $\n",
    "* This should give each point its own Gaussian distribution. Remeber that i and k here represent the dimension and could be large\n",
    "* The next step is to define a probability distribution that is a similar shape but it based on a lower level of dimensions\n",
    "$$q{j|i} = \\frac{exp(-(y_i-y_j)^2)}{\\sum_{j \\neq i} exp(-y_i-y_j)^2)}$$\n",
    "* When determining whether the two probabilities are similar SNE uses KL divergance\n",
    "* However this isn't symmetric - it maintains local structure but is in accurate on global structure\n",
    "* The Gaussian distribution also means that points far away have almost no impact in determining q\n",
    "\n",
    "<b>T-SNE</b>\n",
    "* Uses symmetric loss function when optimizing q\n",
    "* Uses a Students-T distribution for q\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml002.png\" height=\"100\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

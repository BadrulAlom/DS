{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3> Neural Networks </h3>\n",
    "\n",
    "Neural Nets, Artificial Neural Nets (ANNs), Deep Learning, it's all the same thing. \n",
    "\n",
    "If you are new to machine learning I highly recommend taking this course. It explains the concepts on this page better than I can:\n",
    "\n",
    "https://www.udacity.com/course/deep-learning--ud730\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The <b>input</b> into a neuron is described by: $$f(x)= \\sum_i w_ix_i +b$$   (written as $bw^Tx +b$ in vector form).\n",
    "b = bias\n",
    "<br>\n",
    "The activation function takes the input and puts it through some activation function\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Q: What are activation functions in a neural network?\n",
    "\n",
    "Artificial Neural Networks are composed of neurons that take 1 or more input, put it through an activation function, which sends and output if it exceeds a certain threshold. \n",
    "\n",
    "Types of activation functions:\n",
    "- Perceptron (specific threshold point which makes it go from 0 to 1)\n",
    "- Linear (output is is linear rather than hard-limiting. This allows their outputs to take on any value, whereas the perceptron output is limited to either 0 or 1.  Linear networks, like the perceptron, can only solve linearly separable problems.<br>\n",
    "- Sigmoidal \"Logistic\":\n",
    "- Hyperbolic tangent:\n",
    "- Rectified Linear Unit (ReLU):\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>The sigmoid function</h4>\n",
    "\n",
    "The sigmoid function is: $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "This is one possible way to activate a node based on the input. The strength of the sigmoid function increases as x gets close to 0. Many natural processes, such as those of complex system learning curves, exhibit a progression from small beginnings that accelerates and approaches a climax over time. When a detailed description is lacking, a sigmoid function is often used.\n",
    "<img src=\"img/sigmoidfunction.png\" height=\"100\" width=\"200\"> \n",
    "\n",
    "The derivative of the sigmoid function can be shown to be:\n",
    "can be shown to be: $$\\sigma' = \\sigma(1-\\sigma)$$\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### What is Softmax?\n",
    "\n",
    "* Turns any kind of scores into a probability such that it sums up to 1. Used in the the final layer of a neural network.\n",
    "* The lower the scores you give it them more it will trend towards a uniform distribution, but then as the size of the scores increase the more it will spread the probabilities of each score towards 0 or 1. Ie. Softmax can become ore confident about the probabilities the higher the score of the data points.\n",
    "\n",
    "\"Classification problems can take the advantage of condition that the classes are mutually exclusive, within the architecture of the neural network.\n",
    "For example, in the MNIST digit recognition task, we would have 10 different classes. Hence, the dimension of the output layer is 10. Ideally, best prediction is if the probability is 1.0 for a single output node, and probability of rest of the output nodes are zero.\n",
    "We should incorporate such mechanism within the architecture. Best architecture for such requirement is Max-layer output, which will provide probability of 1.0 for the maximum output of previous layer and probability of rest of the output node will be considered as zero. But such output layer will not be differentiable, hence will be difficult to train.\n",
    "Alternatively, if we use Softmax function as output layer, it will almost work like Max layer as well as it will be differentiable to train by gradient descent. Exponential function will increase the probability of maximum value of the previous layer compare to other value. Also, summation of all output will be equal to 1.0 always.\n",
    "Finally, a softmax layer trained on the hand written digits will output a separate probability for each of the ten digits, and the probabilities will all add up to 1.\" (https://www.quora.com/Artificial-Neural-Networks-Why-do-we-use-softmax-function-for-output-layer)\n",
    "\n",
    "Also see:\n",
    "http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Code for softmax in python\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(exp(x),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>What is one Hot Encoding?</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Encoding of categories into a vector of 0's and 1's such that each column represents only one of the categories, and across a row only one column can be a 1 at any one time, with evyerthing else turned off (0).\n",
    "* So I could represent a coin toss being Heads as [1,0] and Tail as being [0,1]\n",
    "\n",
    "* One possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K or one-hot encoding, which is implemented in OneHotEncoder. This estimator transforms each categorical feature with m possible values into m binary features, with only one active.\n",
    "\n",
    "<code>\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  \n",
    "OneHotEncoder(categorical_features='all', dtype=<... 'numpy.float64'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=True)\n",
    "\n",
    "enc.transform([[0, 1, 3]]).toarray()\n",
    "</code>\n",
    "\n",
    "* By default, how many values each feature can take is inferred automatically from the dataset.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is cross-entropy</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Q: Describe a recurrent neural net \n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "\n",
    "Ans: The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks thatâ€™s a very bad idea. If you want to predict the next word in a sentence you better know which words came before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Q: Describe a convolutional neureal net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Q: Describe back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Q: What is the local minima problem?\n",
    "\n",
    "* Gradient decscent can converge on a minima that is not the global minima.\n",
    "* However empirical studies show they are often just as good such that it shouldn't be a major worry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

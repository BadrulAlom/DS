{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Recommended reading:\n",
    "\n",
    "http://www.lauradhamilton.com/introduction-to-principal-component-analysis-pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Principal Components Analysis</h4>\n",
    "* In a table of information, each feature can be considered as a 'dimension'\n",
    "* Some dimensions are more important than others\n",
    "* PCA tries to reduce the number of dimensions by focussing on the features that explain the biggest variance in the data\n",
    "* The curse of dimensionality: As the number of dimensions grow, the amount of empty space in the solution space also grows\n",
    "\n",
    "<h4>Ways of dealing with high dimensionality</h4>\n",
    "By making assumptions about the data:\n",
    "* One way (as used in Naieve Bayes) is to treat each dimension as independent and so rather than having to go through every combination of possible data, you just look at each dimension on its own\n",
    "* Another way is to assume smoothness, that is to say you reduce the variance in the data by propogating some values into neughbouring regions\n",
    "* Assume symmetry\n",
    "\n",
    "By reducing the dimensionality of your data while trying to preserve information:\n",
    "* Through feature selection\n",
    "* Through feature extraction (combining dimensions to come up with a new single dimension) -- this is what PCA does\n",
    "\n",
    "<h4>PCA overview</h4>\n",
    "* PCA works by finding the path through the data that has the largest variance, and sets that as the first principal component. Not this path may not be the same as an individuiual dimension - it looks for the greatest variance in the data so it may be cutting across dimensions to find this\n",
    "* The second principal compoenent is then set to be perpendicular to the first, and the third is perpendicular to that and so on\n",
    "* In other words the model is trying to find the areas in the solution space that still have the high variance\n",
    "\n",
    "<h4>PCA algorithm</h4>\n",
    "* First center the data by subtracting the mean of each dimension from the values in that dimension, so that the center of the data is 0. This makes calculating the covariance matrix easier\n",
    "* The covariance matrix calculates the correlation between any two dimensions. \n",
    "* Multiplying the covariance matrix with any vector over and over again, makes it turn, like a compass, in towards the angle of most variance. So the trick is to figure out where its going to point to .\n",
    "* Given a vector to encode data, a particular data item that lies in rouoghly the same region of the dimension space can be appromixately represented by a lower dimension co-ordinate that best represents the sata\n",
    "* The number two represented by pixels being on/off across a grid for example, will only look like a number 2 in a certain combinaion of ways. But the 'essence' of what makes a 2 stays the same.\n",
    "* We can express this as:\n",
    "$$x^n\\approx c+ \\sum_{j=1}^{M}y_j^nb^j \\equiv \\tilde x^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here:\n",
    "* n denotes each data item\n",
    "* y is a lower dimension output\n",
    "* x is the original data input\n",
    "* c is a constant and represents a point in the subspace\n",
    "* b are the basis vectors - these can be thought of as the skeleton that represents the core of PCA upon which y is some multiplication of it\n",
    "\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> PCA - Measuring loss</h4>\n",
    "\n",
    "$$E(B,Y) = \\sum_{n=1}^{N}\\sum_{i=1}^D \\left[ x_i^n - \\sum_{j=1}^My^n_j b^j_i\\right]^2$$\n",
    "\n",
    "\n",
    "* To find the basis vectors b, and low dimensional co-ordinates Y, we minimize the sum of squared differences between each vector x and its reconstruction x̃\n",
    "* The loss function is the square distance between $x^n$ and $x^{\\tilde n}$ how this does not take direction into account - you could have multiple solutions with the same square loss, e.g. using -b instead of b. \n",
    "* We can make the solution unique by requiring that all b are orthonormal to one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Eigenvalues and Eigenvectors</h4>\n",
    "\n",
    "For a square matrix, A, a non-zero vector x is called an eigenvector if multiplication by\n",
    "A results in a scalar multiple of x.\n",
    "$$Ax = λx$$\n",
    "The scalar λ is called the eigenvalue associated with the eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

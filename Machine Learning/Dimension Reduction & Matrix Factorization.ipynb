{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Recommended reading:\n",
    "\n",
    "http://www.lauradhamilton.com/introduction-to-principal-component-analysis-pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Principal Components Analysis</h4>\n",
    "* Given a vector to encode data, a particular data item that lies in rouoghly the same region of the dimension space can be appromixately represented by a lower dimension co-ordinate that best represents the sata\n",
    "* The number two represented by pixels being on/off across a grid for example, will only look like a number 2 in a certain combinaion of ways. But the 'essence' of what makes a 2 stays the same.\n",
    "* We can express this as:\n",
    "$$x^n\\approx c+ \\sum_{j=1}^{M}y_j^nb^j \\equiv \\tilde x^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here:\n",
    "* n denotes each data item\n",
    "* y is a lower dimension output\n",
    "* x is the original data input\n",
    "* c is a constant and represents a point in the subspace\n",
    "* b are the basis vectors - these can be thought of as the skeleton that represents the core of PCA upon which y is some multiplication of it\n",
    "\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "    <h4> PCA - Measuring loss</h4>\n",
    "\n",
    "$$E(B,Y) = \\sum_{n=1}^{N}\\sum_{i=1}^D \\left[ x_i^n - \\sum_{j=1}^My^n_j b^j_i\\right]^2$$\n",
    "\n",
    "\n",
    "* To find the basis vectors b, and low dimensional co-ordinates Y, we minimize the sum of squared differences between each vector x and its reconstruction x̃\n",
    "* The loss function is the square distance between $x^n$ and $x^{\\tilde n}$ how this does not take direction into account - you could have multiple solutions with the same square loss, e.g. using -b instead of b. \n",
    "* We can make the solution unique by requiring that all b are orthonormal to one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Eigenvalues and Eigenvectors</h4>\n",
    "\n",
    "For a square matrix, A, a non-zero vector x is called an eigenvector if multiplication by\n",
    "A results in a scalar multiple of x.\n",
    "$$Ax = λx$$\n",
    "The scalar λ is called the eigenvalue associated with the eigenvector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

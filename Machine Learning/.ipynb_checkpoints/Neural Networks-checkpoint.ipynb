{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks are composed of neurons that take 1 or more input, put it through an activation function, which sends and output if it exceeds a certain threshold. \n",
    "\n",
    "Types of activation functions:\n",
    "- Perceptron (specific threshold point which makes it go from 0 to 1)\n",
    "- Linear (output is is linear rather than hard-limiting. This allows their outputs to take on any value, whereas the perceptron output is limited to either 0 or 1.  \n",
    "<br> Linear networks, like the perceptron, can only solve linearly separable problems.<br>\n",
    "- Sigmoidal \"Logistic\":\n",
    "- Hyperbolic tangent:\n",
    "- Rectified Linear Unit (ReLU):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>input</b> into a neuron is described by: $$f(x)= b + \\sum_i w_ix_i$$   (written as $b+w^Tx$ in vector form).\n",
    "b = bias\n",
    "<br>\n",
    "The activation function takes the input and puts it through some activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Softmax in Neural Nets:</h3>\n",
    "\n",
    "Passing the final layer of a neural network through a softmax function, normalizes it and prepares it for the log-loss calculation.\n",
    "\n",
    "\"Classification problems can take the advantage of condition that the classes are mutually exclusive, within the architecture of the neural network.\n",
    "For example, in the MNIST digit recognition task, we would have 10 different classes. Hence, the dimension of the output layer is 10. Ideally, best prediction is if the probability is 1.0 for a single output node, and probability of rest of the output nodes are zero.\n",
    "We should incorporate such mechanism within the architecture. Best architecture for such requirement is Max-layer output, which will provide probability of 1.0 for the maximum output of previous layer and probability of rest of the output node will be considered as zero. But such output layer will not be differentiable, hence will be difficult to train.\n",
    "Alternatively, if we use Softmax function as output layer, it will almost work like Max layer as well as it will be differentiable to train by gradient descent. Exponential function will increase the probability of maximum value of the previous layer compare to other value. Also, summation of all output will be equal to 1.0 always.\n",
    "Finally, a softmax layer trained on the hand written digits will output a separate probability for each of the ten digits, and the probabilities will all add up to 1.\" (https://www.quora.com/Artificial-Neural-Networks-Why-do-we-use-softmax-function-for-output-layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>One Hot Encoder:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often features are not given as continuous values but categorical. For example a person could have features [\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], [\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]. Such features can be efficiently coded as integers, for instance [\"male\", \"from US\", \"uses Internet Explorer\"] could be expressed as [0, 1, 3] while [\"female\", \"from Asia\", \"uses Chrome\"] would be [1, 2, 1].\n",
    "Such integer representation can not be used directly with scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered arbitrarily).\n",
    "One possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K or one-hot encoding, which is implemented in OneHotEncoder. This estimator transforms each categorical feature with m possible values into m binary features, with only one active.\n",
    "Continuing the example above:\n",
    "<code>\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  \n",
    "OneHotEncoder(categorical_features='all', dtype=<... 'numpy.float64'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=True)\n",
    "enc.transform([[0, 1, 3]]).toarray()\n",
    "array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])\n",
    "</code>\n",
    "\n",
    "By default, how many values each feature can take is inferred automatically from the dataset. It is possible to specify this explicitly using the parameter n_values. There are two genders, three possible continents and four web browsers in our dataset. Then we fit the estimator, and transform a data point. In the result, the first two numbers encode the gender, the next set of three numbers the continent and the last four the web browser.\n",
    "Note that, if there is a possibilty that the training data might have missing categorical features, one has to explicitly set n_values. For example,\n",
    "<code>\n",
    "enc = preprocessing.OneHotEncoder(n_values=[2, 3, 4])\n",
    "/# Note that there are missing categorical values for the 2nd and 3rd\n",
    "/# features\n",
    "enc.fit([[1, 2, 3], [0, 2, 0]])  \n",
    "OneHotEncoder(categorical_features='all', dtype=<... 'numpy.float64'>,\n",
    "       handle_unknown='error', n_values=[2, 3, 4], sparse=True)\n",
    "enc.transform([[1, 0, 0]]).toarray()\n",
    "array([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The sigmoid function</h3>\n",
    "\n",
    "The sigmoid function is: $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "This is one possible way to activate a node based on the input. The strength of the sigmoid function increases as x gets close to 0. Many natural processes, such as those of complex system learning curves, exhibit a progression from small beginnings that accelerates and approaches a climax over time. When a detailed description is lacking, a sigmoid function is often used.\n",
    "<img src=\"img/sigmoidfunction.png\" height=\"100\" width=\"200\"> \n",
    "\n",
    "The derivative of the sigmoid function can be shown to be:\n",
    "can be shown to be: $$\\sigma' = \\sigma(1-\\sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "     "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

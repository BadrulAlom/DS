{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Na√Øve Bayes Classifier Algorithm\n",
    "2. K Means Clustering Algorithm\n",
    "3. Support Vector Machine Algorithm\n",
    "4. Apriori Algorithm\n",
    "5. Linear Regression\n",
    "6. Logistic Regression\n",
    "7. Artificial Neural Networks & Deep Learning\n",
    "8. Random Forests\n",
    "9. Decision Trees\n",
    "10. k-Nearest Neighbours (kKN)\n",
    "11. Expectation Maximization (EM)\n",
    "12. Page Rank\n",
    "13. AdaBoost\n",
    "14. Classification and Regression Tree (CART)\n",
    "\n",
    "<b>A good high level overview of most of these can be found here: https://www.dezyre.com/article/top-10-machine-learning-algorithms/202 for a good  overview of most of these.</b>\n",
    "\n",
    "I will write my own explanation of each algorithm below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>AdaBoost</H2>\n",
    "<h4>The concept</h4>\n",
    "Create a good classifier by combining the outputs of multiple weak classifier - a \"wisdom of the crowds\" approach.\n",
    "\n",
    "$$F_T(x) = \\sum_{t=1}^T f_t(x)\\,\\!$$\n",
    "\n",
    "- where T = total number of classifiers\n",
    "- $f_t$ is individual weak classifier, $F_T$ is the strong classifier\n",
    "- x is the input value you wish to classify\n",
    "\n",
    "<h4>The algorithm</h4>\n",
    "<br>\n",
    "1. Given N rows of input (x) and output (y), where the y is either a 1 or a -1\n",
    "2. Distribute the initial weights as simply 1/N .   So $D_1(i) = 1/N$\n",
    "3. Iterate through each classifier (t) that you want to try (so t = 1 to T)\n",
    "    * See which classifer has the lowest error given the initial weight distribution\n",
    "    * Update the weight distribution such that that those that matched decrease in weighting, and those that did not match have a stronger weight (the idea being their error is being emphasised in the next iteration)\n",
    "4. Final classifier will be the weighted sum of the best 'weak' classifier in each iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H2>Naieve Bayes</H2>\n",
    "<br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Introduction\"><b>From Wikipedia</b></a><br>\n",
    "Naive Bayes is a simple technique for classifying things. It is not a single algorithm but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class that the data belongs to.<br> \n",
    "For example, a fruit may be considered to be an apple if it is 'red', 'round', and about '10 cm' in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features. An advantage of naive Bayes is that it only requires a small number of training data to estimate the parameters necessary for classification <br>\n",
    "<br>\n",
    "<br>\n",
    "<h4>The algorithm</h4>\n",
    "The maths behond it is explained very well in the <a href=\"http://scikit-learn.org/stable/modules/naive_bayes.html\">Skikit-learn pages</a>\n",
    "\n",
    "In a nutshell your start with Bayes Theorem:\n",
    "$${\\displaystyle p(Y_{k}\\vert \\mathbf {x} )={\\frac {p(Y_{k})\\ p(\\mathbf {x} \\vert Y_{k})}{p(\\mathbf {x} )}}\\,}$$\n",
    "\n",
    "and reason that:\n",
    "* p(x) can consist of mutiple things .. i.e.  inputs $x_1$,$x_2$,$x_3$\n",
    "* if the inputs $x_1$,$x_2$,$x_3$ are \"naievely\" assumed to be independent of each other (more on this later), so they don't impact each others probability of y, then we can use the multiplicative rule or probability and simplify the top to: \n",
    "$$p(Y)\\prod_{{i=1}}^{n} p(x_{i}\\vert Y_{k})$$\n",
    "* As for the denominator, we can say that's fixed from having analyzed a corpus of data and determined the p of each x. For purposes of classification we can therefore ignore it.\n",
    "\n",
    "* All this results in the simplified classificiation formula:\n",
    "\n",
    "\n",
    "$${\\begin{aligned}p(Y_{k}\\vert x_{1},\\dots ,x_{n})&\\varpropto p(Y_{k})\\prod _{{i=1}}^{n}p(x_{i}\\vert Y_{k})\\,.\\end{aligned}}$$\n",
    "<i>$\\varpropto$ means in proportion to</i>\n",
    "\n",
    "<br>The ability to assume independence under a given class is a crucial part of applied Bayes. So the words 'great' and tremendous' are postively correlated in a movie review, but if you took it for granted that the movie review was positive, then the probability of 'great' is unlikely to correlate as strongly with 'tremendous' - the two may be equally likely for instance.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Q: What is the formula for linear regression?</b>\n",
    "\n",
    "$y(x) = \\sum_{j=1}^{D} w_jx_j + e$\n",
    "\n",
    "Note:\n",
    "<br><i>e</i> represents the residual error between model and true output\n",
    "<br>We often assume e has a Gaussian (normal) distribution. We denote this by e ~ <i>N</i>($\\mu,\\sigma)$\n",
    "<br> $p(y|x,\\theta) = N(y | \\mu(x), \\sigma^2(x))$\n",
    "\n",
    "The above formula is a bitch to understand but here goes:\n",
    "- The error term represents the average distance away from the true value across all values of X ... where X may be 1 set of values or it may be a more complex function\n",
    "- The output, y, will follow a normal distribution equavelent to one generated from using the mean of x, and std. dev. of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Independent variables' do not have to be independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is multi-collinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Independent' variables that are highly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>What is hetroesdascity?<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is when variance of the error is increasing/decreasing over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Parametrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. Qunitic function = y(x) = $\\alpha_0+\\alpha_1x+\\alpha_2x+\\alpha_3x+\\alpha_4x$\n",
    "\n",
    "Higher degree poynomials are useful as they can provide a better fit to data if a simple linear model does not suffice. Each extra degree allows an additional level of curvature - notice for quadratic the curve changes once, and for quintic it changes 4 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is Polynomial Least Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Watch this video: https://www.youtube.com/watch?v=Z5iq95Vg2ZY&index=4&list=PLpT5xJ7AmkRW5Q0HwfVRWgUjFdiis-alc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>What is Ridge Regression?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is a technique that deals with over-fitting due to having too many parameters. If you think of this as resulting in lots of co-efficients that are highly tuned to the dataset it was trained on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is Robust Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very common to model the noise (i.e. error term) in regression models using a Gaussian distribution with zero mean and constant variance, \u0005i ∼ N (0, σ 2 ), where \u0005i = yi −wT xi . In this case, maximizing likelihood is equivalent to minimizing the sum of squared residuals, as we have seen. However, if we have outliers in our data, this can result in a poor fit. \n",
    "\n",
    "This is because squared error penalizes deviations quadratically, so points far from the line have more affect on the fit than points near to the line. One way to achieve robustness to outliers is to replace the Gaussian distribution for the response variable with a distribution that has heavy tails. Such a distribution will assign higher likelihood to outliers, without having to perturb the straight line to “explain” them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is the logistic function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) =\\frac{1}{1+e^{-\\theta}}$$\n",
    "\n",
    "This is the sigmoid function. It has a tigh S shaped curve which is better for approximating to binary (0 or 1) type outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> How is this used in logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that for normal linear regression the output y  = $ h_\\theta (X) + c$\n",
    "\n",
    "This can be thought of as '<i>some</i> function of X with parameter theta' ; similar to saying f(X). Remember function can mean a long polynomial formula with higher exponents $X^n$ and a vector for $\\theta$ , not just 1 value). The c above can be ignored for our purposes - it's simply the intercept of y.\n",
    "\n",
    "In linear regression $ h_\\theta (X)$ is  $\\theta^TX$  (where T = transpose)\n",
    "\n",
    "In logistic regression $ h_\\theta (X)$ is also $\\theta^TX$ but then wrapped up in a sigmoid function\n",
    "\n",
    "\n",
    "$$= \\frac{1}{1+e^{-(\\theta^Tx_0)}}$$ \n",
    "\n",
    "This can also be thought of as prob(y|x and $\\theta$) (e.g. 0.7 = 70% chance). \n",
    "\n",
    "In logistic regression Y = 1 when probability >=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/LogisticFunction.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Logistic Regression Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of a Logistic Regression model is measured by the following loss function:\n",
    "\n",
    "$$J(\\theta) = -\\frac1 m \\sum_{i=1}^m\\Big[ y^i log(h_ \\theta x^i)  + (1 - y^i)log (1-h_\\theta (x^i)\\Big]$$\n",
    "\n",
    "This is what we would like to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gradient Descent and Newton-Raphson algorithm <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, the theta, $\\theta$, vector cannot be calculated easily. It requires using gradient descent to find the minimum point of loss. Mathematically this means finding a $\\theta$ vector where the derviative of the loss (the change in loss given the change in $\\theta$) approximates to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done in iterative steps, and the Newton-Raphson method does the iteration efficiently by calculating a second derivative on top of the first derivative:\n",
    "\n",
    "New $\\theta$ = old $\\theta-\\frac {DiffrentialOfLoss} {DiffrentialOfDifferentialOfLoss}$ \n",
    "\n",
    "= $\\theta_{t+1} = \\theta_{t}  - \\frac {f(J(\\theta_{t}))} {f'(J(\\theta_{t}))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above version of the formula works for when your $\\theta$ is not a vector.\n",
    "The generalized version looks like this:\n",
    "\n",
    "= New $\\theta$ = old $\\theta-\\frac {DiffrentialOfLoss} {DiffrentialOfDifferentialOfLoss}$ \n",
    "\n",
    "= New $\\theta$ = old $\\theta$ - DiffrentialOfLoss * inverse of DiffrentialOfDifferentialOfLoss\n",
    "\n",
    "= $\\theta_{t+1} = \\theta_t  - $ GradientVector *  Hessian($\\theta)^{-1}$\n",
    "\n",
    "= $\\theta_{t+1} = \\theta_t  - \\nabla f'(J(\\theta_t))   * H(\\theta)^{-1} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

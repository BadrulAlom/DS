{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Explain Linear Regression</h4>\n",
    "\n",
    "$$y_i = \\beta x^T +\\epsilon$$\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/aml/aml010.png\" height=\"100\" width=\"300\">\n",
    "\n",
    "* Y is called the the regressand, endogenous variable, response variable, measured variable, criterion variable, or dependent variable\n",
    "* X is called the regressors, exogenous variables, explanatory variables, covariates, input variables, predictor variables, or independent variables \n",
    "* $\\beta$ is the parameter vector. <b>You will often see it being denoted by W, a weight vector.</b> Elements of this are interpreted as the partial derivatives of the dependent variable with respect to the various independent variables.\n",
    "* $\\epsilon$ is called the error term, disturbance term, or noise. This variable captures all other factors which influence the dependent variable yi other than the regressors xi. The relationship between the error term and the regressors, for example whether they are correlated, is a crucial step in formulating a linear regression model, as it will determine the method to use for estimation.\n",
    "* Linear regression has a number of assumptions (https://en.wikipedia.org/wiki/Linear_regression#Extensions) some of which are addressed by the many extensions to it\n",
    "<br>\n",
    "\n",
    "\n",
    "More about the error term:\n",
    "<br>We often assume e has a Gaussian (normal) distribution. We denote this by e ~ <i>N</i>($\\mu,\\sigma)$\n",
    "<br> $p(y|x,\\theta) = N(y | \\mu(x), \\sigma^2(x))$\n",
    "\n",
    "1. The error term, e represents the overall mean error - therefore it's a constant that you add on to make and adjustment to the prediction\n",
    "2. The output, y, will follow a normal distribution equavelent to one generated from using the mean of x, and std. dev. of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color:#616161;color:white\">Measuring the loss</h4>\n",
    "\n",
    "Given N rows of data, the loss is\n",
    "\n",
    "$$Loss(\\beta) = \\sum^N_{i=1} (y^i - \\beta^Tx^i)^2$$\n",
    "\n",
    "or\n",
    "\n",
    "$$E(w) = \\sum^N_{i=1} (y^i - w^Tx^i)^2$$\n",
    "\n",
    "<h4 style=\"background-color:#616161;color:white\">How to derive the parameter vector</h4>\n",
    "\n",
    "A large number of procedures have been developed for parameter estimation and inference in linear regression. \n",
    "\n",
    "<b>Least-squares estimation and related techniques:</b>\n",
    "- Ordinary Least Squares (OLS). Minimizes the sum of squared residuals which leads to a closed form (i.e. mathematical) way to dervise the parameter vector:\n",
    "\n",
    "$${\\hat { {\\beta }}}=({X} ^{\\top }{X} )^{-1} {X} ^{\\top }{y} =\\left(\\sum {x} _{i}{x} _{i}^{\\top }\\right)^{-1}\\left(\\sum {x} _{i}y_{i}\\right)$$\n",
    "\n",
    "- Generalised Least Squares (GLS) is an extension of OLS that allows estimation when heteroscedasticity (no constant variance), or correlations are present among the error terms of the model; as long as the form of heteroscedasticity and correlation is known independently of the data.\n",
    "\n",
    "- Total least squares (TLS) is an approach to least squares estimation of the linear regression model that treats the covariates and response variable in a more geometrically symmetric manner than OLS. It is one approach to handling the \"errors in variables\" problem, and is also sometimes used even when the covariates are assumed to be error-free.\n",
    "\n",
    "<b>Maximum-likelihood estimation and related techniques</b>\n",
    "- Maximum likelihood estimation can be performed when the distribution of the error terms is known to belong to a certain parametric family ƒθ of probability distributions. \n",
    "- Ridge regression and other forms of penalized estimation such as Lasso regression deliberately introduce bias into the estimation of $\\beta$ in order to reduce the variability of the estimate. The resulting estimators generally have lower mean squared error than the OLS estimates, particularly when multicollinearity is present or when overfitting is a problem. They are generally used when the goal is to predict the value of the response variable y for values of the predictors x that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4 style=\"background-color:#616161;color:white\">Regularization</h4>\n",
    "\n",
    "Needed to keep your model in check and avoid overfitting There are a number of ways you can do this:\n",
    "\n",
    "* Normalize each data point by dividing by variance: $x^n_i \\rightarrow \\frac{x^n_i}{\\sigma_i}$\n",
    "* Add an extra term to penalize rapid changes in the error  $$E'(w) + \\lambda R(w) $$where R(w) is the function for determining the error and \\lambda controlling the penalty strength\n",
    "* Add an L2 penalty term that's some fuction of the weight vector\n",
    "$$E(w) = \\sum^{N}_{n=1}(y^n-w^TX^n)^2 + \\lambda w^Tw$$\n",
    "\n",
    "In this case the optimal w is given by\n",
    "\n",
    "$$w=\\left(\\sum_nX^n(X^n)^T+\\lambda I \\right)^{-1} \\sum^{N}_{n=1}y^nX^n$$\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "'Independent variables' do not have to be independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What is multi-collinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "'Independent' variables that are highly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What is hetroesdascity?<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Is when variance of the error is increasing/decreasing over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Non-Parametrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "e.g. Qunitic function = y(x) = $\\alpha_0+\\alpha_1x+\\alpha_2x+\\alpha_3x+\\alpha_4x$\n",
    "\n",
    "Higher degree poynomials are useful as they can provide a better fit to data if a simple linear model does not suffice. Each extra degree allows an additional level of curvature - notice for quadratic the curve changes once, and for quintic it changes 4 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What is Polynomial Least Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Watch this video: https://www.youtube.com/watch?v=Z5iq95Vg2ZY&index=4&list=PLpT5xJ7AmkRW5Q0HwfVRWgUjFdiis-alc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What is Ridge Regression?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ridge regression is a technique that deals with over-fitting due to having too many parameters. If you think of this as resulting in lots of co-efficients that are highly tuned to the dataset it was trained on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What is Robust Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is very common to model the noise (i.e. error term) in regression models using a Gaussian distribution with zero mean and constant variance, \u0005i ∼ N (0, σ 2 ), where \u0005i = yi −wT xi . In this case, maximizing likelihood is equivalent to minimizing the sum of squared residuals, as we have seen. However, if we have outliers in our data, this can result in a poor fit. \n",
    "\n",
    "This is because squared error penalizes deviations quadratically, so points far from the line have more affect on the fit than points near to the line. One way to achieve robustness to outliers is to replace the Gaussian distribution for the response variable with a distribution that has heavy tails. Such a distribution will assign higher likelihood to outliers, without having to perturb the straight line to “explain” them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>What is the logistic function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$$f(x) =\\frac{1}{1+e^{-\\theta}}$$\n",
    "\n",
    "This is the sigmoid function. It has a tigh S shaped curve which is better for approximating to binary (0 or 1) type outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> How is this used in logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Remember that for normal linear regression the output y  = $ h_\\theta (X) + c$\n",
    "\n",
    "This can be thought of as '<i>some</i> function of X with parameter theta' ; similar to saying f(X). Remember function can mean a long polynomial formula with higher exponents $X^n$ and a vector for $\\theta$ , not just 1 value). The c above can be ignored for our purposes - it's simply the intercept of y.\n",
    "\n",
    "In linear regression $ h_\\theta (X)$ is  $\\theta^TX$  (where T = transpose)\n",
    "\n",
    "In logistic regression $ h_\\theta (X)$ is also $\\theta^TX$ but then wrapped up in a sigmoid function\n",
    "\n",
    "\n",
    "$$= \\frac{1}{1+e^{-(\\theta^Tx_0)}}$$ \n",
    "\n",
    "This can also be thought of as prob(y|x and $\\theta$) (e.g. 0.7 = 70% chance). \n",
    "\n",
    "In logistic regression Y = 1 when probability >=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"img/LogisticFunction.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Logistic Regression Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The accuracy of a Logistic Regression model is measured by the following loss function:\n",
    "\n",
    "$$J(\\theta) = -\\frac1 m \\sum_{i=1}^m\\Big[ y^i log(h_ \\theta x^i)  + (1 - y^i)log (1-h_\\theta (x^i)\\Big]$$\n",
    "\n",
    "This is what we would like to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Gradient Descent and Newton-Raphson algorithm <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For logistic regression, the theta, $\\theta$, vector cannot be calculated easily. It requires using gradient descent to find the minimum point of loss. Mathematically this means finding a $\\theta$ vector where the derviative of the loss (the change in loss given the change in $\\theta$) approximates to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is done in iterative steps, and the Newton-Raphson method does the iteration efficiently by calculating a second derivative on top of the first derivative:\n",
    "\n",
    "New $\\theta$ = old $\\theta-\\frac {DiffrentialOfLoss} {DiffrentialOfDifferentialOfLoss}$ \n",
    "\n",
    "= $\\theta_{t+1} = \\theta_{t}  - \\frac {f(J(\\theta_{t}))} {f'(J(\\theta_{t}))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above version of the formula works for when your $\\theta$ is not a vector.\n",
    "The generalized version looks like this:\n",
    "\n",
    "= New $\\theta$ = old $\\theta-\\frac {DiffrentialOfLoss} {DiffrentialOfDifferentialOfLoss}$ \n",
    "\n",
    "= New $\\theta$ = old $\\theta$ - DiffrentialOfLoss * inverse of DiffrentialOfDifferentialOfLoss\n",
    "\n",
    "= $\\theta_{t+1} = \\theta_t  - $ GradientVector *  Hessian($\\theta)^{-1}$\n",
    "\n",
    "= $\\theta_{t+1} = \\theta_t  - \\nabla f'(J(\\theta_t))   * H(\\theta)^{-1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Loss functions\n",
    "\n",
    "It’s very common to train models using the squared loss. However, if the task\n",
    "performance is measured by some other loss, it often makes more sense to\n",
    "train the model using the correct loss. One often sees users train a model\n",
    "using one kind of loss, but evaluate it using a very different loss – seems a bad\n",
    "idea in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Optimization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Explain why in many real world applications, relatively simple applications methods are often considered?\n",
    "\n",
    "Ans:\n",
    "- Easier to explain to others\n",
    "- Easier and quicker to compute and maintain\n",
    "- The greater the number of parameters to be estimated the greater the chance of getting it wrong\n",
    "\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>For a dataset of inputs x and scalar outputs y, ${(x^n,y^n),n=1, ...,N}$, linear regression is the model\n",
    "\n",
    "$$y=w^Tx$$\n",
    "\n",
    "i. The least squares criterion sets w based on minimizing\n",
    "\n",
    "$$\\sum_{n=1}^{N}(y^n-w^Tx^n)^2$$\n",
    "\n",
    "Show that the optimal solution is given by\n",
    "\n",
    "$$w= \\left(\\sum_{n=1}^{N}x^n(x^n)^T\\right)^{-1}\\left(\\sum_{n=1}^{N}y^nx^n\\right)$$</h4>\n",
    "\n",
    "Ans\n",
    "\n",
    "1. Note that in proper English the above should read: Given training data $(x^n,y^n)$,where $n=1, ...,N$ for scalar input $x$ and output $y$, a linear regression model is defined as ...\n",
    "2. Loss function is minimizing $$\\sum_{n=1}^{N}(y^n-w^Tx^n)^2$$\n",
    "\n",
    "\n",
    "3. <span style=\"color:red\">*** Next step is my guess not sure about it and how to get to step after this. See Barbers book pg 347 ***</span>\n",
    "<br>\n",
    "This can be-rewritten as: $$\\sum_{n=1}^{N}(y^n)^2 -\\sum_{n=1}^{N}(w^Tx^n)^2 $$\n",
    "<br>\n",
    "\n",
    "4. Differentiating with respect to  w and equating to zero gives:\n",
    "$$\\sum_{n=1}^{N}y^nx^n - \\sum_{n=1}^{N}x^n(x^n)^Tw  = 0$$\n",
    "\n",
    "5. Re-arranging this gives:\n",
    "$$\\sum_{n=1}^{N}x^n(x^n)^Tw  = \\sum_{n=1}^{N}y^nx^n$$\n",
    "\n",
    "6. Finally using Matrix inversion to solve this we get:\n",
    "$$w=\\left(\\sum_{n=1}{N}x^n(x^n)^T\\right)^{-1}\\left(\\sum_{n=1}^{N}y^nx^n\\right)$$\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "--------------------\n",
    "\n",
    "<b>ii. Explain why in practice it is not recommended to find w by using matrix inversion as above, explain an alternative procedure</b>\n",
    "\n",
    "Ans: Although we write the solution using matrix inversion, in practice one finds the numerical solution using\n",
    "Gaussian elimination since this is faster and numerically more stable\n",
    "\n",
    "<h4>iii. Explain the issues of overfitting and describe a modification to the above method that may prevent overfitting. Give the corresponding opitmal solution for w.</h4>\n",
    "\n",
    "* Overfitting is when a statistical model describes random error or noise rather than the underlying relationship\n",
    "* Adding a regularization term to penalise rapid channges in the output helps reduce this\n",
    "* For example adding an L2 regularization (aka Ridge Regression) to the previous solution would make the optimal:\n",
    "$$w=\\left(\\sum_{n=1}{N}x^n(x^n)^T+\\lambda w^Tw\\right)^{-1}\\left(\\sum_{n=1}^{N}y^nx^n\\right)$$\n",
    "\n",
    "\n",
    "<h4>c. i. Show that the least squares error criterion from part i: \n",
    "$$\\sum_{n=1}^{N}(y^n-w^Tx^n)^2$$\n",
    "\n",
    "can be written in the form:\n",
    "$$E(w) = w^TAw-2w^Tb+c$$\n",
    "for suitably defined A,b,c</h4>\n",
    "\n",
    "Ans:\n",
    "* If A is a positive definite\n",
    "\n",
    "<span style=\"color:red\">*** Not sure how to do this one ***</span>\n",
    "\n",
    "\n",
    "\n",
    "<h4>iv.Explain the concept of gradient descent optimization for minimizing E(w) and explain why this is, in general, an inefficient procedure for the problem.</h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "* Gradient descent is an iterative method that continues until f(x) is 0 or very close to 0\n",
    "* It works by determining the new value of x as: $f(x_k+1)\\approx f(x_k)+(x_{k+1}-x_k)^T \\triangledown f(x_k)$ or new weight = old weigtht + chage in weight * change in f(x)\n",
    "* Simply following the curve, one small step change at a time may take a long time to converge. If the step-change is too great it may overshoot the optimal, of it's too small it will take a long time to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the concept of line search optimization and show that for a line going through the point $w_k$ and direction $p_k$,\n",
    "\n",
    "$w=w_k + \\lambda p_k$\n",
    "<br>\n",
    "the optimal point on the line to minimize the squared error E(w) is given when:\n",
    "<br><br>\n",
    "$$\\lambda = \\frac{(b-Ap_k)^Tp_k}{p^T_kAp_k}$$</h4>\n",
    "\n",
    "\n",
    "Ans:\n",
    "* Line search optimization is when you choose to only change one dimension within the weight vector and optimize along that direction before repeating for other dimensions until convergence is found\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the meaning of the 'conjugate direction' and why this means that the optimum of E(w) can be found by optimizing along each conjugate direction independently</h4>\n",
    "\n",
    "Ans\n",
    "* This is the idea of moving along one dimension of the fine the optimal then moving along another dimension that i conjugate to the first one. This way the change in the new dimension will not impact the change from the first dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Describe Forward Automatic Differentiation (AutoDiff) and give two procedures\n",
    "(one exact and the other an approximation) that compute the gradient of a subroutine\n",
    "f(x) with respect to its arguments x, giving time complexities of the approaches.</h4>\n",
    "\n",
    "Ans:\n",
    "* AutoDiff takes a function f(x) and calculates an exact value (up to machine accuracy) for the gradient\n",
    "* AutoDiff exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically and accurately to working precision\n",
    "* Forward autodiff works by calculating the inner differentials first (the ones with respect to independent variables) before moving up the chain.\n",
    "* The complexity of forward autodiff is equiavalent to the original function for which it is calculating the gradient\n",
    "* Reverse autodiff does it the other way round - calculating the outer derivatives first and working inwards. This requires storing all the intermediate calculations in memory as you go along, which may cause problems, unless checkpointing is used in which certain portions would have to reevaluated at the end\n",
    "* The complexity of reverse autodiff is ...?\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Dimensionality Reduction</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> For datapoints x^n,n=1,...N, define the matrix\n",
    "$$X=[x^1,x^2,...x^N]$$\n",
    "\n",
    "That is, for data points x with dimension D, then X is D $\\times$ N dimensional. The data is such that the mean is 0, that is:\n",
    "\n",
    "$$\\sum_{n=1}^{N}x^n=0$$\n",
    "\n",
    "K-dimensional PCA aims to find a representation:\n",
    "\n",
    "$$x^n\\equiv \\sum_{k=1}^{K}y^n_kb^k$$\n",
    "\n",
    "where $b^1,...b^K$ are 'basis' vectors and $y^n_k$ are coeffcients\n",
    "\n",
    "a) Explain how to efficiently compute the basis vectors and coefficients in order to minimize the squared loss between the approximation and each $x^n$, namely:\n",
    "\n",
    "$$\\sum_{n=1}{N}\\left(x^n-\\sum_{k=1}^{K}y^n_kb^k\\right)^2$$\n",
    "\n",
    "b) Explain how to efficiently compute the basis vectors and coefficients in order to minimize the squared loss \n",
    "$$\\sum_{n=1}{N}\\left(\\tilde x - x^n \\right)^2$$ with $h^n=f(Ax^n) and \\tilde x^n = Bh^n$ for matrices A,B and a non linear function f.\n",
    "\n",
    "For k-dimensional h, is this non-linear procedure in principle more powerful than K-dimensional PCA, in the sense that it has lower squared loss? Explain fully your answer.\n",
    "</h4>\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3> Deep Learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### An input-output time-series ($X_t,Y_t$), t = 1...T can be modelled by a recurrent LSTM (Long ShortTerm Memory) network. Explain the essential components of an LSTM network and what difficulties it tries to overcome (compared to standard recurrent networks).\n",
    "\n",
    "Ans:\n",
    "\n",
    "* AN LSTM tried to overcome the limitations of an RNN when it comes to storing long-term dependencies in memory. With a standard RNN this becomes computationally inefficient.\n",
    "* It does this by introducings the concept of a memory gates that can affect the memory state running through the function\n",
    "* The forget gate takes in the current input ($C_t$) and the previous output and  and uses them to decide whic parts of the current state should be reduced (forgotten)\n",
    "* The input gate also takes in the current input ($C_t$) and the previous output and decides whether anything should be added to the memory. This then gets added to the result of the forget step\n",
    "* The output gate then decides whether to activate an output based on the results at this timestep. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Contents</h3>\n",
    "<br>[Linear Regression](#Linear Regression)\n",
    "<br>[Dimensionality Reduction](#Dimensionality Reduction)\n",
    "<br>[Deep Learning](#Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Linear Regression'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Linear Regression</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>For a dataset of inputs x and scalar outputs y, ${(x^n,y^n),n=1, ...,N}$, linear regression is the model\n",
    "\n",
    "$$y=w^Tx$$\n",
    "\n",
    "i. The least squares criterion sets w based on minimizing\n",
    "\n",
    "$$\\sum_{n=1}^{N}(y^n-w^Tx^n)^2$$\n",
    "\n",
    "Show that the optimal solution is given by\n",
    "\n",
    "$$w= \\left(\\sum_{n=1}^{N}x^n(x^n)^T\\right)^{-1}\\left(\\sum_{n=1}^{N}y^nx^n\\right)$$</h4>\n",
    "\n",
    "Ans\n",
    "\n",
    "1. Somehow you get to this: <span style=\"color:red\">*** DISCUSS ON FRIDAY ***</span>\n",
    "$$0=\\sum^N (y-w^Tx^n)x^n$$\n",
    "\n",
    "2. Differente with respect to w, equate to 0, then moving the right hand side over gives:\n",
    "\n",
    "\n",
    "$$\\sum_{n=1}^{N}y^nx^n = \\sum_{i}w\\ \\sum_{n=1}^{N}x^nx^n_k$$\n",
    "\n",
    "(Note 1: You may just want to memorize this results\n",
    "Note 2: Think of this as y = w*x, so then we want to do y/x in the next step)\n",
    "\n",
    "3. Finally using Matrix inversion to solve this we get:\n",
    "$$w=\\left(\\sum_{n=1}^{N}y^nx^n\\right)\\left(\\sum_{n=1}^N x^nx^n_k\\right)^{-1}$$\n",
    "\n",
    "<br>\n",
    "Which becomes the above solution if you convert it to matrix format\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "<h4>ii. Explain why in practice it is not recommended to find w by using matrix inversion as above, explain an alternative procedure</h4>\n",
    "\n",
    "Ans: Although we write the solution using matrix inversion, in practice one finds the numerical solution using\n",
    "Gaussian elimination since this is faster and numerically more stable\n",
    "\n",
    "--------------------------\n",
    "\n",
    "<h4>iii. Explain the issues of overfitting and describe a modification to the above method that may prevent overfitting. Give the corresponding opitmal solution for w.</h4>\n",
    "\n",
    "* Overfitting is when a statistical model describes random error or noise rather than the underlying relationship\n",
    "* Adding a regularization term to penalise rapid channges in the output helps reduce this\n",
    "* For example adding an L2 regularization (aka Ridge Regression) to the previous solution would make the optimal:\n",
    "$$w=\\left(\\sum_{n=1}{N}x^n(x^n)^T+\\lambda w^Tw\\right)^{-1}\\left(\\sum_{n=1}^{N}y^nx^n\\right)$$\n",
    "\n",
    "------------------\n",
    "\n",
    "<h4>iv. Show that the least squares error criterion from part i: \n",
    "$$\\sum_{n=1}^{N}(y^n-w^Tx^n)^2$$\n",
    "\n",
    "can be written in the form:\n",
    "$$E(w) = w^TAw-2w^Tb+c$$\n",
    "for suitably defined A,b,c</h4>\n",
    "\n",
    "Ans:\n",
    "<span style=\"color:red\">*** DISCUSS ON FRIDAY ***</span>\n",
    "\n",
    "* 'A' is a positive definite and symmetric (One simplistic definition of positive definite is a matrix that when used for $x^TAx$ always gives a positive\n",
    "\n",
    "\n",
    "-------------------\n",
    "\n",
    "<h4>iv.Explain the concept of gradient descent optimization for minimizing E(w) and explain why this is, in general, an inefficient procedure for the problem.</h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "* Gradient descent is an iterative method that continues until f(x) is 0 or very close to 0\n",
    "* It works by determining the new value of x as: $f(x_k+1)\\approx f(x_k)+(x_{k+1}-x_k)^T \\triangledown f(x_k)$ or new weight = old weigtht + chage in weight * change in f(x)\n",
    "* Simply following the curve, one small step change at a time may take a long time to converge. If the step-change is too great it may overshoot the optimal, of it's too small it will take a long time to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Consider the linear regression problem for training data with vector input x^n and scalar output y, n=1..., N\n",
    " where we define the regularized loss as $$E(w) =\\sum^{N}_{n=1}(y^n-w^Tx^n)^2+\\lambda w^Tw$$\n",
    " \n",
    " i. Derive an explicit expression for the optimal weight vector $w_{opt}$ in terms of the training data and regularization constant A. Give also an expression for the computational complexity required to find $w_{opt}$ using this expression\n",
    " </h4>\n",
    " \n",
    "Ans:\n",
    "* <span style=\"color:red\">*** DISCUSS ON FRIDAY ***</span> - how to get to this -- Alans answers don't start by differentiating above\n",
    "$$w=\\left(\\sum_nX^n(X^n)^T+\\lambda I \\right)^{-1} \\sum^{N}_{n=1}y^nX^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the concept of line search optimization and show that for a line going through the point $w_k$ and direction $p_k$,\n",
    "\n",
    "$w=w_k + \\lambda p_k$\n",
    "<br>\n",
    "the optimal point on the line to minimize the squared error E(w) is given when:\n",
    "<br><br>\n",
    "$$\\lambda = \\frac{(b-Ap_k)^Tp_k}{p^T_kAp_k}$$</h4>\n",
    "\n",
    "\n",
    "Ans:\n",
    "* Line search optimization is when you choose to only change one dimension within the weight vector and optimize along that direction before repeating for other dimensions until convergence is found\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the meaning of the 'conjugate direction' and why this means that the optimum of E(w) can be found by optimizing along each conjugate direction independently</h4>\n",
    "\n",
    "Ans\n",
    "* This is the idea of moving along one dimension to the find the optimal,then moving along another dimension that is conjugate to the first one. This way the change in the new dimension will not impact the change from the first dimension.\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Describe Forward Automatic Differentiation (AutoDiff) and give two procedures\n",
    "(one exact and the other an approximation) that compute the gradient of a subroutine\n",
    "f(x) with respect to its arguments x, giving time complexities of the approaches.</h4>\n",
    "\n",
    "Ans:\n",
    "* AutoDiff takes a function f(x) and calculates an exact value (up to machine accuracy) for the gradient\n",
    "* AutoDiff exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically and accurately to working precision\n",
    "* Forward autodiff works by calculating the inner differentials first (the ones with respect to independent variables) before moving up the chain.\n",
    "* The complexity of forward autodiff is equiavalent to the original function for which it is calculating the gradient\n",
    "* Reverse autodiff does it the other way round - calculating the outer derivatives first and working inwards. This requires storing all the intermediate calculations in memory as you go along, which may cause problems, unless checkpointing is used in which certain portions would have to reevaluated at the end\n",
    "* The complexity of reverse autodiff is ...?\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> What is the difference between L1 and L2 regularization?</h4>\n",
    "\n",
    "L1\n",
    "* Term is: sum of abs values of w\n",
    "* Heavy and small $w_i$ is penalized by the a constant proportional amount\n",
    "* Peanlty can't be differentiated so requires special otpmization routines\n",
    "* But will mean that only significant weights remain\n",
    "* In practice (e.g. deep learning) people don't worry about it being non-differentiable and just proceed with gradient based training as normal\n",
    "\n",
    "L2\n",
    "* Term is: $ w^T w$ or $\\sum_i{w^2_i}$\n",
    "* Penalizes large W more than small ones\n",
    "* Penalty is differentiable\n",
    "* Small weights will still persists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Classification'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">Classification</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Explain what is meant by nearest neighbour classification for a dataset of N examples with D-dimensional inputs x and discrete class labels c. Explain how to classify a new input x.</h4>\n",
    "\n",
    "* New item x is classified based on the class of it's nearest neighbour in the dataset $x_1 ... x_n$\n",
    "* Distance can be measured using Euclidean squared distance\n",
    "$$d(x,x') = \\sqrt{\\sum_{i=1}^{D}(x_i - x'_i)^2}$$\n",
    "* PCA can be used in cases of a high number of dimensions to improve calculation speed and improve results\n",
    "* It is not clear how to deal with missing data however or incorporate prior beliefs and domain knowledge\n",
    "\n",
    "<h4> What is Mahalanobis distance?</h4>\n",
    "\n",
    "* Where you multiply the distance by the covariance matrix of the input (from all classes) which rescales the input vector and thus large values dominate less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> What is KNN?</h4>\n",
    "\n",
    "* Extension of NN where you use the class of the K nearest neighbours not just the 1st nearest\n",
    "* We first work out the liklihood model for each class:\n",
    "$$ p(x|c=0) = \\frac{1}{N_{class0}} \\sum_n \\mathcal{N}(x|x^n, \\sigma^2I)$$\n",
    "$$ p(x|c=1) = \\frac{1}{N_{class1}} \\sum_n \\mathcal{N}(x|x^n, \\sigma^2I)$$\n",
    "\n",
    "\n",
    "* For each class we use a mixture of Gaussians to model the data from that class p(x|c), placing it at each training a circle representing the Gaussian\n",
    "<span style=\"color:red\">*** DISCUSS ON FRIDAY ***</span>\n",
    "\n",
    "To then classify a new data point we use Bayes rule:\n",
    "\n",
    "$$ p(c=0|x^{*}) = \\frac{p(x^* | c = 0)p(c=0)}{p(x^{*}|c=0)p(c=0)+p(x^{*}|c=1)p(c=1)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Dimensionality Reduction'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\"> Dimensionality Reduction</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Datapoints $x^n,n=1,...N$, define the matrix\n",
    "$$X=[x^1,x^2,...x^N]$$\n",
    "\n",
    "That is, for data points x with dimension D, then X is D $\\times$ N dimensional. The data is such that the mean is 0, that is:\n",
    "\n",
    "$$\\sum_{n=1}^{N}x^n=0$$\n",
    "\n",
    "K-dimensional PCA aims to find a representation:\n",
    "\n",
    "$$x^n\\equiv \\sum_{k=1}^{K}y^n_kb^k$$\n",
    "\n",
    "where $b^1,...b^K$ are 'basis' vectors and $y^n_k$ are coeffcients\n",
    "<br>\n",
    "<br>\n",
    "Explain how to efficiently compute the basis vectors and coefficients in order to minimize the squared loss between the approximation and each $x^n$, namely:\n",
    "\n",
    "$$\\sum_{n=1}{N}\\left(x^n-\\sum_{k=1}^{K}y^n_kb^k\\right)^2$$</h4>\n",
    "\n",
    "Ans:\n",
    "* First find the point at which the error function is minimal by differentiating the loss function\n",
    "* This gives $$y^n=\\sum b^nx^n$$\n",
    "\n",
    "<span style=\"color:red\">*** DISCUSS ON FRIDAY ***</span>\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>\n",
    "Explain how auto-encoders can be used to find low dimensional representations of data and explain how PCA relates to an Autoencoder.</h4>\n",
    "\n",
    "\n",
    "Ans:\n",
    "\n",
    "* Auto-encoders use neural nets to learn to represent data using. Data is fed into a neural net with one or more hidden layers. The hidden layer with the smallest number of weights effective becomes the lower-level representation of the data. The outputs are then scaled back up to map the output back in the form of the original data. \n",
    "\n",
    "* The loss function then checks the difference between what was generated and the original input.\n",
    "\n",
    "* PCA is equivalent to an Autoencoder with a single hidden layer with a linear trasfer function. In this case PCA is a more efficient way of calculating the optimal.\n",
    "-----------------------\n",
    "\n",
    "\n",
    "<h4>\n",
    "Consider and Auto-encoder with structure x$\\rightarrow$ h $\\rightarrow \\tilde x$, trained to minimize the squared loss:\n",
    "\n",
    "$$\\sum_{n=1}{N}\\left(\\tilde x - x^n \\right)^2$$ \n",
    "with $h^n=f(Ax^n)$ and $\\tilde x^n = Bh^n$ for matrices A,B and a non linear function f.\n",
    "\n",
    "For k-dimensional h, is this non-linear procedure in principle more powerful than K-dimensional PCA, in the sense that it has lower squared loss? Explain fully your answer.\n",
    "</h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "------------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain why PCA is often used as a pre-processing step in machine learning and explain what the geometric meaning of PCA is</h4>\n",
    "\n",
    "http://visionandbeyond.blogspot.co.uk/2014/08/how-to-apply-pca.html\n",
    "\n",
    "* PCA is used as it reduces the data dimensions to something more manageable, and in particular by making the matrix more dense, so the algorithm is more efficient\n",
    "* The geometric meaning of is that PCA1 capture the greatest distance in the dimension space, so if the data as shaped like an oval in a 2d space, it would capture the diameter from one end to the other. PCA2 captures the the distance that is perpendicular to this (so the second greatest distance) and the process continues until principal components count = desired num of dimensions and <=D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain how to write the co-variance matrix S, in terms of the matrix multiplication of X</h4>\n",
    "* As we have assumed the mean is zero we can write: \n",
    "$$S = \\frac{1}{N-1}XX^T$$\n",
    "\n",
    "where N = num of observations\n",
    "<br>\n",
    "(Dummys note: so we are matrix-multiplying X with itself to find the strength of the correlation, and taking the average)\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> PCA is typically described an an eigen-decomposition of S. Explain how this procedure works and also discucss the computational complexity of performing PCA based on directly computing the eigen-decomposition of S</h4>\n",
    "\n",
    "\n",
    "How PCA is implemented:\n",
    "* First calculate the constant, c, which is the mean of x, which in this case is 0\n",
    "* Subtract the mean from every data point per dimension and formulate the co-variance between dimensions as matrix, S (so a DxD matrix)\n",
    "* Find the eigenvectors of each dimension of the co-variance matrix S, sort them in order of eigenvalue, and then form a matrix of eigenvectos 1 to M [$e^1...e^m$] (so discarding the ones with lowest eigenvalue)\n",
    "* Multiply the transpose of this (why transpose ? does it matter?) with $X^n - m)$ to get to the lower dimesion representation of X\n",
    "* The reconstructed x value is therefore is: $\\tilde x = c + Ey^n$\n",
    "* The error is calculated as the difference between, $\\tilde x$, and x, squared\n",
    "\n",
    "Complexity based on eigen-decomposition is O($D^3$)\n",
    "\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain</h4>\n",
    "\n",
    "* One way of finding the principal eigenvector (the one with the larges value, PCA1) is to take any non-zero vector in the plane, and repeatedly multiply it with the covariance matrix\n",
    "* This process will converge to the principal eigenvector (eigenvector with the largest eigen-\n",
    "value). \n",
    "* This is particularly efficient for the case that S is sparse since each matrix-vector\n",
    "product will be fast\n",
    "\n",
    "See: https://www.youtube.com/watch?v=fKivxsVlycs\n",
    "\n",
    "-----------------\n",
    "\n",
    "<h4> Continuing with the sparse dataset scenario, what would be a technique to perform full PCA (not just the principal eigenvector) efficiently?</h4>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Sparse_PCA\n",
    "\n",
    "*  Using the method described above one could find the princial eigenvector then deflate \n",
    "\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " <h4> Explain why the SVD method is related to the eigen decomposition of S and explain the computational complexity of this approach to performing PCA compared to directly computing the eigen-dcomposition of S</h4>\n",
    "* Using the SVD to perform PCA makes much better sense numerically than forming the covariance matrix to begin with, since the formation of XX⊤ can cause loss of precision\n",
    "\n",
    "* PCA requires calculating the eigenvalues and eigenvectors of the covariance matrix, which is the product $XX^⊤$, where X is the data matrix and when the mean is 0.\n",
    "\n",
    "* Since the covariance matrix is symmetric, the matrix is diagonalizable, and the eigenvectors can be normalized such that they are orthonormal\n",
    "\n",
    "* The SVD formula \n",
    "\n",
    " -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> How do you enforce orthonormality in PCA </h4>\n",
    "\n",
    "* We use Larange multipliers so that the objective is to minimize:\n",
    "$$-trace(SBB^T)+trace (L(B^TB-I))$$\n",
    "* Since the constraint is symmetric, we can assume that L is also symmetric.\n",
    "* Differentiating with respect to B and equating to zero we obtain that at the optimum SB = BL\n",
    "* We need to find matrices B and L that satisfy this equation. One solution is given when L is diagonal in which case this is a form of eigen-equation and the columns of B are the corresponding eigenvectors of S\n",
    "* One solutioon is $(SBB^T) = travel (L)$ where L is a diagonal, and B are the eigenvetors of S\n",
    "-----------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Explain what is meant by an auto-encoder neural network</h4>\n",
    "\n",
    "* An auto-encoder NN tried to find a lower dimensional representation of data by setting the output to to match the input dimensions, with reduction of dimension in 1 or more hidden layers in between\n",
    "\n",
    "* The hidden layer with the smaller dimensions (bottleneck layer) is the lower dimensional representation of the input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Deep Learning'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\"> Deep Learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### An input-output time-series ($X_t,Y_t$), t = 1...T can be modelled by a recurrent LSTM (Long ShortTerm Memory) network. Explain the essential components of an LSTM network and what difficulties it tries to overcome (compared to standard recurrent networks).\n",
    "\n",
    "Ans:\n",
    "\n",
    "* AN LSTM tried to overcome the limitations of an RNN when it comes to storing long-term dependencies in memory. With a standard RNN this becomes computationally inefficient.\n",
    "* It does this by introducings the concept of a memory gates that can affect the memory state running through the function\n",
    "* The forget gate takes in the current input ($C_t$) and the previous output and  and uses them to decide whic parts of the current state should be reduced (forgotten)\n",
    "* The input gate also takes in the current input ($C_t$) and the previous output and decides whether anything should be added to the memory. This then gets added to the result of the forget step\n",
    "* The output gate then decides whether to activate an output based on the results at this timestep. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

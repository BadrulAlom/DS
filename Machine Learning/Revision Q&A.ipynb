{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3> Optimization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>For a dataset of inputs x and scalar outputs y, ${(x^n,y^n),n=1, ...,N}$, linear regression is the model\n",
    "\n",
    "$$y=w^Tx$$\n",
    "\n",
    "i. The least squares criterion sets w based on minimizing\n",
    "\n",
    "$$\\sum_{n=1}^{N}(y^n-w^Tx^n)^2$$\n",
    "\n",
    "Show that the optimal solution is given by\n",
    "\n",
    "$$w= \\left(\\sum_{n=1}^{N}x^n(x^n)^T\\right)^{-1}\\left(\\sum_{n=1}^{N}y^nx^n\\right)$$</h4>\n",
    "\n",
    "Ans\n",
    "\n",
    "1. Expand the loss function:\n",
    "$$\\sum_{n=1}^{N}(y^n -\\sum_{i}(w_ix^n_i)(y^n -\\sum_{j}(w_jx^n_j) $$\n",
    "\n",
    "2. Differente with respect to w, equate to 0, then moving the right hand side over gives:\n",
    "\n",
    "\n",
    "$$\\sum_{n=1}^{N}y^nx^n = \\sum_{i}w_i\\ \\sum_{n=1}^{N}x^n_ix^n_k$$\n",
    "\n",
    "(Note 1: You may just want to memorize this results\n",
    "Note 2: Think of this as y = w*x, so then we want to do y/x in the next step)\n",
    "\n",
    "3. Finally using Matrix inversion to solve this we get:\n",
    "$$w=\\left(\\sum_{n=1}^{N}y^nx^n\\right)\\left(\\sum_{n=1}^N x^n_ix^n_k\\right)^{-1}$$\n",
    "\n",
    "<br>\n",
    "Which becomes the above solution if you convert it to matrix format\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "<h4>ii. Explain why in practice it is not recommended to find w by using matrix inversion as above, explain an alternative procedure</h4>\n",
    "\n",
    "Ans: Although we write the solution using matrix inversion, in practice one finds the numerical solution using\n",
    "Gaussian elimination since this is faster and numerically more stable\n",
    "\n",
    "--------------------------\n",
    "\n",
    "<h4>iii. Explain the issues of overfitting and describe a modification to the above method that may prevent overfitting. Give the corresponding opitmal solution for w.</h4>\n",
    "\n",
    "* Overfitting is when a statistical model describes random error or noise rather than the underlying relationship\n",
    "* Adding a regularization term to penalise rapid channges in the output helps reduce this\n",
    "* For example adding an L2 regularization (aka Ridge Regression) to the previous solution would make the optimal:\n",
    "$$w=\\left(\\sum_{n=1}{N}x^n(x^n)^T+\\lambda w^Tw\\right)^{-1}\\left(\\sum_{n=1}^{N}y^nx^n\\right)$$\n",
    "\n",
    "------------------\n",
    "\n",
    "<h4>iv. Show that the least squares error criterion from part i: \n",
    "$$\\sum_{n=1}^{N}(y^n-w^Tx^n)^2$$\n",
    "\n",
    "can be written in the form:\n",
    "$$E(w) = w^TAw-2w^Tb+c$$\n",
    "for suitably defined A,b,c</h4>\n",
    "\n",
    "Ans:\n",
    "<span style=\"color:red\">*** Not sure how to do this one ***</span>\n",
    "\n",
    "* 'A' is a positive definite and symmetric (One simplistic definition of positive definite is a matrix that when used for $x^TAx$ always gives a positive\n",
    "\n",
    "\n",
    "-------------------\n",
    "\n",
    "<h4>iv.Explain the concept of gradient descent optimization for minimizing E(w) and explain why this is, in general, an inefficient procedure for the problem.</h4>\n",
    "\n",
    "Ans:\n",
    "\n",
    "* Gradient descent is an iterative method that continues until f(x) is 0 or very close to 0\n",
    "* It works by determining the new value of x as: $f(x_k+1)\\approx f(x_k)+(x_{k+1}-x_k)^T \\triangledown f(x_k)$ or new weight = old weigtht + chage in weight * change in f(x)\n",
    "* Simply following the curve, one small step change at a time may take a long time to converge. If the step-change is too great it may overshoot the optimal, of it's too small it will take a long time to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the concept of line search optimization and show that for a line going through the point $w_k$ and direction $p_k$,\n",
    "\n",
    "$w=w_k + \\lambda p_k$\n",
    "<br>\n",
    "the optimal point on the line to minimize the squared error E(w) is given when:\n",
    "<br><br>\n",
    "$$\\lambda = \\frac{(b-Ap_k)^Tp_k}{p^T_kAp_k}$$</h4>\n",
    "\n",
    "\n",
    "Ans:\n",
    "* Line search optimization is when you choose to only change one dimension within the weight vector and optimize along that direction before repeating for other dimensions until convergence is found\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Explain the meaning of the 'conjugate direction' and why this means that the optimum of E(w) can be found by optimizing along each conjugate direction independently</h4>\n",
    "\n",
    "Ans\n",
    "* This is the idea of moving along one dimension to the find the optimal,then moving along another dimension that is conjugate to the first one. This way the change in the new dimension will not impact the change from the first dimension.\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Describe Forward Automatic Differentiation (AutoDiff) and give two procedures\n",
    "(one exact and the other an approximation) that compute the gradient of a subroutine\n",
    "f(x) with respect to its arguments x, giving time complexities of the approaches.</h4>\n",
    "\n",
    "Ans:\n",
    "* AutoDiff takes a function f(x) and calculates an exact value (up to machine accuracy) for the gradient\n",
    "* AutoDiff exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically and accurately to working precision\n",
    "* Forward autodiff works by calculating the inner differentials first (the ones with respect to independent variables) before moving up the chain.\n",
    "* The complexity of forward autodiff is equiavalent to the original function for which it is calculating the gradient\n",
    "* Reverse autodiff does it the other way round - calculating the outer derivatives first and working inwards. This requires storing all the intermediate calculations in memory as you go along, which may cause problems, unless checkpointing is used in which certain portions would have to reevaluated at the end\n",
    "* The complexity of reverse autodiff is ...?\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3 style=\"background-color:#616161;color:white\"> Dimensionality Reduction</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Datapoints $x^n,n=1,...N$, define the matrix\n",
    "$$X=[x^1,x^2,...x^N]$$\n",
    "\n",
    "That is, for data points x with dimension D, then X is D $\\times$ N dimensional. The data is such that the mean is 0, that is:\n",
    "\n",
    "$$\\sum_{n=1}^{N}x^n=0$$\n",
    "\n",
    "K-dimensional PCA aims to find a representation:\n",
    "\n",
    "$$x^n\\equiv \\sum_{k=1}^{K}y^n_kb^k$$\n",
    "\n",
    "where $b^1,...b^K$ are 'basis' vectors and $y^n_k$ are coeffcients\n",
    "<br>\n",
    "<br>\n",
    "Explain how to efficiently compute the basis vectors and coefficients in order to minimize the squared loss between the approximation and each $x^n$, namely:\n",
    "\n",
    "$$\\sum_{n=1}{N}\\left(x^n-\\sum_{k=1}^{K}y^n_kb^k\\right)^2$$</h4>\n",
    "\n",
    "Ans:\n",
    "*<span style=\"color:red\">NO IDEA ABOUT THIS ANSWER - following is a start</span>\n",
    "* The loss function can be writte in matrix form as: trace $(X − BY)^T (X − BY)$\n",
    "* We need to minimize this therefore we differentiate the loss function with respect to y. \n",
    "* This gives us $E(B) = trace (X^T(I-BB^T)^2x))$\n",
    "* and so the objective for B becomes to minimize $E(B) = (XX^T(I-BB^T))$\n",
    "* Substituting this back into the loss function and simplifying we eventually get to: $E(B)=(N-1)[trace(S)-trace(SBB^T)]$ where S is the sample covariance of the centered data$\n",
    "\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>\n",
    "Explain how auto-encoders can be used to find low dimensional representations of data and explain how PCA relates to an Autoencoder.</h4>\n",
    "\n",
    "\n",
    "Ans:\n",
    "\n",
    "* Auto-encoders use neural nets to learn to represent data using. Data is fed into a neural net with one or more hidden layers. The hidden layer with the smallest number of weights effective becomes the lower-level representation of the data. The outputs are then scaled back up to map the output back in the form of the original data. \n",
    "\n",
    "* The loss function then checks the difference between what was generated and the original input.\n",
    "\n",
    "* PCA is equivalent to an Autoencoder with a single hidden layer with a linear trasfer function. In this case PCA is a more efficient way of calculating the optimal.\n",
    "\n",
    "-----------------------\n",
    "\n",
    "\n",
    "<h4>\n",
    "Consider and Auto-encoder with structure x$\\rightarrow$ h $\\rightarrow \\tilde x$, trained to minimize the squared loss:\n",
    "\n",
    "$$\\sum_{n=1}{N}\\left(\\tilde x - x^n \\right)^2$$ \n",
    "with $h^n=f(Ax^n)$ and $\\tilde x^n = Bh^n$ for matrices A,B and a non linear function f.\n",
    "\n",
    "For k-dimensional h, is this non-linear procedure in principle more powerful than K-dimensional PCA, in the sense that it has lower squared loss? Explain fully your answer.\n",
    "</h4>\n",
    "\n",
    "Ans:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3> Deep Learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### An input-output time-series ($X_t,Y_t$), t = 1...T can be modelled by a recurrent LSTM (Long ShortTerm Memory) network. Explain the essential components of an LSTM network and what difficulties it tries to overcome (compared to standard recurrent networks).\n",
    "\n",
    "Ans:\n",
    "\n",
    "* AN LSTM tried to overcome the limitations of an RNN when it comes to storing long-term dependencies in memory. With a standard RNN this becomes computationally inefficient.\n",
    "* It does this by introducings the concept of a memory gates that can affect the memory state running through the function\n",
    "* The forget gate takes in the current input ($C_t$) and the previous output and  and uses them to decide whic parts of the current state should be reduced (forgotten)\n",
    "* The input gate also takes in the current input ($C_t$) and the previous output and decides whether anything should be added to the memory. This then gets added to the result of the forget step\n",
    "* The output gate then decides whether to activate an output based on the results at this timestep. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question\n",
    "\n",
    "Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

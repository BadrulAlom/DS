{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\" style=\"background-color:#616161;color:white\">Notes on Spark with Scala</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "* https://www.analyticsvidhya.com/blog/2017/01/scala/\n",
    "* https://spark.apache.org/docs/latest/rdd-programming-guide.html#basics\n",
    "\n",
    "<hr>\n",
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data into an RDD: \n",
    "\n",
    "    val lines = sc.textFile(\"text.txt\");\n",
    "\n",
    "View first two lines:\n",
    "\n",
    "    lines.take(2)\n",
    "\n",
    "Get count of rows in file:\n",
    "\n",
    "    lines.count()\n",
    "\n",
    "Basic map operation that counts the number of characters in each row\n",
    "\n",
    "    val variableName = lines.map(s => s.length)\n",
    "\n",
    "To view the value of a variable:\n",
    "    \n",
    "    variableName.collect()\n",
    "\n",
    "Basic reduce operation that sums up a total\n",
    "    \n",
    "    val totalLength = variableName.reduce((a, b) => a + b)\n",
    "\n",
    "To reuse a variable write this before the reduce. Saved in memory.\n",
    "\n",
    "    totalLength.persist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark’s API relies heavily on passing functions in the driver program to run on the cluster. There are two recommended ways to do this:\n",
    "\n",
    "- Anonymous function syntax, which can be used for short pieces of code.\n",
    "- Static methods in a global singleton object. For example, you can define object MyFunctions and then pass MyFunctions.func1, as follows:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "object MyFunctions {\n",
    "  def func1(s: String): String = { ... }\n",
    "}\n",
    "\n",
    "myRdd.map(MyFunctions.func1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. The most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements by a key."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val lines = sc.textFile(\"data.txt\")   # Reads in data into lines\n",
    "val pairs = lines.map(s => (s, 1))    # Splits data into a tuple - the 1 then gets used in b in the next line\n",
    "val counts = pairs.reduceByKey((a, b) => a + b)   # Adds up all the 1's to get the count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

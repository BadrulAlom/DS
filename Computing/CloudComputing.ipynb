{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Cloud Computing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Cloud Platforms'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">1. Cloud Platforms</h3>\n",
    "\n",
    "<b>Five principle of cloud/utility computing:</b>\n",
    "* Shared Resources: Cost: capital vs. operating expenses\n",
    "* Virtualization: “infinite” capacity\n",
    "* Elasticity: scale up or down on demand\n",
    "* Automation\n",
    "* Metered Billing\n",
    "\n",
    "<b>Value of utility computing (i.e. On Demand computing), in addition to above</b>\n",
    "* Lower barrier to entry for tackling big-data problems\n",
    "* Commoditization and democratization of big-data capabilities\n",
    "\n",
    "<b>When does the cloud make sense?</b>\n",
    "* Limited lifetime requirement or short-term need\n",
    "* Scale variability or volatility\n",
    "* Non-strategic applications or low organizational value\n",
    "\n",
    "<b>Cloud operating models</b>\n",
    "* Infrastructure as a Service (IaaS)\n",
    "    * Why buy machines when you can rent cycles?\n",
    "    * Examples: Amazon EC2, Rackspace\n",
    "* Platform as a Service (PaaS)\n",
    "    * Give me nice API and take care of the maintenance, upgrades, …\n",
    "    * Example: Google App Engine (GAE)\n",
    "* Software as a Service (SaaS)\n",
    "    * Just run it for me!\n",
    "    * Example: Gmail, Salesforce’s Online CRM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Distributed Systems'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">2. Distributed Systems</h3>\n",
    "\n",
    "<b>RESTful API</b>\n",
    "* one way of providing interoperability between computer systems on the internet\n",
    "* the user progresses through an application by selecting links (state transitions), resulting in the next page (representing the next state of the application) being transferred to the user and rendered for their use\n",
    "* REST-compliant Web services allow requesting systems to access and manipulate textual representations of Web resources using a uniform and predefined set of stateless operations\n",
    "\n",
    "<b>HTTP Requests:</b>\n",
    "* GET method should be safe (nullipotent) - calling it produces no side effects (unlike PUT and GET which may change things)\n",
    "* Allow requesting systems to access and manipulate textual representations of Web resources using a uniform and predefined set of stateless operations.\n",
    "* The GET, PUT and DELETE methods should be idempotent - repeated calls should have the same effect as a single request \n",
    "* Post is not idempotent\n",
    "* By making use of a stateless protocol and standard operations, REST systems aim for fast performance, reliability, and the ability to grow, by re-using components that can be managed and updated without affecting the system as a whole, even while it is running\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Lamport Timestamps</h4>\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/cloud/cloud001.png\" height=\"100\" width=\"500\">\n",
    "\n",
    "How the timestamp (i.e. integer incrementing) works:\n",
    "* Everything is set to 0 to begin with\n",
    "* Everything can do 3 things: Internal action, Send, & Recieve.\n",
    "* Whenver internal or send happens it increments its own timestamp by 1\n",
    "* Whenever something recieves something it takes the max of its own timestamp and that of the recieved message, and increments that by 1 to get to a new timestamp number for itself\n",
    "\n",
    "Notes:\n",
    "* Does not allow global ordering of event\n",
    "* Lamport lacks Gap Detection - cannot tell if an event exists elsewhere that sits between two events\n",
    "* Can however assume that if an event from a comes before event from b, then a's clock is lower than b's clock (Known as the Clock consistency condition)\n",
    "* The strong clock consistency condition assumes this and vice-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Deadlocks</h4>\n",
    "\n",
    "A deadlock is a situation in which two or more competing actions are each waiting for the other to finish, and thus neither ever does\n",
    "\n",
    "The Cauffman conditions for a deadlock to occur are:\n",
    "* Mutual Exclusion: There is a resource that can only be used by one process at a time\n",
    "* Hold and Wait: There is a process that needs to hold a resource and wait for others\n",
    "* No Preemption: The monitor cannot forve a process to give up a resource\n",
    "* Circular Wait: A waits for B who waits for C who waits for A\n",
    "\n",
    "<h4>Livelock</h4>\n",
    "Similar to a deadlock, except that the states of the processes involved constantly change with regard to one another, none progressing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The Cap Theorem</h4>\n",
    "\n",
    "Published by Eric Brewer in 2000, the theorem is a set of basic requirements that describe any distributed system. If you imagine a distributed database system with multiple servers, here's how the CAP theorem applies:\n",
    "\n",
    "* Consistency - All the servers in the system will have the same data so users will get the same copy regardless of which server answers their request.\n",
    "* Availability - The system will always respond to a request (even if it's not the latest data or consistent across the system or just a message saying the system isn't working).\n",
    "* Partition Tolerance - The system continues to operate as a whole even if individual servers fail or can't be reached.\n",
    "\n",
    "It's theoretically impossible to have all 3 requirements met, so a combination of 2 must be chosen and this is usually the deciding factor in what technology is used.\n",
    "\n",
    "When it comes to distributed databases, the two choices are only AP or CP because if it's not partition tolerant, it's not really a reliable distributed database. So the choice is simpler: if a network split happens, do you want the database to keep answering but with possibly old/bad data (AP)? Or should it just stop responding unless you can get the absolute latest copy (CP)?\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Fault Tolerance'></a>\n",
    "<h4 style=\"background-color:#616161;color:white\">Fault Tolerance</h4>\n",
    "\n",
    "* Many cloud computations run for extended periods of time on multiple servers.\n",
    "* Checkpoints are taken periodically in anticipation of the need to restart a process when one or more\n",
    "systems fail.\n",
    "* When a failure occurs, the computation is restarted from the last checkpoint rather than from the beginning.\n",
    "* Intuitively, the construction of the global state is equivalent to taking snapshots of individual processes and then combining these snapshots into a global view.\n",
    "\n",
    "* Yet, combining snapshots is straightforward if and only if all processes have access to a global clock and the snapshots are taken at the same time\n",
    "\n",
    "<b>Chandy-Lamport Algorithm</b>\n",
    "\n",
    "1. The observer process (the process taking a snapshot):  \n",
    "    * Saves its own local state\n",
    "    * Sends a snapshot request message bearing a snapshot token to all other processes\n",
    "2. A process receiving the snapshot token for the first time on any message: \n",
    "    * Sends the observer process its own saved state\n",
    "    * Attaches the snapshot token to all subsequent messages (to help propagate the snapshot token)\n",
    "3. When a process that has already received the snapshot token receives a message that does not bear the snapshot token, this process will forward that message to the observer process. This message was obviously sent before the snapshot “cut off” (as it does not bear a snapshot token and thus must have come from before the snapshot token was sent out) and needs to be included in the snapshot.\n",
    "\n",
    "From this, the observer builds up a complete snapshot: a saved state for each process and all messages “in the ether” are saved\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Crash Failures</h4>\n",
    "1. Crash failure: the process abruptly stops and does not resume\n",
    "2. Byzantine failure: A Byzantine fault is any fault presenting different symptoms to different observers. A Byzantine failure is the loss of a system service due to a Byzantine fault in systems that require consensus.\n",
    "    * Omission failures, e.g., failing to receive a request\n",
    "    or send a response.\n",
    "    * Commission failures, e.g., corrupting the local state, sending an incorrect/inconsistent response to the request.\n",
    "\n",
    "Of the two types of failures,Byzantine failures are far more disruptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Paxos Consensus Protocol</h4>\n",
    "\n",
    "18min 44: https://www.youtube.com/watch?v=WX4gjowx45E\n",
    "\n",
    "The basic Paxos protocol assumes\n",
    "– Processes run on processors and communicate\n",
    "through a network;\n",
    "processors and network may experience failures,\n",
    "but not Byzantine failures.\n",
    "\n",
    "The Processors\n",
    "* operate at arbitrary speeds;\n",
    "* have stable storage and may rejoin the protocol after a failure;\n",
    "* send messages to one another\n",
    "\n",
    "The network:\n",
    "* may lose, reorder, or duplicate messages;\n",
    "* messages are sent asynchronously; it may take arbitrary long time to reach the destination\n",
    "\n",
    "The basic Paxos protocol has two phases.\n",
    "\n",
    "Phase I\n",
    "* Proposal preparation\n",
    "* Proposal promise\n",
    "\n",
    "Phase II\n",
    "* Accept request\n",
    "* Accepted\n",
    "\n",
    "The benefit of the Paxos protocols is the\n",
    "guarantee of three safety properties\n",
    "* Non-triviality: Only proposed values can be\n",
    "learned.\n",
    "\n",
    "* Consistency: at most one value can be learned.\n",
    "\n",
    "* Liveness: if a value v has been proposed,\n",
    "eventually every learner will learn some value,\n",
    "provided that sufficient processors remain nonfaulty.\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Parallel Computing</b>\n",
    "\n",
    "The speed-up S measures the effectiveness of parallelization:\n",
    "\n",
    "S(N) = sequential\n",
    "computation time of a task / N parallel\n",
    "computation time of task\n",
    "\n",
    "<b>Embarrassingly Parallel Problems</b>\n",
    "*Little or no effort is required to separate the\n",
    "problem into a number of parallel tasks, i.e.,\n",
    "$\\alpha$ ≈ 0\n",
    "\n",
    "\n",
    "<b>Amdahl’s Law</b>\n",
    "* When the problem/dataset size is fixed Amdahl’s Law accounts for the fact that there will still be a non-parallelizable element\n",
    "– If $\\alpha$ = % of sequential time that can't be parallelized:\n",
    "\n",
    "S(N) = 1 / [$\\alpha$ + (1-$\\alpha$)/N] < 1 / $\\alpha$\n",
    "\n",
    "* The speedup of a program using multiple processors in parallel computing is limited by the sequential fraction of the program.\n",
    "\n",
    "<b>Gustafan's Law</b>\n",
    "S(N) = $\\alpha$ + N (1- $\\alpha$) = N - $\\alpha$ (N-1)\n",
    "\n",
    "The limitations of the sequential part of a code can be balanced by increasing the problem size (i.e. do more in one go).\n",
    "\n",
    "<b>Comparison</b>\n",
    "– Amdahl’s law argues that\n",
    "more computing power will make an analysis of\n",
    "the same dataset faster.\n",
    "– Gustafson’s law argues that\n",
    "more computing power will make bigger datasets\n",
    "be analysed or deeper analysis be performed\n",
    "\n",
    "\n",
    "<b>Example question:</b>\n",
    "A data mining program consists of four consecutive parts, P1, P2, P3 and P4 with\n",
    "the percentages of runtime being 10%, 30%, 40% and 20% respectively on a single\n",
    "processor. It is known that P1 and P3 can be parallelised while P2 and P4 cannot. If\n",
    "the problem/dataset size is fixed, how much speedup can this program achieve at most\n",
    "through parallel computing, according to Amdahl’s Law? If the problem/dataset size\n",
    "can be arbitrarily large, how much speedup can this program achieve at most through\n",
    "parallel computing, according to Gustafson’s Law?\n",
    "\n",
    "<h4>Single Program Multiple Data (SPMD)</h4>\n",
    "Single Program Multiple Data (SPMD):\n",
    "multiple copies of the same program run\n",
    "concurrently, each one on a different data block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Concurrency</h4>\n",
    "Concurrency is important\n",
    "* Many cloud applications are data-intensive and\n",
    "use a number of instances which run concurrently\n",
    "\n",
    "Concurrency is challenging\n",
    "* It could lead to race conditions, an undesirable\n",
    "effect when the results of concurrent execution\n",
    "depend on the sequence or timing of events\n",
    "* Shared resources must be protected by locks /\n",
    "semaphores / monitors to ensure serial access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Distributed File Systems</h4>\n",
    "\n",
    "* E.g. Google's GFS, and HDFS for Hadoop\n",
    "* Data kept in \"chunks\" spread across machines\n",
    "* Each chunk replicated on different machines\n",
    "* Seamless recovery from disk or machine failure\n",
    "\n",
    "<h4>Assumptions behind distributed file systems</h4>\n",
    "\n",
    "* Commodity hardware over “exotic” hardware\n",
    "    * Scale “out”, not “up”\n",
    "* High component failure rates\n",
    "    * Inexpensive commodity components fail all the time\n",
    "* “Modest” number of huge files\n",
    "    * Multi-gigabyte files are common, if not encouraged\n",
    "* Files are write-once, mostly appended to\n",
    "    * Perhaps concurrently\n",
    "* Large streaming reads over random access\n",
    "    * High sustained throughput over low latency\n",
    "\n",
    "<h4>HFDS Indexing & Hashing</h4>\n",
    "* Assumes a very large Key-Value collection\n",
    "* The Key-Value index supports insertions, deletions, key searches, range searches, and node leave & join operations.\n",
    "* Uses a hashing function to map data into a specified number of buckets. servers. This way, you can control the size of the data subsets (i.e., buckets) and optimize for query speed.\n",
    "\n",
    "A naïve solution\n",
    "* Let N be the number of servers.\n",
    "* The function modulo(h(k), N) = i maps a pair (k,v)\n",
    "to server Si .\n",
    "* Fact: if N changes, or if a client uses an invalid\n",
    "value for N, the mapping becomes inconsistent\n",
    "\n",
    "With consistent hashing, addition or removal of an instance does not significantly change the mapping of keys to servers.\n",
    "\n",
    "* A simple, non-mutable hash function h maps both the keys and the servers’ IPs to a large address space A (e.g., [0, 264−1]) organized as a ring in clockwise order.\n",
    "* If S and S′ are two adjacent servers on the ring: all the keys in range [h(S),h(S′)] are mapped to S.\n",
    "* If a server fails put a copy on the next machine (on the ring)\n",
    "\n",
    "<h4>Load Balancing</h4>\n",
    "Map a server to several points on the ring (virtual\n",
    "nodes)\n",
    "* the more points, the more load received by a server;\n",
    "* also useful if the server fails: data relocation is more\n",
    "evenly distributed.\n",
    "* also useful in case of heterogeneity (the rule in largescale\n",
    "systems).\n",
    "\n",
    "<h4>Hashing Directory</h4>\n",
    "* Stored on the master node which is therefore not scalable\n",
    "* Aamzon Dynamo duplicates and maintains a full hash directory at each node via 'gossiping': Queries can be routed to the right server in 1 message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Relational Databases</h4>\n",
    "* Multipurpose\n",
    "    * transactions & analysis\n",
    "    * Batch & interactive\n",
    "* Data integrity via ACID transactions\n",
    "* Lots of tools in software ecosystem\n",
    "    * for ingesting, reporting, etc.\n",
    "* Supports SQL (and SQL integration, e.g., JDBC)\n",
    "* Automatic SQL query optimization\n",
    "\n",
    "* Projecting means choosing some columns from each record and leaving others out.\n",
    "    * Easy in Map-Reduce: Map over tuples, emit new tuples with appropriate attributes\n",
    "    * No reducers\n",
    "        * unless for regrouping or resorting tuples\n",
    "    * Speed of encoding/decoding tuples becomes important\n",
    "    * Relational databases take advantage ofcompression\n",
    "    * Semi-structured data? No problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Hadoop</h4>\n",
    "\n",
    "MapReduce (Hadoop):\n",
    "* Designed for large clusters, fault tolerant\n",
    "* Data is accessed in “native format”\n",
    "* Supports many query languages\n",
    "* Programmers retain control over performance\n",
    "* Open source\n",
    "\n",
    "Online Transaction Processing (OLTP)\n",
    "* Typical applications:\n",
    "    * e-commerce, banking, airline reservations\n",
    "* User facing:\n",
    "    * real-time, low latency, highly-concurrent\n",
    "* Tasks:\n",
    "    * relatively small set of “standard” transactional queries\n",
    "* Data access pattern:\n",
    "    * random reads, updates, writes (involving relatively\n",
    "small amounts of data\n",
    "\n",
    "Online Analytical Processing (OLAP)\n",
    "* Typical applications:\n",
    "    * business intelligence, data mining\n",
    "* Back-end processing:\n",
    "    * batch workloads, less concurrency\n",
    "* Tasks:\n",
    "    * complex analytical queries, often ad hoc\n",
    "* Data access pattern:\n",
    "    * table scans, large amounts of data involved per query\n",
    "    \n",
    "<b>Hadoop is perfect for ETL:</b>\n",
    "* Most likely, you already have some data\n",
    "warehousing solution\n",
    "* Ingestion is limited by the speed of HDFS\n",
    "* Scales out with more nodes\n",
    "* Massively parallel\n",
    "* Ability to use any processing tool\n",
    "* Much cheaper than parallel databases\n",
    "* ETL is a batch process anyway!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Distributed Systems'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">2. Map-Reduce</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/cloud/cloud002.png\" height=\"100\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/cloud/cloud003.png\" height=\"100\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Partitioning:</b>\n",
    "* Used to ensure records with the same key end up at the same worker\n",
    "* Uses a simple function hash(key) mod R to decide which Reducer to go to\n",
    "* Sometimes useful to override. E.g hash(hostname(URL)) mod R ensures URLs from one host end up in the same output file\n",
    "\n",
    "<b>Join query in Map-Reduce</b>\n",
    "\n",
    "* Reduce-side join: \n",
    "    * Map over both tables and emit join key & value\n",
    "    * Reducer: Group by join key, then perform actual join\n",
    "\n",
    "* Map-side join: \n",
    "    * Assume two datasets are sorted by the join key to start with\n",
    "    * Partition and sort both datasets in the same manner\n",
    "    * Map over one dataset, read from other corresponding partition\n",
    "    * But a consistenly partioned dataset may not be realistic\n",
    "    \n",
    "* In-memory join\n",
    "    * Basic version: Each node has a copy of the lookup table so can do a join during map stage. No reducers needed.\n",
    "    – Striped variant: Divide lookup table up across nodes so that it can fit in memory. Combine results in reducer later.\n",
    "    – Memcached variant: In memory key-value store. Very fast. Scales out with cluster.\n",
    "\n",
    "Limitations of each?\n",
    "* In-memory join: memory\n",
    "* Map-side join: sort order and partitioning\n",
    "* Reduce-side join: general purpose\n",
    "\n",
    "<h4> Combiners </h4>\n",
    "\n",
    "This mapper function which has a multii-doc wrapper for a single doc function exemplifies the need for a combiner in a word count of individual documents\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/cloud/cloud005.png\" height=\"100\" width=\"400\">\n",
    "\n",
    "* The combiner could group together counts for the same document\n",
    "* While the combiner can do a similar role to the reducer, bear in mind the combiner only gets a subset of the data, so if you were trying to calculate averages for example you'd end calculating a mini average and pushing that forward to the reducer\n",
    "* Using the reducer as a combiner works only if the function we're computing is both commutative (a + b = b + a) and associative (a + (b + c) = (a + b) + c). \n",
    "\n",
    "http://www.philippeadjiman.com/blog/2010/01/14/hadoop-tutorial-series-issue-4-to-use-or-not-to-use-a-combiner/\n",
    "\n",
    "<b>Calculating a co-occurence matrix</b>\n",
    "\n",
    "<b>Pairs method:</b>\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/cloud/cloud006.png\" height=\"100\" width=\"500\">\n",
    "\n",
    "<b>Stripes method:(Essentially you can emit an entire row at a time)</b>\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/cloud/cloud007.png\" height=\"100\" width=\"500\">\n",
    "\n",
    "<h4>Drawback of stripes</h4>\n",
    "* First, what if the data structure in Reduce exceeds the size of the available memory? This would not happen for the color example, but it might for other problems where the table has many more columns. \n",
    "* Second, the granularity of the Reducer workload is limited by the number of different keys. What if we have more machines than keys? How can we keep all of them busy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='MapReduce Optimization'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">4. MapReduce Optimization</h3>\n",
    "\n",
    "<b>Ways to optimize:</b>\n",
    "* Compression of data from one flow to the next\n",
    "    * Trade-off between the volume of data transfer and the (de)compression time.\n",
    "    * Usually, compressing map outputs using a fast compressor increases efficiency\n",
    "\n",
    "* Available memory\n",
    "    * Particularly for Reduce Shuffle and Sort\n",
    "\n",
    "* Speculative Execution\n",
    "    * Jobtracker tries to detect tasks that are taking a long time, such as one node being very slow\n",
    "    * In this case it would make copies of the tasks and execute them\n",
    "    * Whichever one finishes first wins\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='AlgoDesign'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">5. Map-Reduce Algorithm Design</h3>\n",
    "\n",
    "* <b>“In-Mapper Combining”</b>\n",
    "    * The in-mapper combiner takes this optimization a bit further: the aggregations do not even write to local disk: they occur in-memory in the Mapper itself.\n",
    "    * So one example is to have an Initialize, Map, an Call method in your Mapper, where the intialize setups an an associative array to store results, the mappers does each indicidual processing and stores in the array, and close does an aggregation before emitting\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/cloud/cloud005.png\" height=\"100\" width=\"500\">\n",
    "\n",
    "Advantages: Speed as not writing to disk\n",
    "Disadvantages: Explicit memory management required. Potential for order-dependent bugs\n",
    "\n",
    "* <b>“Pairs” vs “Stripes”</b>\n",
    " * Pairs is just emitting a key and a value\n",
    " * Stripes is grouping multiple results into an associative array with a defined structure so thaat in the combiner or reduce phase it can efficiently interpret the results\n",
    " \n",
    "* <b>Secondary Sorting</b>\n",
    "    * Value-to-Key Conversion: Creating a composite key is straight forward. What we need to do is analyze what part(s) of the value we want to account for during the sort and add the appropriate part(s) to the natural key. \n",
    "    \n",
    "* Order Inversion\n",
    "    * The design pattern for controlling the order in which intermediate results are computed is called “order inversion.” \n",
    "    * The Mapper will emit a dummy along with the value, where the dummy should always come first if sorted\n",
    "    * There would be an IF statement in the reducer to then calculate the dummy first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Hadoop'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">3. Hadoop</h3>\n",
    "\n",
    "* SequenceFiles: Binary encoded of a sequence of key/value pairs\n",
    "\n",
    "* Concrete classes for different data types:\n",
    "    * IntWritable\n",
    "    * LongWritable\n",
    "    * Text\n",
    "    * etc.\n",
    "\n",
    "* Writable Defines a de/serialization protocol.\n",
    "* Every data type in Hadoop is a Writable.\n",
    "* WritableComparable Defines a sort order.\n",
    "* All keys must be of this type (but not values).\n",
    "\n",
    "\n",
    "<b>Job submission process</b>\n",
    "* Client (i.e., driver program) creates a job, configures it, and submits it to job tracker\n",
    "* JobClient computes input splits (on client end)\n",
    "* Job data (jar, configuration XML) are sent to JobTracker\n",
    "* JobTracker puts job data in shared location, enqueues tasks\n",
    "* TaskTrackers poll for tasks\n",
    "* Off to the races…\n",
    "\n",
    "<h4>Hadoop Map-Reduce Process</h4>\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/cloud/cloud004.png\" height=\"100\" width=\"400\">\n",
    "\n",
    "* Map outputs are buffered in memory in a circular\n",
    "buffer\n",
    "* When buffer reaches threshold, contents are\n",
    "“spilled” to disk\n",
    "* Spills merged in a single, partitioned file (sorted\n",
    "within each partition): combiner runs here\n",
    "\n",
    "\n",
    "Reduce side\n",
    "* First, map outputs are copied over to reducer\n",
    "machine\n",
    "* “Sort” is a multi-pass merge of map outputs\n",
    "(happens in memory and on disk): combiner runs\n",
    "here\n",
    "* Final merge pass goes directly into reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Spark'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">6. Spark</h3>\n",
    "\n",
    "<b>Advantages:</b>\n",
    "* Speed due to being in memory    \n",
    "* RDDs allow for data sharing across jobs\n",
    "* Good for data that comes in batches (e.g. streaming)\n",
    "\n",
    "<b>Disadvantages:</b>\n",
    "* Its “in-memory” capability can become a bottleneck when it comes to cost-efficient processing of big (transactional) data.\n",
    "    \n",
    "<b>RDD's</b> \n",
    "* Resiliant Distributed Datasets\n",
    "    * A read-only, partitioned collection of records.\n",
    "    * Held as distributed shared memory\n",
    "    * Can only be built from a series of deterministic transformations (e.g. map, filter, join)\n",
    "    * This is known as a DAG (Direct Acyclic Graph)\n",
    "    * Fault tolerance - RDDs track the graph of transformations that built them (their lineage) to rebuild lost data\n",
    "    * Spark offers over 80 high-level operators that make it easy to build parallel apps\n",
    "\n",
    "RDDs can express many existing parallel\n",
    "models\n",
    "– MapReduce, DryadLINQ\n",
    "– Iterative MapReduce\n",
    "– Pregel\n",
    "– SQL\n",
    "\n",
    "<b>Iterative vs. interactive</b>\n",
    "* Spark makes 'working set' of data a first‐class concept to efficiently support:\n",
    "    * Iterative algorithms    (many in machine learning)    \n",
    "    * Interactive data mining tools (R, Excel, Python)    \n",
    "\n",
    "<b>Operations on RDDs</b>\n",
    "* Transformations define new RDDs\n",
    "    * 'map' means a one-to-one mapping\n",
    "    * 'flatMap' maps each input value to one or more outputs (similar to the map in MapReduce)\n",
    "* Actions use RDDs to return results to the driver program, or export results to the storage system\n",
    "    * collect\n",
    "    * reduce\n",
    "    * count\n",
    "    * save\n",
    "    * lookupKey\n",
    "\n",
    "* Spark computes RDDs lazily the first time they are used in an action so that it can pipeline transformations. i.e., an RDDs is defined by transformations but its computation is actually triggered by actions\n",
    "\n",
    "Control of RDDs\n",
    "* persistence (storage in RAM, on disk, etc)\n",
    "    * Users can indicate which RDDs they will reuse and choose a storage strategy for them.\n",
    "\n",
    "* partitioning (layout across nodes)\n",
    "    * Users can indicate how an RDD’s elements be partitioned across machines based on a key in each record. This is useful for placement optimizations, such as ensuring that two datasets that will be joined together are hash-partitioned in the same way\n",
    "\n",
    "\n",
    "* Narrow / wide dependency\n",
    "    * Narrow: Each partition of the parent RDD is used by at most one partition of the child RDD\n",
    "    * Wide:Each partition of the parent RDD is used by multiple partitions of the child RDD\n",
    "    * Allows for pipelined execution on one cluster node, while wide dependencies require data from all parent partitions to be available and to be shuffled across the nodes using a MapReduce-like operation.\n",
    "    * Recovery after a node failure is more efficient with a narrow dependency as only the lost parent partitions need to be recomputed, and they can be recomputed in parallel on different nodes.\n",
    "\n",
    "* Discretized Stream (D-Stream) Model\n",
    "    * Run a streaming computation as a series of small, stateless, and deterministic batch computations\n",
    "    * Same recovery schemes at much smaller timescale\n",
    "    * Work to make batch size as small as possible\n",
    "    \n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/cloud/cloud009.png\" height=\"100\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Q&A'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">8. Q&A</h3>\n",
    "\n",
    "<h4>\n",
    "There is a large text file of information about films stored in the HDFS over a number of\n",
    "machines. Each line of this file describes the details of one film in the following format.\n",
    "title | year | runtime | genres | stars\n",
    "<br><br>\n",
    "The different fields are separated by the | character; the list of genres and the list of stars\n",
    "are both separated by commas; the runtime is measured in minutes.\n",
    "An example line is given below.\n",
    "The Godfather | 1972 | 175 | Crime, Drama | Marlon Brando, Al Pacino, James Caan\n",
    "You can assume that there are no duplicate records, and each distinct star’s name is unique.\n",
    "Write a MapReduce program (in pseudo-code) to calculate for each genre the average\n",
    "runtime of film in the 1970s.\n",
    "A combiner should be implemented to accelerate the computation.\n",
    "</h4>\n",
    "\n",
    "<code>\n",
    "\n",
    "<img src=\"https://github.com/BadrulAlom/Data-Science-Notes/raw/master/_img/cloud/cloud008.png\" height=\"100\" width=\"400\">\n",
    "\n",
    "---------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>There is a large text file of computer science bibliography data held in an HDFS over a\n",
    "number of machines. It is the same file as described in the previous question.\n",
    "Each line of this file describes the details of one paper in the following format.\n",
    "authors | title | conference | year\n",
    "The different fields are separated by the | character, and the list of authors are separated by\n",
    "commas. An example line is given below.\n",
    "D Zhang, J Wang, D Cai, J Lu | Self-Taught Hashing for Fast Similarity Search | SIGIR | 2010\n",
    "You can assume that there are no duplicate records, and each distinct author or conference\n",
    "has a different name.\n",
    "Write a MapReduce program (in pseudo-code) to calculate for each author the total number\n",
    "of papers he/she has published.\n",
    "The “in-mapper combining” pattern should be implemented to accelerate the computation.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>There is a large text file of computer science bibliography data held in an HDFS over a\n",
    "number of machines. It is the same file as described in the previous two questions.\n",
    "Each line of this file describes the details of one paper in the following format.\n",
    "authors | title | conference | year\n",
    "The different fields are separated by the | character, and the list of authors are separated by\n",
    "commas. An example line is given below.\n",
    "D Zhang, | Wang, D Cai, | Lu | Self-Taught Hashing for Fast Similarity Search | SIGIR | 2010\n",
    "You can assume that there are no duplicate records, and each distinct author or conference\n",
    "has a different name.\n",
    "Write a MapReduce program (in pseudo-code), using the “stripes” pattern, to calculate for\n",
    "each author the total number of papers he/she has co-authored with each different collaborator\n",
    "(i.e., researcher who has co-authored papers with him/her before). For example, the\n",
    "output corresponding to the author D Zhang could be as follows.\n",
    "D Zhang — D Cai: 3, WS Lee: 13, | Lu: 6, | Wang: 8\n",
    "A combine function should be implemented to accelerate the computation.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Revision Lecture Notes'></a>\n",
    "<h3 style=\"background-color:#616161;color:white\">9. Revision Lecture Notes</h3>\n",
    "\n",
    "* 8 factual questions - keep answers short\n",
    "* 3 algorithm questions (1 graph based)\n",
    "* New for this year - Apache Spark\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
